'use client'

import { useState, useRef, useEffect } from 'react'
import { IOSCard, IOSCardHeader, IOSCardTitle, IOSCardContent } from '@/components/ui/ios-card'
import { IOSButton } from '@/components/ui/ios-button'
import { Mic, Sparkles, StopCircle, Waves } from 'lucide-react'
import { triggerHaptic } from '@/lib/ios-utils'

interface VoiceQuickQueryProps {
  onResult?: (query: string, result: any) => void
}

export function VoiceQuickQuery({ onResult }: VoiceQuickQueryProps) {
  const [listening, setListening] = useState(false)
  const [processing, setProcessing] = useState(false)
  const [transcript, setTranscript] = useState('')
  const [response, setResponse] = useState('')
  const [audioLevel, setAudioLevel] = useState(0) // éŸ³é‡ç­‰ç´šï¼ˆ0-100ï¼‰
  const [isUsingProfessionalASR, setIsUsingProfessionalASR] = useState(false) // æ˜¯å¦ä½¿ç”¨å°ˆæ¥­ ASR

  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const streamRef = useRef<MediaStream | null>(null)
  const recognitionRef = useRef<any>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const analyserRef = useRef<AnalyserNode | null>(null)
  const animationFrameRef = useRef<number | null>(null)

  // æª¢æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨å°ˆæ¥­ ASRï¼ˆDeepgramï¼‰
  useEffect(() => {
    const checkProfessionalASR = async () => {
      try {
        const res = await fetch('/api/voice/chat')
        const data = await res.json()
        setIsUsingProfessionalASR(data?.services?.deepgram?.available === true)
      } catch {
        setIsUsingProfessionalASR(false)
      }
    }
    checkProfessionalASR()
  }, [])

  // éŸ³é‡æª¢æ¸¬
  const startAudioLevelMonitoring = (stream: MediaStream) => {
    try {
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)()
      const analyser = audioContext.createAnalyser()
      const microphone = audioContext.createMediaStreamSource(stream)
      
      analyser.fftSize = 256
      microphone.connect(analyser)
      
      audioContextRef.current = audioContext
      analyserRef.current = analyser
      
      const dataArray = new Uint8Array(analyser.frequencyBinCount)
      
      const updateLevel = () => {
        if (!analyserRef.current || !listening) {
          return
        }
        
        analyserRef.current.getByteFrequencyData(dataArray)
        const average = dataArray.reduce((a, b) => a + b) / dataArray.length
        setAudioLevel(Math.min(100, (average / 255) * 100))
        
        if (listening) {
          animationFrameRef.current = requestAnimationFrame(updateLevel)
        }
      }
      
      updateLevel()
    } catch (error) {
      console.warn('[Voice] Audio level monitoring failed:', error)
    }
  }

  const stopAudioLevelMonitoring = () => {
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current)
      animationFrameRef.current = null
    }
    if (audioContextRef.current) {
      audioContextRef.current.close()
      audioContextRef.current = null
    }
    analyserRef.current = null
    setAudioLevel(0)
  }

  // ä½¿ç”¨å°ˆæ¥­ ASRï¼ˆDeepgramï¼‰éŒ„è£½å’Œè™•ç†
  const startProfessionalRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 16000,
        } 
      })
      
      streamRef.current = stream
      startAudioLevelMonitoring(stream)
      
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus',
      })
      
      mediaRecorderRef.current = mediaRecorder
      audioChunksRef.current = []
      
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }
      
      mediaRecorder.onstop = async () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' })
        await processAudioWithProfessionalASR(audioBlob)
        stopRecording()
      }
      
      mediaRecorder.start()
    } catch (error: any) {
      console.error('[Voice] Professional recording failed:', error)
      setListening(false)
      setResponse(`âŒ ç„¡æ³•è¨ªå•éº¥å…‹é¢¨ï¼š${error.message}`)
      stopRecording()
    }
  }

  // ä½¿ç”¨å°ˆæ¥­ ASR è™•ç†éŸ³é »
  const processAudioWithProfessionalASR = async (audioBlob: Blob) => {
    setProcessing(true)
    triggerHaptic('light')
    
    try {
      const formData = new FormData()
      formData.append('audio', audioBlob, 'recording.webm')
      // ä¸éœ€è¦ conversationHistoryï¼Œè®©æœåŠ¡ç«¯ä½¿ç”¨é»˜è®¤

      const response = await fetch('/api/voice/chat', {
        method: 'POST',
        body: formData,
      })

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}`)
      }

      const data = await response.json()

      if (!data.transcript || data.transcript.trim().length === 0) {
        setResponse('âŒ ç„¡æ³•è­˜åˆ¥èªéŸ³å…§å®¹ï¼Œè«‹é‡è©¦')
        setProcessing(false)
        return
      }

      setTranscript(data.transcript)

      // æ˜¾ç¤º AI å›æ‡‰
      if (data.response) {
        setResponse(data.response)
      }

      // å¦‚æœæœ‰æ•¸æ“šçµæœï¼Œå‚³å›çµ¦çˆ¶çµ„ä»¶
      if (onResult) {
        onResult(data.transcript, data)
      }
    } catch (error: any) {
      console.error('[Voice] èªéŸ³è™•ç†éŒ¯èª¤:', error)
      setResponse(`âŒ èªéŸ³è™•ç†å¤±æ•—ï¼š${error.message}`)
    } finally {
      setProcessing(false)
    }
  }

  // ä½¿ç”¨ç€è¦½å™¨åŸç”ŸèªéŸ³è­˜åˆ¥ï¼ˆé™ç´šæ–¹æ¡ˆï¼‰
  const startBrowserRecognition = () => {
    // æª¢æŸ¥ç€è¦½å™¨æ˜¯å¦æ”¯æ´èªéŸ³è­˜åˆ¥
    if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
      setListening(false)
      setResponse('âŒ æ‚¨çš„ç€è¦½å™¨ä¸æ”¯æ´èªéŸ³è¼¸å…¥åŠŸèƒ½')
      return
    }

    const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition
    const recognition = new SpeechRecognition()

    recognition.lang = 'zh-TW'
    recognition.continuous = false
    recognition.interimResults = true
    recognition.maxAlternatives = 1

    recognition.onresult = (event: any) => {
      let interimTranscript = ''
      let finalTranscript = ''
      
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript
        if (event.results[i].isFinal) {
          finalTranscript += transcript
        } else {
          interimTranscript += transcript
        }
      }
      
      // å¯¦æ™‚é¡¯ç¤ºè­˜åˆ¥çµæœ
      if (interimTranscript) {
        setTranscript(interimTranscript)
      }
      
      // å¦‚æœæ˜¯æœ€çµ‚çµæœï¼Œè™•ç†æŸ¥è©¢
      if (finalTranscript) {
        setTranscript(finalTranscript)
        handleQuery(finalTranscript)
      }
    }

    recognition.onerror = (event: any) => {
      console.error('Speech recognition error:', event.error)
      setListening(false)
      
      let errorMsg = 'âŒ èªéŸ³è­˜åˆ¥å¤±æ•—'
      switch (event.error) {
        case 'no-speech':
          errorMsg = 'âŒ æ²’æœ‰æª¢æ¸¬åˆ°èªéŸ³ï¼Œè«‹é‡è©¦'
          break
        case 'audio-capture':
          errorMsg = 'âŒ ç„¡æ³•è¨ªå•éº¥å…‹é¢¨ï¼Œè«‹æª¢æŸ¥æ¬Šé™'
          break
        case 'not-allowed':
          errorMsg = 'âŒ éº¥å…‹é¢¨æ¬Šé™è¢«æ‹’çµ•ï¼Œè«‹å…è¨±è¨ªå•'
          break
        case 'network':
          errorMsg = 'âŒ ç¶²è·¯éŒ¯èª¤ï¼Œè«‹æª¢æŸ¥é€£æ¥'
          break
      }
      setResponse(errorMsg)
    }

    recognition.onend = () => {
      setListening(false)
    }

    recognitionRef.current = recognition
    recognition.start()

    // 15 ç§’å¾Œè‡ªå‹•åœæ­¢
    setTimeout(() => {
      if (listening && recognitionRef.current) {
        recognitionRef.current.stop()
        setListening(false)
      }
    }, 15000)
  }

  const startListening = async () => {
    triggerHaptic('light')
    setListening(true)
    setTranscript('')
    setResponse('')
    setAudioLevel(0)

    // å„ªå…ˆä½¿ç”¨å°ˆæ¥­ ASRï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if (isUsingProfessionalASR) {
      await startProfessionalRecording()
    } else {
      // é™ç´šåˆ°ç€è¦½å™¨åŸç”Ÿ API
      startBrowserRecognition()
    }
  }

  const stopListening = () => {
    triggerHaptic('medium')
    
    // åœæ­¢å°ˆæ¥­éŒ„è£½
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop()
    }
    
    // åœæ­¢ç€è¦½å™¨è­˜åˆ¥
    if (recognitionRef.current) {
      recognitionRef.current.stop()
      recognitionRef.current = null
    }
    
    stopRecording()
  }

  const stopRecording = () => {
    setListening(false)
    stopAudioLevelMonitoring()
    
    // åœæ­¢æ‰€æœ‰éŸ³é »è»Œé“
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop())
      streamRef.current = null
    }
    
    mediaRecorderRef.current = null
    audioChunksRef.current = []
  }

  // æ¸…ç†è³‡æº
  useEffect(() => {
    return () => {
      if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
        mediaRecorderRef.current.stop()
      }
      if (recognitionRef.current) {
        recognitionRef.current.stop()
      }
      stopAudioLevelMonitoring()
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop())
      }
    }
  }, [])

  const handleQuery = async (query: string) => {
    setProcessing(true)
    triggerHaptic('light')

    try {
      // ç¢ºä¿ query æ˜¯ç´”å­—ç¬¦ä¸²
      const safeQuery = typeof query === 'string' ? query.trim() : String(query || '').trim()
      
      if (safeQuery.length === 0) {
        setResponse('âŒ è«‹è¼¸å…¥æœ‰æ•ˆçš„æŸ¥è©¢å…§å®¹')
        setProcessing(false)
        return
      }
      
      // æ§‹å»ºå®‰å…¨çš„è«‹æ±‚é«”
      const requestBody = {
        message: safeQuery,
        conversationHistory: [], // èªéŸ³æŸ¥è©¢ä¸ä½¿ç”¨æ­·å²è¨˜éŒ„
        stream: false,
      }
      
      // é©—è­‰è«‹æ±‚é«”å¯åºåˆ—åŒ–
      let requestBodyString: string
      try {
        requestBodyString = JSON.stringify(requestBody)
      } catch (serializeError) {
        console.error('[VoiceQuickQuery] è«‹æ±‚é«”åºåˆ—åŒ–å¤±æ•—:', serializeError)
        setResponse('âŒ è«‹æ±‚æ ¼å¼éŒ¯èª¤')
        setProcessing(false)
        return
      }
      
      // èª¿ç”¨ AI åŠ©æ‰‹è™•ç†æŸ¥è©¢
      const response = await fetch('/api/ai/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: requestBodyString,
      })

      if (response.ok) {
        const data = await response.json()
        setResponse(data.content || 'è™•ç†å®Œæˆ')

        // å¦‚æœæœ‰æ•¸æ“šçµæœï¼Œå‚³å›çµ¦çˆ¶çµ„ä»¶
        if (data.action && onResult) {
          onResult(query, data.action)
        }
      } else {
        setResponse('âŒ æŸ¥è©¢å¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦')
      }
    } catch (error) {
      console.error('Query error:', error)
      setResponse('âŒ æŸ¥è©¢å¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦')
    } finally {
      setProcessing(false)
    }
  }

  const quickQueries = [
    { label: 'ä»Šæ—¥ç‡Ÿæ”¶', query: 'ä»Šå¤©è³ºäº†å¤šå°‘' },
    { label: 'åº«å­˜æŸ¥è©¢', query: '20kg ç“¦æ–¯é‚„æœ‰å¤šå°‘' },
    { label: 'å¾…é…é€', query: 'é‚„æœ‰å¤šå°‘è¨‚å–®æ²’é€' },
    { label: 'æœ¬æœˆçµ±è¨ˆ', query: 'é€™å€‹æœˆç”Ÿæ„æ€éº¼æ¨£' },
  ]

  return (
    <IOSCard>
      <IOSCardHeader>
        <div className="flex items-center gap-2">
          <Sparkles className="h-6 w-6 text-purple-500" />
          <IOSCardTitle>èªéŸ³åŠ©æ‰‹</IOSCardTitle>
        </div>
      </IOSCardHeader>
      <IOSCardContent className="space-y-4">
        {/* èªéŸ³è¼¸å…¥æŒ‰éˆ• */}
        <div className="flex flex-col items-center gap-3">
          <IOSButton
            size="lg"
            onClick={listening ? stopListening : startListening}
            className={`gap-3 relative ${
              listening
                ? 'bg-red-500 hover:bg-red-600 animate-pulse'
                : 'bg-gradient-to-r from-purple-500 to-pink-500 hover:from-purple-600 hover:to-pink-600'
            }`}
          >
            {listening ? (
              <>
                <StopCircle className="h-6 w-6" />
                åœæ­¢è†è½
              </>
            ) : (
              <>
                <Mic className="h-6 w-6" />
                é»æ“Šèªªè©±
              </>
            )}
          </IOSButton>
          
          {/* éŸ³é‡æŒ‡ç¤ºå™¨ */}
          {listening && (
            <div className="w-full max-w-xs">
              <div className="flex items-center gap-2 mb-1">
                <Waves className="h-4 w-4 text-purple-500" />
                <div className="flex-1 h-2 bg-gray-200 rounded-full overflow-hidden">
                  <div
                    className="h-full bg-gradient-to-r from-purple-400 to-pink-400 transition-all duration-100"
                    style={{ width: `${audioLevel}%` }}
                  />
                </div>
                <span className="text-xs text-gray-500 w-8 text-right">{Math.round(audioLevel)}%</span>
              </div>
              <p className="text-xs text-center text-gray-500">
                {isUsingProfessionalASR ? 'ğŸ¯ ä½¿ç”¨å°ˆæ¥­èªéŸ³è­˜åˆ¥' : 'ğŸŒ ä½¿ç”¨ç€è¦½å™¨èªéŸ³è­˜åˆ¥'}
              </p>
            </div>
          )}
        </div>

        {/* è­˜åˆ¥çµæœ */}
        {transcript && (
          <div className="rounded-xl bg-blue-50 p-4 border-2 border-blue-200 animate-in fade-in slide-in-from-top-2">
            <p className="text-easy-caption text-blue-600 font-semibold mb-1">ğŸ¤ æ‚¨èªªï¼š</p>
            <p className="text-easy-body text-gray-900">{transcript}</p>
            {processing && (
              <p className="text-easy-caption text-blue-500 mt-2 animate-pulse">â³ æ­£åœ¨è™•ç†ä¸­...</p>
            )}
          </div>
        )}

        {/* è™•ç†ä¸­ */}
        {processing && !transcript && (
          <div className="text-center py-4">
            <div className="inline-flex items-center gap-2">
              <div className="w-2 h-2 bg-purple-500 rounded-full animate-bounce" style={{ animationDelay: '0ms' }} />
              <div className="w-2 h-2 bg-purple-500 rounded-full animate-bounce" style={{ animationDelay: '150ms' }} />
              <div className="w-2 h-2 bg-purple-500 rounded-full animate-bounce" style={{ animationDelay: '300ms' }} />
            </div>
            <p className="text-easy-body text-gray-600 mt-2">ğŸ¤” æ­£åœ¨è™•ç†èªéŸ³...</p>
          </div>
        )}

        {/* AI å›æ‡‰ */}
        {response && !processing && (
          <div className="rounded-xl bg-green-50 p-4 border-2 border-green-200">
            <div className="flex items-start gap-2">
              <div className="flex-1">
                <p className="text-easy-caption text-green-600 font-semibold mb-1">ğŸ¤– åŠ©æ‰‹å›å¾©ï¼š</p>
                <p className="text-easy-body text-gray-900 whitespace-pre-wrap">{response}</p>
              </div>
            </div>
          </div>
        )}

        {/* å¿«é€ŸæŸ¥è©¢æŒ‰éˆ• */}
        <div className="pt-4 border-t border-gray-200">
          <p className="text-easy-subheading font-bold text-gray-900 mb-3">ğŸ”¥ ç†±é–€å•é¡Œ</p>
          <div className="grid grid-cols-2 gap-2">
            {quickQueries.map((item) => (
              <button
                key={item.label}
                onClick={() => {
                  triggerHaptic('light')
                  setTranscript(item.query)
                  handleQuery(item.query)
                }}
                className="rounded-lg border-2 border-gray-200 bg-gray-50 p-3 text-center transition-all hover:border-purple-300 hover:bg-purple-50 active:scale-95"
              >
                <p className="text-easy-body font-semibold text-gray-900">{item.label}</p>
              </button>
            ))}
          </div>
        </div>

        {/* ä½¿ç”¨æç¤º */}
        <div className="rounded-lg bg-yellow-50 p-3 border border-yellow-200">
          <p className="text-easy-caption text-gray-700">
            ğŸ’¡ <strong>æç¤ºï¼š</strong>å¯ä»¥è©¢å•ã€Œä»Šå¤©è³ºäº†å¤šå°‘ã€ã€ã€Œåº«å­˜é‚„æœ‰æ²’æœ‰ã€ã€ã€Œé‚„æœ‰å¤šå°‘è¨‚å–®æ²’é€ã€ç­‰å•é¡Œ
          </p>
        </div>
      </IOSCardContent>
    </IOSCard>
  )
}
