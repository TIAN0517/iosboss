'use client'

import { useState, useRef, useEffect } from 'react'
import { IOSButton } from '@/components/ui/ios-button'
import { Mic, Sparkles, X, MessageCircle, Volume2, StopCircle } from 'lucide-react'
import { triggerHaptic } from '@/lib/ios-utils'

interface Message {
  id: string
  role: 'user' | 'assistant'
  content: string
  timestamp: Date
}

interface ImmersiveVoiceAssistantProps {
  onClose?: () => void
}

export function ImmersiveVoiceAssistant({ onClose }: ImmersiveVoiceAssistantProps) {
  const [listening, setListening] = useState(false)
  const [processing, setProcessing] = useState(false)
  const [speaking, setSpeaking] = useState(false)
  const [transcript, setTranscript] = useState('')
  const [interimTranscript, setInterimTranscript] = useState('')
  const [messages, setMessages] = useState<Message[]>([])
  const [audioLevel, setAudioLevel] = useState(0)
  const [isUsingProfessionalASR, setIsUsingProfessionalASR] = useState(false)

  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const streamRef = useRef<MediaStream | null>(null)
  const recognitionRef = useRef<any>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const analyserRef = useRef<AnalyserNode | null>(null)
  const animationFrameRef = useRef<number | null>(null)
  const messagesEndRef = useRef<HTMLDivElement>(null)

  // è‡ªå‹•æ»¾å‹•åˆ°æœ€æ–°æ¶ˆæ¯
  useEffect(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' })
  }, [messages])

  // æª¢æŸ¥å°ˆæ¥­ ASR
  useEffect(() => {
    const checkProfessionalASR = async () => {
      try {
        const res = await fetch('/api/voice/chat')
        const data = await res.json()
        setIsUsingProfessionalASR(data?.services?.deepgram?.available === true)
      } catch {
        setIsUsingProfessionalASR(false)
      }
    }
    checkProfessionalASR()
  }, [])

  // éŸ³é‡ç›£æ¸¬
  const startAudioLevelMonitoring = (stream: MediaStream) => {
    try {
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)()
      const analyser = audioContext.createAnalyser()
      const microphone = audioContext.createMediaStreamSource(stream)

      analyser.fftSize = 256
      microphone.connect(analyser)

      audioContextRef.current = audioContext
      analyserRef.current = analyser

      const dataArray = new Uint8Array(analyser.frequencyBinCount)

      const updateLevel = () => {
        if (!analyserRef.current || !listening) return

        analyserRef.current.getByteFrequencyData(dataArray)
        const average = dataArray.reduce((a, b) => a + b) / dataArray.length
        setAudioLevel(Math.min(100, (average / 255) * 100))

        if (listening) {
          animationFrameRef.current = requestAnimationFrame(updateLevel)
        }
      }

      updateLevel()
    } catch (error) {
      console.warn('[Voice] Audio level monitoring failed:', error)
    }
  }

  const stopAudioLevelMonitoring = () => {
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current)
      animationFrameRef.current = null
    }
    if (audioContextRef.current) {
      audioContextRef.current.close()
      audioContextRef.current = null
    }
    analyserRef.current = null
    setAudioLevel(0)
  }

  // å°ˆæ¥­ ASR éŒ„éŸ³
  const startProfessionalRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 16000,
        }
      })

      streamRef.current = stream
      startAudioLevelMonitoring(stream)

      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus',
      })

      mediaRecorderRef.current = mediaRecorder
      audioChunksRef.current = []

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }

      mediaRecorder.onstop = async () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' })
        await processAudioWithProfessionalASR(audioBlob)
        stopRecording()
      }

      mediaRecorder.start()
    } catch (error: any) {
      console.error('[Voice] Professional recording failed:', error)
      setListening(false)
      addMessage('assistant', `âŒ ç„¡æ³•è¨ªå•éº¥å…‹é¢¨ï¼š${error.message}`)
      stopRecording()
    }
  }

  const processAudioWithProfessionalASR = async (audioBlob: Blob) => {
    setProcessing(true)
    triggerHaptic('light')

    try {
      const formData = new FormData()
      formData.append('audio', audioBlob, 'recording.webm')

      const response = await fetch('/api/voice/chat', {
        method: 'POST',
        body: formData,
      })

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}`)
      }

      const data = await response.json()

      if (!data.transcript || data.transcript.trim().length === 0) {
        addMessage('assistant', 'âŒ ç„¡æ³•è­˜åˆ¥èªéŸ³å…§å®¹ï¼Œè«‹é‡è©¦')
        setProcessing(false)
        return
      }

      // é¡¯ç¤ºç”¨æˆ¶æ¶ˆæ¯
      addMessage('user', data.transcript)

      // é¡¯ç¤º AI å›æ‡‰
      if (data.response) {
        addMessage('assistant', data.response)

        // ä¸è‡ªå‹•æ’­æ”¾èªéŸ³ï¼Œé¿å…æ©Ÿæ¢°æœ—è®€æ„Ÿ
        // if (data.audio && data.audio.data) {
        //   playTTSAudio(data.audio.data, data.audio.mime || 'audio/mpeg')
        // } else {
        //   speakText(data.response)
        // }
      }
    } catch (error: any) {
      console.error('[Voice] èªéŸ³è™•ç†éŒ¯èª¤:', error)
      addMessage('assistant', `âŒ èªéŸ³è™•ç†å¤±æ•—ï¼š${error.message}`)
    } finally {
      setProcessing(false)
    }
  }

  const playTTSAudio = (base64Audio: string, mimeType: string) => {
    try {
      const audio = new Audio(`data:${mimeType};base64,${base64Audio}`)
      setSpeaking(true)

      audio.onended = () => {
        setSpeaking(false)
      }

      audio.onerror = () => {
        setSpeaking(false)
      }

      audio.play().catch(err => {
        console.warn('[Voice] TTS playback failed:', err)
        setSpeaking(false)
      })
    } catch (error) {
      console.warn('[Voice] TTS audio decode failed:', error)
      setSpeaking(false)
    }
  }

  // ç€è¦½å™¨åŸç”ŸèªéŸ³è­˜åˆ¥ï¼ˆé™ç´šï¼‰
  const startBrowserRecognition = () => {
    if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
      setListening(false)
      addMessage('assistant', 'âŒ æ‚¨çš„ç€è¦½å™¨ä¸æ”¯æ´èªéŸ³è¼¸å…¥åŠŸèƒ½')
      return
    }

    const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition
    const recognition = new SpeechRecognition()

    recognition.lang = 'zh-TW'
    recognition.continuous = false
    recognition.interimResults = true
    recognition.maxAlternatives = 1

    recognition.onresult = (event: any) => {
      let interimTranscript = ''
      let finalTranscript = ''

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript
        if (event.results[i].isFinal) {
          finalTranscript += transcript
        } else {
          interimTranscript += transcript
        }
      }

      if (interimTranscript) {
        setInterimTranscript(interimTranscript)
      }

      if (finalTranscript) {
        setTranscript(finalTranscript)
        setInterimTranscript('')
        handleQuery(finalTranscript)
      }
    }

    recognition.onerror = (event: any) => {
      console.error('Speech recognition error:', event.error)
      setListening(false)

      let errorMsg = 'âŒ èªéŸ³è­˜åˆ¥å¤±æ•—'
      switch (event.error) {
        case 'no-speech':
          errorMsg = 'âŒ æ²’æœ‰æª¢æ¸¬åˆ°èªéŸ³ï¼Œè«‹é‡è©¦'
          break
        case 'audio-capture':
          errorMsg = 'âŒ ç„¡æ³•è¨ªå•éº¥å…‹é¢¨ï¼Œè«‹æª¢æŸ¥æ¬Šé™'
          break
        case 'not-allowed':
          errorMsg = 'âŒ éº¥å…‹é¢¨æ¬Šé™è¢«æ‹’çµ•ï¼Œè«‹å…è¨±è¨ªå•'
          break
      }
      addMessage('assistant', errorMsg)
    }

    recognition.onend = () => {
      setListening(false)
    }

    recognitionRef.current = recognition
    recognition.start()

    setTimeout(() => {
      if (listening && recognitionRef.current) {
        recognitionRef.current.stop()
        setListening(false)
      }
    }, 15000)
  }

  const addMessage = (role: 'user' | 'assistant', content: string) => {
    const newMessage: Message = {
      id: Date.now().toString(),
      role,
      content,
      timestamp: new Date()
    }
    setMessages(prev => [...prev, newMessage])
  }

  // æ–‡å­—è½‰èªéŸ³ï¼ˆä½¿ç”¨ç€è¦½å™¨å…§ç½® TTSï¼‰
  const speakText = (text: string) => {
    if (!text || typeof text !== 'string') return

    // åœæ­¢ä¹‹å‰çš„èªéŸ³
    if ('speechSynthesis' in window) {
      speechSynthesis.cancel()

      // ç­‰å¾…ä¸€ä¸‹å†æ’­æ”¾æ–°çš„èªéŸ³
      setTimeout(() => {
        try {
          const utterance = new SpeechSynthesisUtterance(text)

          // è¨­ç½®èªéŸ³åƒæ•¸ - æ›´è‡ªç„¶çš„å°è©±é¢¨æ ¼
          utterance.lang = 'zh-TW' // ç¹é«”ä¸­æ–‡
          utterance.rate = 1.0 // æ­£å¸¸èªé€Ÿ
          utterance.pitch = 1.0 // æ­£å¸¸éŸ³èª¿
          utterance.volume = 1.0 // æœ€å¤§éŸ³é‡

          // å˜—è©¦é¸æ“‡æœ€å¥½çš„èªéŸ³
          const voices = speechSynthesis.getVoices()
          console.log('[Voice] å¯ç”¨èªéŸ³æ•¸é‡:', voices.length)

          // å„ªå…ˆé¸æ“‡è‡ªç„¶çš„èªéŸ³
          const taiwanVoice = voices.find(v =>
            v.lang.includes('zh-TW') &&
            (v.name.includes('Google') || v.name.includes('Microsoft') || v.name.includes('Neural'))
          ) || voices.find(v =>
            v.lang.includes('zh-TW') || v.lang.includes('zh-Hant')
          ) || voices.find(v =>
            v.lang.includes('zh')
          ) || voices.find(v =>
            v.name.includes('Chinese') || v.name.includes('Taiwan')
          )

          if (taiwanVoice) {
            utterance.voice = taiwanVoice
            console.log('[Voice] ä½¿ç”¨èªéŸ³:', taiwanVoice.name, taiwanVoice.lang)
          } else {
            console.log('[Voice] ä½¿ç”¨é è¨­èªéŸ³')
          }

          // æ¨™é»è™•ç† - è®“èªéŸ³æ›´è‡ªç„¶
          // åœ¨ä¸­æ–‡ä¸­ï¼Œå¥å­çµå°¾ç¨å¾®åœé “æœƒæ›´è‡ªç„¶
          const sentences = text.split(/[ã€‚ï¼ï¼Ÿ.!?]/)
          if (sentences.length > 1 && sentences[sentences.length - 1].trim() === '') {
            // å¦‚æœæœ‰æ¨™é»ç¬¦è™Ÿï¼Œè‡ªç„¶åˆ†å‰²å³å¯
            // ä¸éœ€è¦ç‰¹åˆ¥è™•ç†ï¼Œç€è¦½å™¨æœƒè‡ªå‹•è™•ç†
          }

          // äº‹ä»¶ç›£è½
          utterance.onstart = () => {
            setSpeaking(true)
            console.log('[Voice] â–¶ï¸ é–‹å§‹æ’­æ”¾')
          }

          utterance.onend = () => {
            setSpeaking(false)
            console.log('[Voice] âœ… æ’­æ”¾çµæŸ')
          }

          utterance.onerror = (event) => {
            console.error('[Voice] âŒ TTS éŒ¯èª¤:', event)
            setSpeaking(false)
          }

          // æ’­æ”¾èªéŸ³
          speechSynthesis.speak(utterance)
        } catch (error) {
          console.error('[Voice] TTS æ’­æ”¾å¤±æ•—:', error)
          setSpeaking(false)
        }
      }, 100)
    }
  }

  const handleQuery = async (query: string) => {
    setProcessing(true)
    triggerHaptic('light')

    try {
      const response = await fetch('/api/ai/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          message: query,
          conversationHistory: messages.map(m => ({ role: m.role, content: m.content })),
          stream: false,
        }),
      })

      if (response.ok) {
        const data = await response.json()
        const aiResponse = data.content || 'è™•ç†å®Œæˆ'

        addMessage('assistant', aiResponse)

        // ä¸è‡ªå‹•æ’­æ”¾èªéŸ³ï¼Œè®“ç”¨æˆ¶é¸æ“‡æ˜¯å¦æ’­æ”¾
        // speakText(aiResponse)
      } else {
        const errorMsg = 'âŒ æŸ¥è©¢å¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦'
        addMessage('assistant', errorMsg)
      }
    } catch (error) {
      console.error('Query error:', error)
      addMessage('assistant', 'âŒ æŸ¥è©¢å¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦')
    } finally {
      setProcessing(false)
    }
  }

  const startListening = async () => {
    triggerHaptic('light')
    setListening(true)
    setTranscript('')
    setInterimTranscript('')

    if (isUsingProfessionalASR) {
      await startProfessionalRecording()
    } else {
      startBrowserRecognition()
    }
  }

  const stopListening = () => {
    triggerHaptic('medium')

    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop()
    }

    if (recognitionRef.current) {
      recognitionRef.current.stop()
      recognitionRef.current = null
    }

    stopRecording()
  }

  const stopRecording = () => {
    setListening(false)
    stopAudioLevelMonitoring()

    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop())
      streamRef.current = null
    }

    mediaRecorderRef.current = null
    audioChunksRef.current = []
  }

  const stopSpeaking = () => {
    if ('speechSynthesis' in window) {
      speechSynthesis.cancel()
    }
    setSpeaking(false)
    triggerHaptic('light')
  }

  // æ¸…ç†è³‡æº
  useEffect(() => {
    return () => {
      if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
        mediaRecorderRef.current.stop()
      }
      if (recognitionRef.current) {
        recognitionRef.current.stop()
      }
      stopAudioLevelMonitoring()
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop())
      }
      if ('speechSynthesis' in window) {
        speechSynthesis.cancel()
      }
    }
  }, [])

  return (
    <div className="fixed inset-0 bg-gradient-to-b from-slate-900 via-purple-900 to-slate-900 flex flex-col items-center justify-center p-6 overflow-hidden">
      {/* é—œé–‰æŒ‰éˆ• */}
      <button
        onClick={() => {
          triggerHaptic('medium')
          onClose?.()
        }}
        className="absolute top-6 right-6 p-3 rounded-full bg-white/10 backdrop-blur-sm hover:bg-white/20 transition-all"
      >
        <X className="h-6 w-6 text-white" />
      </button>

      {/* æ¨™é¡Œ */}
      <div className="absolute top-6 left-6 flex items-center gap-3">
        <div className="p-3 rounded-full bg-gradient-to-r from-purple-500 to-pink-500">
          <Sparkles className="h-6 w-6 text-white" />
        </div>
        <div>
          <h1 className="text-xl font-bold text-white">AI èªéŸ³åŠ©æ‰‹</h1>
          <p className="text-sm text-white/60">éš¨æ™‚æº–å‚™ç‚ºæ‚¨æœå‹™</p>
        </div>
      </div>

      {/* å°è©±æ­·å² */}
      {messages.length > 0 && (
        <div className="absolute inset-x-0 top-24 bottom-48 overflow-y-auto px-6 space-y-4">
          {messages.map((message) => (
            <div
              key={message.id}
              className={`flex ${message.role === 'user' ? 'justify-end' : 'justify-start'}`}
            >
              <div
                className={`max-w-[80%] rounded-2xl px-4 py-3 ${
                  message.role === 'user'
                    ? 'bg-gradient-to-r from-purple-500 to-pink-500 text-white'
                    : 'bg-white/10 backdrop-blur-sm text-white'
                }`}
              >
                <p className="text-base whitespace-pre-wrap">{message.content}</p>
                <p className="text-xs opacity-60 mt-1">
                  {message.timestamp.toLocaleTimeString('zh-TW', { hour: '2-digit', minute: '2-digit' })}
                </p>
              </div>
            </div>
          ))}
          <div ref={messagesEndRef} />
        </div>
      )}

      {/* å¯¦æ™‚è­˜åˆ¥æ–‡å­— */}
      {(transcript || interimTranscript) && (
        <div className="absolute top-1/3 left-1/2 -translate-x-1/2 -translate-y-1/2 w-full max-w-lg px-6">
          <div className="bg-white/10 backdrop-blur-sm rounded-2xl p-6 border border-white/20">
            <p className="text-center text-white text-xl font-medium">
              {transcript || interimTranscript}
            </p>
            {processing && (
              <div className="flex justify-center mt-4 gap-2">
                <div className="w-2 h-2 bg-white rounded-full animate-bounce" style={{ animationDelay: '0ms' }} />
                <div className="w-2 h-2 bg-white rounded-full animate-bounce" style={{ animationDelay: '150ms' }} />
                <div className="w-2 h-2 bg-white rounded-full animate-bounce" style={{ animationDelay: '300ms' }} />
              </div>
            )}
          </div>
        </div>
      )}

      {/* ä¸­å¤®èªéŸ³æŒ‰éˆ• */}
      <div className="relative flex flex-col items-center">
        {/* æ³¢ç´‹å‹•ç•« */}
        {listening && (
          <>
            <div className="absolute inset-0 rounded-full bg-purple-500/30 animate-ping" style={{ animationDuration: '1s' }} />
            <div className="absolute inset-0 rounded-full bg-purple-500/20 animate-ping" style={{ animationDuration: '1.5s', animationDelay: '0.2s' }} />
          </>
        )}

        {/* éŸ³é‡æ³¢å½¢ */}
        {listening && (
          <div className="absolute -inset-8">
            {Array.from({ length: 8 }).map((_, i) => {
              const size = 80 + audioLevel * 0.8 + Math.sin(Date.now() / 100 + i) * 10
              return (
                <div
                  key={i}
                  className="absolute inset-0 rounded-full border-2 border-purple-400/30"
                  style={{
                    width: `${size}px`,
                    height: `${size}px`,
                    transform: 'translate(-50%, -50%)',
                    animation: `ripple 1.5s ease-out ${i * 0.15}s infinite`,
                  }}
                />
              )
            })}
          </div>
        )}

        {/* ä¸»æŒ‰éˆ• */}
        <button
          onClick={listening ? stopListening : startListening}
          disabled={processing || speaking}
          className={`relative z-10 w-32 h-32 rounded-full flex items-center justify-center transition-all duration-300 ${
            listening
              ? 'bg-gradient-to-r from-red-500 to-orange-500 shadow-lg shadow-red-500/50 scale-110'
              : 'bg-gradient-to-r from-purple-500 to-pink-500 shadow-lg shadow-purple-500/50 hover:scale-105 active:scale-95'
          } ${processing || speaking ? 'opacity-50 cursor-not-allowed' : ''}`}
        >
          {listening ? (
            <>
              <StopCircle className="h-16 w-16 text-white" />
              <div className="absolute inset-0 rounded-full border-4 border-white/30 animate-pulse" />
            </>
          ) : processing ? (
            <div className="flex items-center gap-3">
              <div className="w-4 h-4 bg-white rounded-full animate-bounce" style={{ animationDelay: '0ms' }} />
              <div className="w-4 h-4 bg-white rounded-full animate-bounce" style={{ animationDelay: '150ms' }} />
              <div className="w-4 h-4 bg-white rounded-full animate-bounce" style={{ animationDelay: '300ms' }} />
            </div>
          ) : speaking ? (
            <Volume2 className="h-16 w-16 text-white animate-pulse" />
          ) : (
            <Mic className="h-16 w-16 text-white" />
          )}
        </button>

        {/* ç‹€æ…‹æ–‡å­— */}
        <div className="mt-8 text-center">
          <p className="text-white text-lg font-medium">
            {listening ? 'æ­£åœ¨è†è½...' : processing ? 'æ­£åœ¨æ€è€ƒ...' : speaking ? 'æ­£åœ¨èªªè©±...' : 'é»æ“Šé–‹å§‹èªªè©±'}
          </p>
          {listening && isUsingProfessionalASR && (
            <p className="text-white/60 text-sm mt-2">ğŸ¯ å°ˆæ¥­èªéŸ³è­˜åˆ¥</p>
          )}
        </div>

        {/* åœæ­¢èªªè©±æŒ‰éˆ• */}
        {speaking && (
          <button
            onClick={stopSpeaking}
            className="mt-4 px-6 py-2 rounded-full bg-white/10 backdrop-blur-sm text-white hover:bg-white/20 transition-all flex items-center gap-2"
          >
            <StopCircle className="h-4 w-4" />
            åœæ­¢æ’­æ”¾
          </button>
        )}
      </div>

      {/* æç¤ºæ–‡å­— */}
      {messages.length === 0 && !listening && !processing && (
        <div className="absolute bottom-32 text-center">
          <p className="text-white/60 text-sm">æ‚¨å¯ä»¥èªªï¼š</p>
          <div className="mt-3 flex flex-wrap justify-center gap-2">
            {['ä»Šå¤©è³ºäº†å¤šå°‘', 'åº«å­˜é‚„æœ‰æ²’æœ‰', 'é‚„æœ‰å¤šå°‘è¨‚å–®æ²’é€'].map((query) => (
              <button
                key={query}
                onClick={() => {
                  triggerHaptic('light')
                  setTranscript(query)
                  handleQuery(query)
                }}
                className="px-4 py-2 rounded-full bg-white/10 backdrop-blur-sm text-white text-sm hover:bg-white/20 transition-all"
              >
                {query}
              </button>
            ))}
          </div>
        </div>
      )}

      <style jsx>{`
        @keyframes ripple {
          0% {
            transform: scale(1);
            opacity: 1;
          }
          100% {
            transform: scale(1.5);
            opacity: 0;
          }
        }
      `}</style>
    </div>
  )
}
