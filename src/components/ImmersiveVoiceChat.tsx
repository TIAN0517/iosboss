'use client'

/**
 * æ²‰æµ¸å¼å…¨å±èªéŸ³å°è©±çµ„ä»¶ï¼ˆæœå‹™ç«¯ç‰ˆï¼‰
 * æ‰€æœ‰èªéŸ³è™•ç†åœ¨ Docker ä¸­å®Œæˆï¼š
 * - Deepgram ASRï¼ˆèªéŸ³è½‰æ–‡å­—ï¼‰
 * - GLM AIï¼ˆç”Ÿæˆå›æ‡‰ï¼‰
 * - ElevenLabs/Azure TTSï¼ˆæ–‡å­—è½‰èªéŸ³ï¼‰
 *
 * æ ¸å¿ƒç‰¹æ€§ï¼š
 * - å…¨å±æ²‰æµ¸å¼ UI
 * - éº¥å…‹é¢¨éŒ„éŸ³ä¸Šå‚³
 * - æœå‹™ç«¯å°ˆæ¥­éŸ³é »è¿”å›
 * - è‡ªå‹•å¾ªç’°å°è©±
 * - éŸ³é »æ³¢å½¢å¯è¦–åŒ–
 */

import { useState, useRef, useEffect, useCallback } from 'react'
import { X, Mic, Sparkles, Volume2 } from 'lucide-react'
import { triggerHaptic } from '@/lib/ios-utils'

// ========================================
// é¡å‹å®šç¾©
// ========================================

type ChatState = 'idle' | 'recording' | 'processing' | 'playing' | 'error'

interface VoiceMessage {
  id: string
  role: 'user' | 'assistant'
  text: string
  timestamp: number
  audioUrl?: string  // TTS éŸ³é » URL
}

// ========================================
// çµ„ä»¶
// ========================================

interface ImmersiveVoiceChatProps {
  onClose?: () => void
  initialMessage?: string
}

export function ImmersiveVoiceChat({ onClose, initialMessage }: ImmersiveVoiceChatProps) {
  // ç‹€æ…‹
  const [chatState, setChatState] = useState<ChatState>('idle')
  const [messages, setMessages] = useState<VoiceMessage[]>([])
  const [currentTranscript, setCurrentTranscript] = useState('')
  const [currentResponse, setCurrentResponse] = useState('')
  const [audioLevel, setAudioLevel] = useState(0)
  const [servicesReady, setServicesReady] = useState(false)

  // Refs
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const streamRef = useRef<MediaStream | null>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const analyserRef = useRef<AnalyserNode | null>(null)
  const animationFrameRef = useRef<number | null>(null)
  const currentAudioRef = useRef<HTMLAudioElement | null>(null)
  const currentSessionRef = useRef<number>(0) // ç”¨æ–¼ä¸­æ–·ç•¶å‰æœƒè©±

  // éŸ³é »æ³¢å½¢æ•¸æ“š
  const [waveformData, setWaveformData] = useState<number[]>([])

  // ========================================
  // éŸ³é »æ³¢å½¢å¯è¦–åŒ–
  // ========================================

  const startWaveformVisualization = useCallback((stream: MediaStream) => {
    try {
      const AudioContext = window.AudioContext || (window as any).webkitAudioContext
      const audioContext = new AudioContext()
      const analyser = audioContext.createAnalyser()
      const source = audioContext.createMediaStreamSource(stream)

      analyser.fftSize = 256
      source.connect(analyser)

      audioContextRef.current = audioContext
      analyserRef.current = analyser

      const dataArray = new Uint8Array(analyser.frequencyBinCount)

      const updateWaveform = () => {
        if (!analyserRef.current || chatState !== 'recording') {
          return
        }

        analyserRef.current.getByteFrequencyData(dataArray)

        // è¨ˆç®—éŸ³é‡ç­‰ç´š
        const average = dataArray.reduce((a, b) => a + b, 0) / dataArray.length
        setAudioLevel(Math.min(100, (average / 128) * 100))

        // è¨­ç½®æ³¢å½¢æ•¸æ“š
        setWaveformData(Array.from(dataArray))

        if (chatState === 'recording') {
          animationFrameRef.current = requestAnimationFrame(updateWaveform)
        }
      }

      updateWaveform()
    } catch (error) {
      console.warn('[Waveform] Visualization failed:', error)
    }
  }, [chatState])

  const stopWaveformVisualization = useCallback(() => {
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current)
      animationFrameRef.current = null
    }
    if (audioContextRef.current) {
      audioContextRef.current.close()
      audioContextRef.current = null
    }
    analyserRef.current = null
    setAudioLevel(0)
    setWaveformData([])
  }, [])

  // ========================================
  // éŒ„éŸ³è™•ç†
  // ========================================

  const startRecording = useCallback(async () => {
    try {
      console.log('[Recording] è«‹æ±‚éº¥å…‹é¢¨...')

      // æª¢æŸ¥ç€è¦½å™¨æ”¯æ´
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        throw new Error('ç€è¦½å™¨ä¸æ”¯æ´éº¥å…‹é¢¨åŠŸèƒ½')
      }

      // åˆ—å‡ºå¯ç”¨çš„éŸ³é »è¨­å‚™
      const devices = await navigator.mediaDevices.enumerateDevices()
      const audioInputs = devices.filter(d => d.kind === 'audioinput')
      console.log('[Recording] å¯ç”¨éº¥å…‹é¢¨:', audioInputs.map(d => d.label || d.deviceId))

      if (audioInputs.length === 0) {
        throw new Error('æ²’æœ‰æª¢æ¸¬åˆ°éº¥å…‹é¢¨è¨­å‚™')
      }

      // è«‹æ±‚éº¥å…‹é¢¨
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 16000,
          channelCount: 1,
        }
      })

      console.log('[Recording] éº¥å…‹é¢¨å·²ç²å–:', stream.getAudioTracks().map(t => ({
        label: t.label,
        enabled: t.enabled,
        muted: t.muted,
      })))

      streamRef.current = stream
      startWaveformVisualization(stream)

      // å‰µå»ºéŒ„éŸ³å™¨
      let mimeType = 'audio/webm;codecs=opus'
      if (!MediaRecorder.isTypeSupported(mimeType)) {
        mimeType = 'audio/webm'
        console.log('[Recording] ä¸æ”¯æ´ opusï¼Œä½¿ç”¨ audio/webm')
      }
      if (!MediaRecorder.isTypeSupported(mimeType)) {
        mimeType = 'audio/mp4'
        console.log('[Recording] ä¸æ”¯æ´ webmï¼Œä½¿ç”¨ audio/mp4')
      }

      const mediaRecorder = new MediaRecorder(stream, { mimeType })

      mediaRecorderRef.current = mediaRecorder
      audioChunksRef.current = []

      mediaRecorder.ondataavailable = (event) => {
        console.log('[Recording] æ”¶åˆ°éŸ³é »æ•¸æ“š:', {
          size: event.data.size,
          type: event.data.type,
        })
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }

      mediaRecorder.onstop = async () => {
        console.log('[Recording] éŒ„éŸ³å·²åœæ­¢ï¼Œè™•ç†éŸ³é »...')
        const audioBlob = new Blob(audioChunksRef.current, { type: mimeType })
        console.log('[Recording] éŸ³é » Blob:', {
          size: audioBlob.size,
          type: audioBlob.type,
        })
        await processAudio(audioBlob)
      }

      mediaRecorder.start(100) // æ¯ 100ms è§¸ç™¼ä¸€æ¬¡ ondataavailable
      setChatState('recording')
      triggerHaptic('medium')

      console.log('[Recording] éŒ„éŸ³å·²é–‹å§‹')

      // 10 ç§’å¾Œè‡ªå‹•åœæ­¢
      setTimeout(() => {
        if (chatState === 'recording') {
          console.log('[Recording] é”åˆ°æœ€å¤§æ™‚é•·ï¼Œè‡ªå‹•åœæ­¢')
          stopRecording()
        }
      }, 10000)

    } catch (error: any) {
      console.error('[Recording] Failed:', error)

      let errorMsg = 'ç„¡æ³•è¨ªå•éº¥å…‹é¢¨'
      if (error.name === 'NotAllowedError' || error.name === 'PermissionDeniedError') {
        errorMsg = 'éº¥å…‹é¢¨æ¬Šé™è¢«æ‹’çµ•ï¼Œè«‹åœ¨ç€è¦½å™¨è¨­ç½®ä¸­å…è¨±éº¥å…‹é¢¨æ¬Šé™'
      } else if (error.name === 'NotFoundError') {
        errorMsg = 'æ²’æœ‰æª¢æ¸¬åˆ°éº¥å…‹é¢¨è¨­å‚™ï¼Œè«‹ç¢ºèªå·²é€£æ¥éº¥å…‹é¢¨'
      } else if (error.name === 'NotReadableError') {
        errorMsg = 'éº¥å…‹é¢¨è¢«å…¶ä»–æ‡‰ç”¨ä½”ç”¨ï¼Œè«‹é—œé–‰å…¶ä»–æ‡‰ç”¨å¾Œé‡è©¦'
      } else {
        errorMsg = `éº¥å…‹é¢¨éŒ¯èª¤ï¼š${error.message}`
      }

      setChatState('error')
      setCurrentTranscript(`âŒ ${errorMsg}`)
    }
  }, [startWaveformVisualization, chatState])

  const stopRecording = useCallback(() => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop()
    }
    stopWaveformVisualization()
    triggerHaptic('light')
  }, [stopWaveformVisualization])

  // ========================================
  // ç€è¦½å™¨ TTS æ’­æ”¾ï¼ˆé™ç´šæ–¹æ¡ˆï¼‰
  // ========================================

  const playBrowserTTS = useCallback(async (text: string): Promise<void> => {
    if (!('speechSynthesis' in window)) {
      console.warn('[Browser TTS] ç€è¦½å™¨ä¸æ”¯æ´èªéŸ³åˆæˆ')
      return
    }

    console.log('[Browser TTS] é–‹å§‹æ’­æ”¾:', text.substring(0, 50))

    return new Promise<void>((resolve, reject) => {
      // åœæ­¢ç•¶å‰æ’­æ”¾
      window.speechSynthesis.cancel()

      const utterance = new SpeechSynthesisUtterance(text)

      // è¨­ç½®èªéŸ³åƒæ•¸
      utterance.lang = 'zh-TW'
      utterance.rate = 1.1
      utterance.pitch = 1.05
      utterance.volume = 1.0

      // ç²å–æœ€ä½³èªéŸ³
      const voices = window.speechSynthesis.getVoices()
      const chineseVoice = voices.find(v =>
        v.lang.includes('zh') && (v.name.includes('Female') || v.name.includes('Neural') || v.name.includes('Google'))
      ) || voices.find(v => v.lang.includes('zh'))

      if (chineseVoice) {
        utterance.voice = chineseVoice
        console.log('[Browser TTS] ä½¿ç”¨èªéŸ³:', chineseVoice.name)
      }

      utterance.onstart = () => {
        console.log('[Browser TTS] é–‹å§‹èªªè©±')
        setChatState('playing')
      }

      utterance.onend = () => {
        console.log('[Browser TTS] æ’­æ”¾å®Œæˆ')
        setChatState('idle')
        resolve()
      }

      utterance.onerror = (e) => {
        console.error('[Browser TTS] æ’­æ”¾éŒ¯èª¤:', e)
        setChatState('idle')
        reject(e)
      }

      window.speechSynthesis.speak(utterance)
    })
  }, [])

  // ========================================
  // TTS éŸ³é »æ’­æ”¾
  // ========================================

  const playTTSAudio = useCallback(async (base64Audio: string, mimeType: string): Promise<void> => {
    console.log('[TTS] é–‹å§‹æ’­æ”¾éŸ³é »:', {
      mimeType,
      dataLength: base64Audio?.length,
      previewSize: base64Audio?.substring(0, 50),
    })

    return new Promise((resolve, reject) => {
      try {
        // åœæ­¢ç•¶å‰æ’­æ”¾
        if (currentAudioRef.current) {
          currentAudioRef.current.pause()
          currentAudioRef.current = null
        }

        // å‰µå»ºéŸ³é »å…ƒç´ 
        const audioSrc = `data:${mimeType};base64,${base64Audio}`
        console.log('[TTS] éŸ³é » URL é•·åº¦:', audioSrc.length)

        const audio = new Audio(audioSrc)
        currentAudioRef.current = audio

        // è¨­ç½®éŸ³é‡
        audio.volume = 1.0

        setChatState('playing')
        console.log('[TTS] é–‹å§‹æ’­æ”¾...')

        // æ·»åŠ åŠ è¼‰ç›£è½
        audio.onloadedmetadata = () => {
          console.log('[TTS] éŸ³é »å…ƒæ•¸æ“šåŠ è¼‰å®Œæˆ:', {
            duration: audio.duration,
          })
        }

        audio.oncanplay = () => {
          console.log('[TTS] éŸ³é »å¯ä»¥æ’­æ”¾')
        }

        audio.onended = () => {
          console.log('[TTS] æ’­æ”¾å®Œæˆ')
          setChatState('idle')
          resolve()
        }

        audio.onerror = (e) => {
          console.error('[TTS] æ’­æ”¾éŒ¯èª¤:', {
            error: audio.error,
            code: audio.error?.code,
            message: audio.error?.message,
          })
          setChatState('idle')
          reject(new Error(`éŸ³é »æ’­æ”¾å¤±æ•—: ${audio.error?.message || 'æœªçŸ¥éŒ¯èª¤'}`))
        }

        // å˜—è©¦æ’­æ”¾
        const playPromise = audio.play()

        if (playPromise) {
          playPromise
            .then(() => {
              console.log('[TTS] æ’­æ”¾æˆåŠŸå•Ÿå‹•')
            })
            .catch((err) => {
              console.error('[TTS] play() è¢«æ‹’çµ•:', err)
              // ç€è¦½å™¨å¯èƒ½é˜»æ­¢äº†è‡ªå‹•æ’­æ”¾ï¼Œéœ€è¦ç”¨æˆ¶äº¤äº’
              setChatState('idle')
              reject(new Error('ç€è¦½å™¨é˜»æ­¢äº†è‡ªå‹•æ’­æ”¾ï¼Œè«‹é»æ“Šæ’­æ”¾æŒ‰éˆ•'))
            })
        }

      } catch (error) {
        console.error('[TTS] å‰µå»ºéŸ³é »å¤±æ•—:', error)
        reject(error)
      }
    })
  }, [])

  // ========================================
  // è™•ç†éŸ³é »ï¼ˆæœå‹™ç«¯æµå¼è™•ç†ï¼‰
  // ========================================

  const processAudio = useCallback(async (audioBlob: Blob) => {
    setChatState('processing')
    setCurrentTranscript('ğŸ¤ æ­£åœ¨è­˜åˆ¥...')
    setCurrentResponse('')
    triggerHaptic('light')

    // æ¨™è¨˜ç•¶å‰æœƒè©± IDï¼ˆç”¨æ–¼ä¸­æ–·æª¢æ¸¬ï¼‰
    const sessionId = Date.now()
    currentSessionRef.current = sessionId

    try {
      // æº–å‚™è¡¨å–®æ•¸æ“š
      const formData = new FormData()
      formData.append('audio', audioBlob, 'recording.webm')
      formData.append('conversationHistory', JSON.stringify(
        messages.map(m => ({ role: m.role, content: m.text }))
      ))

      console.log('[Voice] ç™¼é€åˆ°æœå‹™ç«¯ï¼ˆæµå¼ï¼‰...')

      // ä½¿ç”¨æµå¼ API
      const response = await fetch('/api/voice/stream', {
        method: 'POST',
        body: formData,
      })

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}))
        throw new Error(errorData.error || `HTTP ${response.status}`)
      }

      // è™•ç† SSE æµ
      const reader = response.body?.getReader()
      const decoder = new TextDecoder()

      if (!reader) {
        throw new Error('ç„¡æ³•è®€å–éŸ¿æ‡‰æµ')
      }

      let userText = ''
      let assistantText = ''
      let audioChunks: string[] = []
      let audioMimeType = 'audio/mpeg'
      let buffer = ''
      let ttsStarted = false
      let lastSentence = ''

      while (true) {
        // æª¢æŸ¥æ˜¯å¦è¢«ä¸­æ–·
        if (currentSessionRef.current !== sessionId) {
          console.log('[Voice] æœƒè©±å·²ä¸­æ–·')
          break
        }

        const { done, value } = await reader.read()

        if (done) break

        // è§£ç¢¼ä¸¦è™•ç†æ•¸æ“š
        buffer += decoder.decode(value, { stream: true })
        const lines = buffer.split('\n')
        buffer = lines.pop() || '' // ä¿ç•™ä¸å®Œæ•´çš„è¡Œ

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            try {
              const event = JSON.parse(line.slice(6))

              switch (event.type) {
                case 'transcript':
                  // èªéŸ³è­˜åˆ¥å®Œæˆ
                  userText = event.data
                  setCurrentTranscript(userText)

                  // æ·»åŠ ç”¨æˆ¶è¨Šæ¯
                  const userMessage: VoiceMessage = {
                    id: Date.now().toString(),
                    role: 'user',
                    text: userText,
                    timestamp: Date.now(),
                  }
                  setMessages(prev => [...prev, userMessage])
                  break

                case 'text':
                  // AI æµå¼æ–‡æœ¬ - å³æ™‚æ’­æ”¾ï¼
                  if (!event.data.includes('ğŸ¤') && !event.data.includes('ğŸ¤”') && !event.data.includes('ğŸ”Š')) {
                    lastSentence += event.data
                    assistantText += event.data
                    setCurrentResponse(assistantText)

                    // æª¢æŸ¥æ˜¯å¦å®Œæˆä¸€å¥è©±ï¼ˆé‡åˆ°å¥è™Ÿã€å•è™Ÿã€å˜†è™Ÿï¼‰
                    if ((event.data.includes('ã€‚') || event.data.includes('ï¼Ÿ') || event.data.includes('ï¼') || event.data.includes('\n')) && !ttsStarted) {
                      ttsStarted = true
                      // ç«‹å³é–‹å§‹ç€è¦½å™¨ TTSï¼ˆä¸ç­‰å…¨éƒ¨ç”Ÿæˆå®Œï¼‰
                      console.log('[Voice] ç«‹å³æ’­æ”¾å¥å­:', lastSentence)
                      setChatState('playing')
                      playBrowserTTS(lastSentence).then(() => {
                        setChatState('idle')
                      }).catch(() => {
                        setChatState('idle')
                      })
                    }
                  }
                  break

                case 'audio':
                  // éŸ³é »æ•¸æ“šå¡Šï¼ˆå¾Œå°ä¸‹è¼‰ï¼Œä¸ç«‹å³æ’­æ”¾ï¼‰
                  audioChunks.push(event.data)
                  audioMimeType = event.mimeType
                  break

                case 'error':
                  throw new Error(event.data)
              }
            } catch (e) {
              console.warn('[Voice] SSE è§£æéŒ¯èª¤:', e)
            }
          }
        }
      }

      // å¦‚æœé‚„æ²’é–‹å§‹ TTSï¼ˆæ²’æœ‰æ¨™é»ç¬¦è™Ÿï¼‰ï¼Œç¾åœ¨æ’­æ”¾å®Œæ•´æ–‡å­—
      if (!ttsStarted && assistantText && currentSessionRef.current === sessionId) {
        await playBrowserTTS(assistantText)
      }

      // æ·»åŠ  AI è¨Šæ¯
      if (currentSessionRef.current === sessionId) {
        const assistantMessage: VoiceMessage = {
          id: (Date.now() + 1).toString(),
          role: 'assistant',
          text: assistantText,
          timestamp: Date.now(),
          audioUrl: audioChunks.length > 0 ? `data:${audioMimeType};base64,${audioChunks.join('')}` : undefined,
        }
        setMessages(prev => [...prev, assistantMessage])

        // è‡ªå‹•é‡æ–°é–‹å§‹éŒ„éŸ³ï¼ˆå¾ªç’°å°è©±ï¼‰
        setTimeout(() => {
          if (currentSessionRef.current === sessionId) {
            setChatState('idle')
            setCurrentTranscript('')
            setCurrentResponse('')
          }
        }, 1000)
      }

    } catch (error: any) {
      if (currentSessionRef.current === sessionId) {
        console.error('[Voice] è™•ç†å¤±æ•—:', error)
        setChatState('error')
        setCurrentTranscript(`âŒ è™•ç†å¤±æ•—ï¼š${error.message}`)
      }
    } finally {
      // æ¸…ç†éŸ³é »è»Œé“
      if (streamRef.current && currentSessionRef.current === sessionId) {
        streamRef.current.getTracks().forEach(track => track.stop())
        streamRef.current = null
      }
    }
  }, [messages, startWaveformVisualization, stopWaveformVisualization, playBrowserTTS, playTTSAudio])

  // ========================================
  // æ§åˆ¶å‡½æ•¸
  // ========================================

  const toggleRecording = useCallback(() => {
    // åœæ­¢ç•¶å‰æ’­æ”¾
    if (currentAudioRef.current) {
      currentAudioRef.current.pause()
      currentAudioRef.current = null
    }

    // å¦‚æœæ­£åœ¨è™•ç†æˆ–æ’­æ”¾ï¼Œä¸­æ–·ä¸¦é‡æ–°é–‹å§‹éŒ„éŸ³
    if (chatState === 'processing' || chatState === 'playing') {
      console.log('[Voice] ç”¨æˆ¶ä¸­æ–·')
      // æ”¹è®Š session ID ä»¥ä¸­æ–·ç•¶å‰æµ
      currentSessionRef.current = Date.now()
      // åœæ­¢éŒ„éŸ³
      if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
        mediaRecorderRef.current.stop()
      }
      // ç«‹å³é–‹å§‹æ–°çš„éŒ„éŸ³
      setChatState('idle')
      setTimeout(() => startRecording(), 100)
      return
    }

    if (chatState === 'recording') {
      stopRecording()
    } else if (chatState === 'idle' || chatState === 'error') {
      startRecording()
    }
  }, [chatState, startRecording, stopRecording])

  // ========================================
  // åˆå§‹åŒ–
  // ========================================

  useEffect(() => {
    // æª¢æŸ¥æœå‹™ç‹€æ…‹
    const checkServices = async () => {
      // #region agent log
      fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'ImmersiveVoiceChat.tsx:588',message:'æª¢æŸ¥èªéŸ³æœå‹™ç‹€æ…‹',data:{url:'/api/voice/chat'},timestamp:Date.now(),sessionId:'debug-session',runId:'voice-check',hypothesisId:'C'})}).catch(()=>{});
      // #endregion
      try {
        const res = await fetch('/api/voice/chat')
        // #region agent log
        fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'ImmersiveVoiceChat.tsx:591',message:'èªéŸ³æœå‹™éŸ¿æ‡‰',data:{status:res.status,ok:res.ok},timestamp:Date.now(),sessionId:'debug-session',runId:'voice-check',hypothesisId:'C'})}).catch(()=>{});
        // #endregion
        const data = await res.json()
        // #region agent log
        fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'ImmersiveVoiceChat.tsx:592',message:'èªéŸ³æœå‹™æ•¸æ“š',data:{hasServices:!!data.services,deepgram:data.services?.deepgram,azure:data.services?.azure,elevenlabs:data.services?.elevenlabs},timestamp:Date.now(),sessionId:'debug-session',runId:'voice-check',hypothesisId:'C'})}).catch(()=>{});
        // #endregion
        setServicesReady(data.services?.deepgram || false)
        console.log('[Voice] æœå‹™ç‹€æ…‹:', data.services)
      } catch (error) {
        // #region agent log
        fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'ImmersiveVoiceChat.tsx:595',message:'èªéŸ³æœå‹™æª¢æŸ¥å¤±æ•—',data:{errorMessage:error instanceof Error ? error.message : String(error)},timestamp:Date.now(),sessionId:'debug-session',runId:'voice-check',hypothesisId:'C'})}).catch(()=>{});
        // #endregion
        console.warn('[Voice] ç„¡æ³•æª¢æŸ¥æœå‹™ç‹€æ…‹:', error)
      }
    }

    checkServices()

    // ç™¼é€åˆå§‹æ­¡è¿è¨Šæ¯
    if (initialMessage) {
      const welcomeMsg: VoiceMessage = {
        id: 'welcome',
        role: 'assistant',
        text: initialMessage,
        timestamp: Date.now(),
      }
      setMessages([welcomeMsg])
    }

    return () => {
      // æ¸…ç†
      stopWaveformVisualization()
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(t => t.stop())
      }
      if (currentAudioRef.current) {
        currentAudioRef.current.pause()
      }
      // åœæ­¢ç€è¦½å™¨èªéŸ³åˆæˆ
      if ('speechSynthesis' in window) {
        window.speechSynthesis.cancel()
      }
    }
  }, [])

  // ========================================
  // æ¸²æŸ“
  // ========================================

  const getStatusText = () => {
    switch (chatState) {
      case 'idle': return servicesReady ? 'é»æ“Šéº¥å…‹é¢¨é–‹å§‹' : 'æœå‹™åˆå§‹åŒ–ä¸­...'
      case 'recording': return 'æ­£åœ¨éŒ„éŸ³...'
      case 'processing': return 'æ­£åœ¨è™•ç†...'
      case 'playing': return 'æ­£åœ¨æ’­æ”¾...'
      case 'error': return 'å‡ºéŒ¯äº†'
    }
  }

  const getStatusColor = () => {
    switch (chatState) {
      case 'idle': return 'bg-gray-400'
      case 'recording': return 'bg-red-500 animate-pulse'
      case 'processing': return 'bg-blue-500 animate-pulse'
      case 'playing': return 'bg-green-500 animate-pulse'
      case 'error': return 'bg-orange-500'
    }
  }

  return (
    <div className="fixed inset-0 bg-gradient-to-b from-purple-600 via-purple-700 to-indigo-800 z-50 flex flex-col">
      {/* é ‚éƒ¨é—œé–‰æŒ‰éˆ• */}
      <div className="absolute top-0 left-0 right-0 p-4 flex justify-between items-start safe-area-top">
        <button
          onClick={() => {
            triggerHaptic('light')
            // åœæ­¢æ‰€æœ‰èªéŸ³
            if ('speechSynthesis' in window) {
              window.speechSynthesis.cancel()
            }
            if (currentAudioRef.current) {
              currentAudioRef.current.pause()
            }
            onClose?.()
          }}
          className="w-12 h-12 rounded-full bg-black bg-opacity-30 flex items-center justify-center hover:bg-opacity-50 transition-all"
        >
          <X className="w-6 h-6 text-white" />
        </button>

        <div className="flex items-center gap-2">
          <div className={`w-3 h-3 rounded-full ${getStatusColor()}`} />
          <span className="text-white text-sm font-medium">{getStatusText()}</span>
        </div>

        <div className="w-12" /> {/* ä½”ä½ */}
      </div>

      {/* å°è©±å…§å®¹å€åŸŸ */}
      <div className="flex-1 overflow-y-auto px-4 pt-20 pb-4">
        <div className="max-w-2xl mx-auto space-y-4">
          {messages.map((msg) => (
            <div
              key={msg.id}
              className={`flex ${msg.role === 'user' ? 'justify-end' : 'justify-start'}`}
            >
              <div
                className={`max-w-[80%] rounded-3xl px-5 py-3 ${
                  msg.role === 'user'
                    ? 'bg-white bg-opacity-20 text-white rounded-br-sm'
                    : 'bg-white bg-opacity-10 text-white rounded-bl-sm'
                }`}
              >
                <div className="flex items-start gap-2">
                  <p className="text-lg leading-relaxed whitespace-pre-wrap">
                    {msg.text}
                  </p>
                  {msg.audioUrl && chatState !== 'playing' && (
                    <button
                      onClick={() => {
                        triggerHaptic('light')
                        const audio = new Audio(msg.audioUrl)
                        audio.play()
                      }}
                      className="flex-shrink-0 mt-1"
                    >
                      <Volume2 className="w-5 h-5 text-white" />
                    </button>
                  )}
                </div>
                <p className="text-xs text-white text-opacity-60 mt-1">
                  {new Date(msg.timestamp).toLocaleTimeString('zh-TW', {
                    hour: '2-digit',
                    minute: '2-digit',
                  })}
                </p>
              </div>
            </div>
          ))}

          {/* ç•¶å‰è­˜åˆ¥æ–‡å­— */}
          {currentTranscript && chatState === 'processing' && (
            <div className="flex justify-end">
              <div className="bg-white bg-opacity-20 text-white rounded-3xl rounded-br-sm px-5 py-3">
                <p className="text-lg leading-relaxed">{currentTranscript}</p>
              </div>
            </div>
          )}

          {/* ç•¶å‰ AI å›æ‡‰ */}
          {currentResponse && chatState === 'processing' && (
            <div className="flex justify-start">
              <div className="bg-white bg-opacity-10 text-white rounded-3xl rounded-bl-sm px-5 py-3">
                <p className="text-lg leading-relaxed">{currentResponse}</p>
                <p className="text-xs text-white text-opacity-60 mt-1 animate-pulse">
                  æ­£åœ¨ç”ŸæˆèªéŸ³...
                </p>
              </div>
            </div>
          )}
        </div>
      </div>

      {/* åº•éƒ¨æ§åˆ¶å€åŸŸ */}
      <div className="pb-12 pt-4 px-4 safe-area-bottom">
        {/* éŸ³é »æ³¢å½¢ */}
        {chatState === 'recording' && waveformData.length > 0 && (
          <div className="flex justify-center items-end gap-1 h-16 mb-4">
            {waveformData.map((value, i) => (
              <div
                key={i}
                className="w-1 bg-white bg-opacity-60 rounded-full transition-all duration-75"
                style={{
                  height: `${Math.max(4, (value / 255) * 64)}px`,
                }}
              />
            ))}
          </div>
        )}

        {/* ä¸»æ§åˆ¶æŒ‰éˆ• */}
        <div className="flex justify-center">
          <button
            onClick={toggleRecording}
            disabled={!servicesReady && chatState === 'idle'}
            className={`w-24 h-24 rounded-full flex flex-col items-center justify-center gap-1 transition-all ${
              chatState === 'recording'
                ? 'bg-red-500 scale-110 shadow-lg shadow-red-500/50'
                : 'bg-white scale-100 shadow-xl'
            } ${!servicesReady && chatState === 'idle' ? 'opacity-50' : ''}`}
          >
            {chatState === 'recording' ? (
              <>
                <div className="w-4 h-4 rounded-full bg-white animate-ping absolute" />
                <Mic className="w-10 h-10 text-white relative z-10" />
              </>
            ) : (
              <Mic className={`w-10 h-10 ${chatState === 'idle' ? 'text-purple-600' : 'text-gray-400'}`} />
            )}
          </button>
        </div>

        {/* æç¤ºæ–‡å­— */}
        <p className="text-center text-white text-opacity-80 text-sm mt-4">
          {!servicesReady && chatState === 'idle' && 'æ­£åœ¨é€£æ¥èªéŸ³æœå‹™...'}
          {servicesReady && chatState === 'idle' && 'é»æ“Šéº¥å…‹é¢¨é–‹å§‹å°è©±'}
          {chatState === 'recording' && 'æ­£åœ¨éŒ„éŸ³...'}
          {(chatState === 'processing' || chatState === 'playing') && 'é»æ“Šå¯ä¸­æ–·ï¼Œç«‹å³èªªä¸‹ä¸€å¥'}
        </p>

        {/* æœå‹™ç‹€æ…‹æç¤º */}
        {!servicesReady && (
          <div className="mt-4 bg-yellow-500 bg-opacity-20 rounded-lg p-3 text-center">
            <p className="text-yellow-100 text-sm">
              âš ï¸ èªéŸ³æœå‹™æœªå°±ç·’ï¼Œè«‹æª¢æŸ¥ Docker é…ç½®
            </p>
          </div>
        )}
      </div>
    </div>
  )
}

// ========================================
// è§¸ç™¼å™¨æŒ‰éˆ•ï¼ˆåœ¨å…¶ä»–é é¢ä½¿ç”¨ï¼‰
// ========================================

export function ImmersiveVoiceChatButton() {
  const [isOpen, setIsOpen] = useState(false)

  return (
    <>
      {!isOpen && (
        <button
          onClick={() => setIsOpen(true)}
          className="fixed bottom-24 right-4 w-16 h-16 bg-gradient-to-br from-purple-500 to-pink-500 rounded-full shadow-xl flex items-center justify-center z-40 hover:scale-110 transition-transform"
        >
          <Sparkles className="w-8 h-8 text-white" />
        </button>
      )}

      {isOpen && (
        <ImmersiveVoiceChat onClose={() => setIsOpen(false)} />
      )}
    </>
  )
}
