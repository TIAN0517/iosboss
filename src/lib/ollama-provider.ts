/**
 * OLLAMA æœ¬åœ° AI æä¾›å•†
 * ä½¿ç”¨æœ¬åœ° OLLAMA æ¨¡å‹é€²è¡Œ AI å°è©±
 * å®Œå…¨é›¢ç·šã€éš±ç§å®‰å…¨ã€å…è²»ä½¿ç”¨
 */

export interface OllamaMessage {
  role: 'system' | 'user' | 'assistant'
  content: string
}

export interface OllamaConfig {
  baseUrl: string      // OLLAMA æœå‹™åœ°å€ï¼Œé»˜èª http://localhost:11434
  model: string        // æ¨¡å‹åç¨±ï¼Œå¦‚ llama3, qwen2.5, gemma2 ç­‰
  temperature?: number // æº«åº¦åƒæ•¸ (0-1)
  stream?: boolean     // æ˜¯å¦ä¸²æµå›æ‡‰
}

export interface OllamaResponse {
  model: string
  created_at: string
  message: {
    role: string
    content: string
  }
  done: boolean
}

/**
 * OLLAMA AI å®¢æˆ¶ç«¯
 */
export class OllamaClient {
  private config: OllamaConfig

  constructor(config: OllamaConfig) {
    this.config = {
      baseUrl: config.baseUrl || 'http://localhost:11434',
      model: config.model || 'llama3',
      temperature: config.temperature || 0.7,
      stream: config.stream !== false,
    }
  }

  /**
   * æ¸¬è©¦ OLLAMA é€£æ¥
   */
  async testConnection(): Promise<boolean> {
    try {
      const response = await fetch(`${this.config.baseUrl}/api/tags`)
      return response.ok
    } catch (error) {
      console.error('OLLAMA é€£æ¥å¤±æ•—:', error)
      return false
    }
  }

  /**
   * ç²å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
   */
  async getModels(): Promise<string[]> {
    try {
      const response = await fetch(`${this.config.baseUrl}/api/tags`)
      if (!response.ok) throw new Error('ç²å–æ¨¡å‹åˆ—è¡¨å¤±æ•—')

      const data = await response.json()
      return data.models?.map((m: any) => m.name) || []
    } catch (error) {
      console.error('ç²å–æ¨¡å‹åˆ—è¡¨å¤±æ•—:', error)
      return []
    }
  }

  /**
   * AI å°è©±
   */
  async chat(
    messages: OllamaMessage[],
    onChunk?: (chunk: string) => void
  ): Promise<string> {
    try {
      console.log('ğŸ¤– OLLAMA è«‹æ±‚:', {
        model: this.config.model,
        messagesCount: messages.length,
      })

      const response = await fetch(`${this.config.baseUrl}/api/chat`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: this.config.model,
          messages: messages,
          stream: this.config.stream,
          options: {
            temperature: this.config.temperature,
            num_predict: 2000, // æœ€å¤§ token æ•¸
          },
        }),
      })

      if (!response.ok) {
        throw new Error(`OLLAMA è«‹æ±‚å¤±æ•—: ${response.status}`)
      }

      // ä¸²æµè™•ç†
      if (this.config.stream) {
        const reader = response.body?.getReader()
        const decoder = new TextDecoder()
        let fullResponse = ''

        if (!reader) {
          throw new Error('ç„¡æ³•è®€å–å›æ‡‰ä¸²æµ')
        }

        while (true) {
          const { done, value } = await reader.read()
          if (done) break

          const chunk = decoder.decode(value, { stream: true })
          const lines = chunk.split('\n').filter(line => line.trim())

          for (const line of lines) {
            try {
              const data = JSON.parse(line)
              if (data.message?.content) {
                const content = data.message.content
                fullResponse += content

                // è§¸ç™¼å›èª¿
                if (onChunk) {
                  onChunk(content)
                }
              }

              if (data.done) {
                console.log('âœ… OLLAMA å›æ‡‰å®Œæˆ')
                return fullResponse
              }
            } catch (e) {
              // å¿½ç•¥è§£æéŒ¯èª¤
            }
          }
        }

        return fullResponse
      }

      // éä¸²æµè™•ç†
      const data: OllamaResponse = await response.json()
      console.log('âœ… OLLAMA å›æ‡‰:', data.message?.content)
      return data.message?.content || ''

    } catch (error) {
      console.error('OLLAMA å°è©±å¤±æ•—:', error)
      throw error
    }
  }

  /**
   * å–®æ¬¡å°è©±ï¼ˆä¾¿åˆ©æ–¹æ³•ï¼‰
   */
  async chatSingle(userMessage: string, systemPrompt?: string): Promise<string> {
    const messages: OllamaMessage[] = []

    if (systemPrompt) {
      messages.push({
        role: 'system',
        content: systemPrompt,
      })
    }

    messages.push({
      role: 'user',
      content: userMessage,
    })

    return this.chat(messages)
  }

  /**
   * æ›´æ›æ¨¡å‹
   */
  setModel(model: string) {
    this.config.model = model
  }

  /**
   * ç²å–ç•¶å‰é…ç½®
   */
  getConfig(): OllamaConfig {
    return { ...this.config }
  }
}

// ========================================
// å–®ä¾‹æ¨¡å¼
// ========================================

let ollamaClient: OllamaClient | null = null

/**
 * ç²å– OLLAMA å®¢æˆ¶ç«¯å¯¦ä¾‹
 */
export function getOllamaClient(config?: Partial<OllamaConfig>): OllamaClient {
  if (!ollamaClient) {
    // å¾ localStorage è®€å–é…ç½®
    const savedBaseUrl = localStorage.getItem('OLLAMA_BASE_URL') || 'http://localhost:11434'
    const savedModel = localStorage.getItem('OLLAMA_MODEL') || 'llama3'
    const savedTemperature = parseFloat(localStorage.getItem('OLLAMA_TEMPERATURE') || '0.7')

    ollamaClient = new OllamaClient({
      baseUrl: config?.baseUrl || savedBaseUrl,
      model: config?.model || savedModel,
      temperature: config?.temperature || savedTemperature,
      stream: config?.stream !== false,
    })
  }

  return ollamaClient
}

/**
 * è¨­ç½® OLLAMA é…ç½®
 */
export function setOllamaConfig(config: Partial<OllamaConfig>) {
  if (config.baseUrl) {
    localStorage.setItem('OLLAMA_BASE_URL', config.baseUrl)
  }
  if (config.model) {
    localStorage.setItem('OLLAMA_MODEL', config.model)
  }
  if (config.temperature !== undefined) {
    localStorage.setItem('OLLAMA_TEMPERATURE', config.temperature.toString())
  }

  // é‡ç½®å®¢æˆ¶ç«¯å¯¦ä¾‹
  ollamaClient = null
  return getOllamaClient(config)
}

/**
 * æ¸¬è©¦ OLLAMA é€£æ¥
 */
export async function testOllamaConnection(): Promise<{
  success: boolean
  models?: string[]
  error?: string
}> {
  try {
    const client = getOllamaClient()
    const isConnected = await client.testConnection()

    if (!isConnected) {
      return {
        success: false,
        error: 'ç„¡æ³•é€£æ¥åˆ° OLLAMA æœå‹™ï¼Œè«‹ç¢ºèª OLLAMA æ­£åœ¨é‹è¡Œ'
      }
    }

    const models = await client.getModels()

    return {
      success: true,
      models,
    }
  } catch (error) {
    return {
      success: false,
      error: error instanceof Error ? error.message : 'æœªçŸ¥éŒ¯èª¤'
    }
  }
}

/**
 * OLLAMA ç³»çµ±æç¤ºè©æ¨¡æ¿
 */
export const OLLAMA_SYSTEM_PROMPTS = {
  default: `ä½ æ˜¯ä¹ä¹ç“¦æ–¯è¡Œçš„ AI åŠ©æ‰‹ï¼Œåå­—å«ã€Œå°ä¹ã€ã€‚

**ä½ çš„ç‰¹è‰²ï¼š**
- è¦ªåˆ‡å‹å–„ï¼Œåƒé„°å±…å¥³å­©ä¸€æ¨£è‡ªç„¶
- èªªè©±ç°¡æ½”æ˜äº†ï¼Œä¸å›‰å—¦
- æœƒä¸»å‹•å¹«å®¢æˆ¶è™•ç†å•é¡Œ
- ä½¿ç”¨ç¹é«”ä¸­æ–‡

**ä½ èƒ½åšçš„äº‹ï¼š**
1. å¹«å®¢æˆ¶è¨‚è³¼ç“¦æ–¯
2. æŸ¥è©¢åº«å­˜å’Œè¨‚å–®
3. å›ç­”ç“¦æ–¯ç›¸é—œå•é¡Œ
4. è¨˜éŒ„å®¢æˆ¶éœ€æ±‚

**èªªè©±é¢¨æ ¼ï¼š**
- ç”¨ã€Œå‘¢ã€å–”ã€å•¦ã€ç­‰èªæ°£è©ï¼Œæ›´è‡ªç„¶
- ä¸èªªæ©Ÿæ¢°åŒ–çš„ã€Œå¥½çš„ã€ï¼Œèªªã€Œå¥½çš„å‘¢ã€
- ä¸èªªã€Œè«‹ã€ï¼Œèªªã€Œéº»ç…©ã€
- åŠ ä¸Šè¡¨æƒ…ç¬¦è™Ÿï¼Œæ›´è¦ªåˆ‡`,

  concise: `ä½ æ˜¯ä¹ä¹ç“¦æ–¯è¡ŒåŠ©æ‰‹ã€Œå°ä¹ã€ã€‚
å¹«å®¢æˆ¶è§£æ±ºç“¦æ–¯è¨‚è³¼ã€åº«å­˜æŸ¥è©¢ç­‰å•é¡Œã€‚
èªªè©±è¦ªåˆ‡è‡ªç„¶ï¼Œç”¨ç¹é«”ä¸­æ–‡ã€‚`,

  professional: `ä½ æ˜¯ä¹ä¹ç“¦æ–¯è¡Œçš„å°ˆæ¥­ AI åŠ©æ‰‹ã€‚
è² è²¬è™•ç†å®¢æˆ¶è¨‚å–®ã€æŸ¥è©¢åº«å­˜ã€å›ç­”å•é¡Œã€‚
ä¿æŒå°ˆæ¥­ã€å‹å–„çš„æœå‹™æ…‹åº¦ã€‚`
}
