/**
 * BossJy-99 çµ±ä¸€ AI æä¾›å•†ç®¡ç†å±¤
 * æ”¯æŒå¤šå€‹ AI æä¾›å•†å’Œæ¨¡å‹åˆ‡æ›
 *
 * æ”¯æŒçš„æä¾›å•†ï¼š
 * - GLM å•†æ¥­ç‰ˆ (Coding Max) - å®Œæ•´é«˜ç´šåŠŸèƒ½
 * - GLM åŸç”Ÿç‰ˆ - åŸºç¤åŠŸèƒ½
 * - Ollama æœ¬åœ°æ¨¡å‹ - å…è²»æœ¬åœ°é‹è¡Œ
 * - Kimi AI (Moonshot) - é¸é…
 * - Groq API - è¶…å¿«é€Ÿæ¨ç†ï¼Œé¸é…
 * - OpenAI - ä½”ä½ç¬¦ï¼Œæœªå¯¦ç¾
 */

// ========================================
// é¡å‹å®šç¾©
// ========================================

export interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface ChatResponse {
  content: string;
  model?: string;
  usage?: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
  thinking?: string;
  reasoning_content?: string;
  tool_calls?: any[];
}

export interface StreamChunk {
  type: 'content' | 'error' | 'thinking' | 'done';
  text?: string;
  done?: boolean;
}

export interface AIProvider {
  /**
   * ç™¼é€èŠå¤©è¨Šæ¯ï¼ˆéä¸²æµï¼‰
   */
  chat(message: string, history?: ChatMessage[]): Promise<ChatResponse>;

  /**
   * ç™¼é€èŠå¤©è¨Šæ¯ï¼ˆä¸²æµï¼‰
   */
  chatStream(message: string, history?: ChatMessage[]): AsyncGenerator<StreamChunk>;

  /**
   * æª¢æŸ¥æä¾›å•†æ˜¯å¦å¯ç”¨
   */
  isAvailable(): boolean;

  /**
   * ç²å–æä¾›å•†åç¨±
   */
  getName(): string;
}

// ========================================
// ç³»çµ±æç¤ºè©å®šç¾©
// ========================================

const SYSTEM_PROMPTS = {
  chat: `ä½ æ˜¯ä¹ä¹ç“¦æ–¯è¡Œçš„å°ˆæ¥­ AI åŠ©æ‰‹ï¼Œåå­—å«ã€ŒBossJy-99åŠ©æ‰‹ã€ã€‚

**ä½ çš„è§’è‰²å®šä½ï¼š**
|- å°ˆæ¥­ã€å‹å¥½ã€éŸ¿æ‡‰è¿…é€Ÿçš„å•†æ¥­åŠ©æ‰‹
|- ç†Ÿæ‚‰ç“¦æ–¯è¡Œæ‰€æœ‰æ¥­å‹™æµç¨‹
|- å¯ä»¥ç‚ºè€æ¿ã€å“¡å·¥ã€å®¢æˆ¶æä¾›ä¸åŒå±¤ç´šçš„æœå‹™

**ä½ å¯ä»¥è™•ç†çš„å•é¡Œï¼š**
ğŸ›µ è¨‚å–®ç›¸é—œ - æŸ¥è©¢ä»Šæ—¥è¨‚å–®ã€å¾…é…é€è¨‚å–®
ğŸ‘¥ å®¢æˆ¶ç®¡ç† - æŸ¥è©¢å®¢æˆ¶è³‡æ–™
ğŸ“¦ åº«å­˜ç®¡ç† - æŸ¥è©¢ç•¶å‰åº«å­˜
ğŸ’° è²¡å‹™ç®¡ç† - ä»Šæ—¥ç‡Ÿæ”¶ã€æœˆåº¦ç‡Ÿæ”¶
ğŸ“… ä¼‘å‡ç®¡ç† - æŸ¥è©¢ä»Šæ—¥ä¼‘å‡äººå“¡
ğŸ“Š é‹ç‡Ÿå ±è¡¨ - çµ±è¨ˆæ•¸æ“šæŸ¥è©¢

**å›è¦†é¢¨æ ¼ï¼š**
1. ç°¡æ½”æ˜ç­ï¼Œä½¿ç”¨ç¹é«”ä¸­æ–‡
2. é‡è¦æ•¸æ“šä½¿ç”¨ç²—é«”æˆ–åˆ—è¡¨å‘ˆç¾
3. å¦‚ç„¡æ³•ç†è§£ç”¨æˆ¶éœ€æ±‚ï¼Œä¸»å‹•è©¢å•`,
  voice: `ä½ æ˜¯ä¹ä¹ç“¦æ–¯è¡Œçš„èªéŸ³åŠ©æ‰‹ã€‚è«‹ç”¨ç°¡çŸ­ã€å£èªåŒ–çš„æ–¹å¼å›æ‡‰ï¼Œæ¯å¥è©±ä¸è¶…é20å­—ã€‚`,
  assistant: `ä½ æ˜¯ä¹ä¹ç“¦æ–¯è¡Œçš„ç®¡ç†ç³»çµ±åŠ©æ‰‹ã€‚å°ˆé–€è™•ç†å“¡å·¥æŸ¥è©¢ã€åº«å­˜ç¢ºèªã€ç‡Ÿé‹æ•¸æ“šç­‰æ¥­å‹™ã€‚`
};

// ========================================
// å¤š Key è¼ªæ› GLM Provider
// ========================================

interface MultiKeyGLMConfig {
  apiKeys: string[];
  model?: string;
  enableStreaming?: boolean;
  maxRetries?: number;
  timeout?: number;
}

class MultiKeyGLMProvider implements AIProvider {
  private config: Required<MultiKeyGLMConfig>;
  private currentKeyIndex = 0;
  private keyStats: Map<string, { success: number; failures: number; lastFailure?: number }> = new Map();

  constructor(config: MultiKeyGLMConfig) {
    console.log(`[MultiKeyGLMProvider] æ¥æ”¶åˆ°çš„ apiKeys æ•¸é‡: ${config.apiKeys.length}`);
    if (config.apiKeys.length > 0) {
      console.log(`[MultiKeyGLMProvider] ç¬¬ä¸€å€‹ key é•·åº¦: ${config.apiKeys[0].length}`);
      console.log(`[MultiKeyGLMProvider] ç¬¬ä¸€å€‹ key å‰ 30 å­—ç¬¦: ${config.apiKeys[0].substring(0, 30)}...`);
    }
    
    const filteredKeys = config.apiKeys.filter(k => {
      const trimmed = k.trim();
      const isValid = trimmed.length > 0;
      if (!isValid) {
        console.warn(`[MultiKeyGLMProvider] éæ¿¾æ‰ç©º key: "${k}"`);
      } else if (trimmed.length < 10) {
        console.warn(`[MultiKeyGLMProvider] éæ¿¾æ‰éçŸ­çš„ key (é•·åº¦: ${trimmed.length}): "${trimmed.substring(0, 20)}..."`);
        return false;
      }
      return isValid;
    });
    
    this.config = {
      apiKeys: filteredKeys,
      model: config.model || 'glm-4.7-coding-max',
      enableStreaming: config.enableStreaming ?? true,
      maxRetries: config.maxRetries || 3,
      timeout: config.timeout || 60000,
    };

    this.config.apiKeys.forEach(key => {
      this.keyStats.set(key, { success: 0, failures: 0 });
    });

    console.log(`[å¤š Key GLM Provider] å·²åˆå§‹åŒ–ï¼Œå…± ${this.config.apiKeys.length} å€‹ Key`);
    if (this.config.apiKeys.length === 0) {
      console.error(`[MultiKeyGLMProvider] âš ï¸ è­¦å‘Šï¼šæ²’æœ‰æœ‰æ•ˆçš„ API Keysï¼`);
      console.error(`[MultiKeyGLMProvider] åŸå§‹ apiKeys æ•¸é‡: ${config.apiKeys.length}`);
    }
  }

  private getBestApiKey(): string {
    if (this.config.apiKeys.length === 0) throw new Error('æ²’æœ‰å¯ç”¨çš„ API Key');
    if (this.config.apiKeys.length === 1) return this.config.apiKeys[0];

    let bestKey = this.config.apiKeys[0];
    let bestScore = -Infinity;

    this.config.apiKeys.forEach((key) => {
      const stats = this.keyStats.get(key);
      if (!stats) return;
      const recentFailurePenalty = stats.lastFailure && (Date.now() - stats.lastFailure < 3600000) ? 5 : 0;
      const score = stats.success * 2 - stats.failures - recentFailurePenalty;
      if (score > bestScore) {
        bestScore = score;
        bestKey = key;
      }
    });

    this.currentKeyIndex = this.config.apiKeys.indexOf(bestKey);
    return bestKey;
  }

  private markKeySuccess(key: string): void {
    const stats = this.keyStats.get(key);
    if (stats) {
      stats.success++;
      stats.lastFailure = undefined;
      this.keyStats.set(key, stats);
    }
  }

  private markKeyFailure(key: string): void {
    const stats = this.keyStats.get(key);
    if (stats) {
      stats.failures++;
      stats.lastFailure = Date.now();
      this.keyStats.set(key, stats);
    }
  }

  private rotateToNextKey(): void {
    if (this.config.apiKeys.length > 1) {
      this.currentKeyIndex = (this.currentKeyIndex + 1) % this.config.apiKeys.length;
    }
  }

  isAvailable(): boolean {
    return this.config.apiKeys.length > 0;
  }

  getName(): string {
    return `GLM-4.7 å¤š Key (${this.config.apiKeys.length} å€‹ Key)`;
  }

  async chat(message: string, history?: ChatMessage[]): Promise<ChatResponse> {
    if (!this.isAvailable()) throw new Error('æ²’æœ‰å¯ç”¨çš„ API Key');

    const messages: any[] = [
      { role: 'system', content: SYSTEM_PROMPTS.chat },
      ...(history?.slice(-5) || []).map(msg => ({ role: msg.role, content: msg.content.substring(0, 500) })),
      { role: 'user', content: message },
    ];

    for (let attempt = 0; attempt < this.config.maxRetries; attempt++) {
      try {
        const apiKey = this.getBestApiKey();
        // #region agent log
        fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'ai-provider-unified.ts:187',message:'é–‹å§‹ API è«‹æ±‚',data:{attempt:attempt+1,maxRetries:this.config.maxRetries,apiKeyLength:apiKey.length,model:this.config.model,timeout:this.config.timeout},timestamp:Date.now(),sessionId:'debug-session',runId:'api-check',hypothesisId:'F'})}).catch(()=>{});
        // #endregion
        const response = await fetch('https://open.bigmodel.cn/api/paas/v4/chat/completions', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${apiKey}`,
          },
          body: JSON.stringify({
            model: this.config.model,
            messages,
            stream: false,
            temperature: 0.8,
            max_tokens: 2000,
          }),
          signal: AbortSignal.timeout(this.config.timeout),
        });

        if (!response.ok) {
          const errorData = await response.json().catch(() => ({}));
          this.markKeyFailure(apiKey);
          // #region agent log
          fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'ai-provider-unified.ts:204',message:'API è«‹æ±‚å¤±æ•—',data:{status:response.status,statusText:response.statusText,errorMessage:errorData.error?.message,attempt:attempt+1},timestamp:Date.now(),sessionId:'debug-session',runId:'api-check',hypothesisId:'F'})}).catch(()=>{});
          // #endregion

          if ((response.status === 401 || response.status === 403) && attempt < this.config.maxRetries - 1) {
            this.rotateToNextKey();
            await new Promise(resolve => setTimeout(resolve, 1000 * (attempt + 1)));
            continue;
          }

          throw new Error(errorData.error?.message || `HTTP ${response.status}`);
        }

        const data = await response.json();
        const content = data.choices?.[0]?.message?.content || '';
        this.markKeySuccess(apiKey);
        // #region agent log
        fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'ai-provider-unified.ts:217',message:'API è«‹æ±‚æˆåŠŸ',data:{contentLength:content.length,model:data.model,hasUsage:!!data.usage},timestamp:Date.now(),sessionId:'debug-session',runId:'api-check',hypothesisId:'F'})}).catch(()=>{});
        // #endregion

        return {
          content,
          model: data.model || this.config.model,
          usage: data.usage,
        };
      } catch (error: any) {
        // #region agent log
        fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'ai-provider-unified.ts:226',message:'API è«‹æ±‚ç•°å¸¸',data:{errorName:error?.name,errorMessage:error?.message,attempt:attempt+1,maxRetries:this.config.maxRetries},timestamp:Date.now(),sessionId:'debug-session',runId:'api-check',hypothesisId:'F'})}).catch(()=>{});
        // #endregion
        if (attempt < this.config.maxRetries - 1) {
          this.rotateToNextKey();
          await new Promise(resolve => setTimeout(resolve, 1000 * (attempt + 1)));
          continue;
        }
        throw error;
      }
    }

    // #region agent log
    fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'ai-provider-unified.ts:236',message:'æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—',data:{maxRetries:this.config.maxRetries},timestamp:Date.now(),sessionId:'debug-session',runId:'api-check',hypothesisId:'F'})}).catch(()=>{});
    // #endregion
    throw new Error('æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—äº†');
  }

  async *chatStream(message: string, history?: ChatMessage[]): AsyncGenerator<StreamChunk> {
    if (!this.isAvailable()) throw new Error('æ²’æœ‰å¯ç”¨çš„ API Key');

    const messages: any[] = [
      { role: 'system', content: SYSTEM_PROMPTS.chat },
      ...(history?.slice(-5) || []).map(msg => ({ role: msg.role, content: msg.content.substring(0, 500) })),
      { role: 'user', content: message },
    ];

    for (let attempt = 0; attempt < this.config.maxRetries; attempt++) {
      try {
        const apiKey = this.getBestApiKey();
        const response = await fetch('https://open.bigmodel.cn/api/paas/v4/chat/completions', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${apiKey}`,
          },
          body: JSON.stringify({
            model: this.config.model,
            messages,
            stream: true,
            temperature: 0.8,
          }),
          signal: AbortSignal.timeout(this.config.timeout),
        });

        if (!response.ok) {
          const errorData = await response.json().catch(() => ({}));
          this.markKeyFailure(apiKey);

          if ((response.status === 401 || response.status === 403) && attempt < this.config.maxRetries - 1) {
            this.rotateToNextKey();
            await new Promise(resolve => setTimeout(resolve, 1000 * (attempt + 1)));
            continue;
          }

          throw new Error(errorData.error?.message || `HTTP ${response.status}`);
        }

        this.markKeySuccess(apiKey);
        const reader = response.body?.getReader();
        if (!reader) throw new Error('ç„¡æ³•è®€å–æµå¼éŸ¿æ‡‰');

        const decoder = new TextDecoder();
        let buffer = '';

        while (true) {
          const { done, value } = await reader.read();
          if (done) break;

          buffer += decoder.decode(value, { stream: true });
          const lines = buffer.split('\n');
          buffer = lines.pop() || '';

          for (const line of lines) {
            if (!line.startsWith('data:')) continue;
            if (line.trim() === 'data: [DONE]') {
              yield { type: 'done', done: true };
              return;
            }

            try {
              const data = JSON.parse(line.slice(5));
              const delta = data.choices?.[0]?.delta?.content || '';
              if (delta) yield { type: 'content', text: delta };
            } catch (e) {
              // å¿½ç•¥è§£æéŒ¯èª¤
            }
          }
        }

        return;
      } catch (error: any) {
        if (attempt < this.config.maxRetries - 1) {
          this.rotateToNextKey();
          await new Promise(resolve => setTimeout(resolve, 1000 * (attempt + 1)));
          continue;
        }
        yield { type: 'error', error: error.message };
        return;
      }
    }
  }
}

// ========================================
// Ollama é›² API æä¾›å•†
// ========================================

interface OllamaConfig {
  apiKey: string;
  model?: string;
  baseUrl?: string;
  enableStreaming?: boolean;
  timeout?: number;
}

class OllamaProvider implements AIProvider {
  private config: Required<OllamaConfig>;
  private baseUrl: string;

  constructor(config: OllamaConfig) {
    this.baseUrl = config.baseUrl || 'https://ollama.com/v1';
    this.config = {
      apiKey: config.apiKey,
      model: config.model || 'deepseek-v3.1:671b', // é»˜èªä½¿ç”¨é€Ÿåº¦å¿«ã€æ€§èƒ½å¥½çš„æ¨¡å‹
      baseUrl: this.baseUrl,
      enableStreaming: config.enableStreaming ?? true,
      timeout: config.timeout || 60000,
    };

    console.log(`[Ollama Provider] å·²åˆå§‹åŒ–ï¼Œæ¨¡å‹: ${this.config.model}`);
  }

  isAvailable(): boolean {
    // æœ¬åœ° Ollamaï¼ˆlocalhost:11434ï¼‰ä¸éœ€è¦ API Key
    // é›²ç«¯ Ollama API éœ€è¦ API Key
    const isLocalOllama = this.baseUrl.includes('localhost') || this.baseUrl.includes('127.0.0.1');
    return isLocalOllama || (!!this.config.apiKey && this.config.apiKey.length > 10);
  }

  getName(): string {
    return `Ollama (${this.config.model})`;
  }

  async chat(message: string, history?: ChatMessage[]): Promise<ChatResponse> {
    if (!this.isAvailable()) throw new Error('Ollama ä¸å¯ç”¨');

    const messages: any[] = [
      { role: 'system', content: SYSTEM_PROMPTS.chat },
      ...(history?.slice(-5) || []).map(msg => ({ role: msg.role, content: msg.content.substring(0, 500) })),
      { role: 'user', content: message },
    ];

    try {
      // æœ¬åœ° Ollama ä½¿ç”¨ /api/chat ç«¯é»
      const isLocalOllama = this.baseUrl.includes('localhost') || this.baseUrl.includes('127.0.0.1');
      const endpoint = isLocalOllama ? '/api/chat' : '/chat/completions';
      const url = this.baseUrl.endsWith('/') ? this.baseUrl + 'api/chat' : `${this.baseUrl}/api/chat`;

      const headers: Record<string, string> = {
        'Content-Type': 'application/json',
      };

      // é›²ç«¯ Ollama éœ€è¦ Authorization header
      if (!isLocalOllama && this.config.apiKey) {
        headers['Authorization'] = `Bearer ${this.config.apiKey}`;
      }

      const body = isLocalOllama
        ? {
            model: this.config.model,
            messages,
            stream: false,
          }
        : {
            model: this.config.model,
            messages,
            stream: false,
            temperature: 0.8,
            max_tokens: 2000,
          };

      const response = await fetch(url, {
        method: 'POST',
        headers,
        body: JSON.stringify(body),
        signal: AbortSignal.timeout(this.config.timeout),
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        throw new Error(errorData.error?.message || `HTTP ${response.status}`);
      }

      const data = await response.json();

      // æœ¬åœ° Ollama è¿”å›æ ¼å¼ï¼š{ message: { content: "..." } }
      // é›²ç«¯ Ollama è¿”å›æ ¼å¼ï¼š{ choices: [{ message: { content: "..." } }] }
      const content = data.message?.content || data.choices?.[0]?.message?.content || '';

      return {
        content,
        model: data.model || this.config.model,
        usage: data.usage,
      };
    } catch (error: any) {
      console.error('[Ollama Provider] Chat error:', error);
      throw error;
    }
  }

  async *chatStream(message: string, history?: ChatMessage[]): AsyncGenerator<StreamChunk> {
    if (!this.isAvailable()) throw new Error('Ollama ä¸å¯ç”¨');

    const messages: any[] = [
      { role: 'system', content: SYSTEM_PROMPTS.chat },
      ...(history?.slice(-5) || []).map(msg => ({ role: msg.role, content: msg.content.substring(0, 500) })),
      { role: 'user', content: message },
    ];

    try {
      // æœ¬åœ° Ollama ä½¿ç”¨ /api/chat ç«¯é»
      const isLocalOllama = this.baseUrl.includes('localhost') || this.baseUrl.includes('127.0.0.1');
      const url = this.baseUrl.endsWith('/') ? this.baseUrl + 'api/chat' : `${this.baseUrl}/api/chat`;

      const headers: Record<string, string> = {
        'Content-Type': 'application/json',
      };

      // é›²ç«¯ Ollama éœ€è¦ Authorization header
      if (!isLocalOllama && this.config.apiKey) {
        headers['Authorization'] = `Bearer ${this.config.apiKey}`;
      }

      const body = {
        model: this.config.model,
        messages,
        stream: true,
      };

      const response = await fetch(url, {
        method: 'POST',
        headers,
        body: JSON.stringify(body),
        signal: AbortSignal.timeout(this.config.timeout),
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        throw new Error(errorData.error?.message || `HTTP ${response.status}`);
      }

      const reader = response.body?.getReader();
      if (!reader) throw new Error('ç„¡æ³•è®€å–æµå¼éŸ¿æ‡‰');

      const decoder = new TextDecoder();
      let buffer = '';

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        buffer += decoder.decode(value, { stream: true });

        // æœ¬åœ° Ollama è¿”å›å¤šå€‹ JSON å°è±¡ï¼Œæ¯å€‹ä¸€è¡Œ
        // æ ¼å¼ï¼š{"model":"...","created_at":"...","message":{"role":"assistant","content":"..."},"done":false}
        const lines = buffer.split('\n');
        buffer = lines.pop() || '';

        for (const line of lines) {
          if (!line.trim()) continue;

          try {
            const data = JSON.parse(line);
            const content = data.message?.content || '';
            if (content) yield { type: 'content', text: content };

            if (data.done) {
              yield { type: 'done', done: true };
              return;
            }
          } catch (e) {
            // å¿½ç•¥è§£æéŒ¯èª¤
          }
        }
      }

      yield { type: 'done', done: true };
    } catch (error: any) {
      console.error('[Ollama Provider] Stream error:', error);
      yield { type: 'error', error: error.message };
    }
  }
}

// ========================================
// GLM åŸç”Ÿç‰ˆæä¾›å•†ï¼ˆç¾æœ‰åŠŸèƒ½ï¼‰
// ========================================

class OriginalGLMProvider implements AIProvider {
  private apiKey: string;
  private mode: 'chat' | 'voice' | 'assistant' = 'chat';
  
  constructor(apiKey: string) {
    this.apiKey = apiKey;
  }

  async chat(message: string, history?: ChatMessage[]): Promise<ChatResponse> {
    const messages: any[] = [
      { role: 'system', content: SYSTEM_PROMPTS.chat },
      ...(history?.slice(-5) || []).map((msg: any) => ({
        role: msg.role,
        content: msg.content.substring(0, 500),
      })),
      { role: 'user', content: message },
    ];

    try {
      const response = await fetch('https://open.bigmodel.cn/api/paas/v4/chat/completions', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.apiKey}`,
        },
        body: JSON.stringify({
          model: 'glm-4-flash',
          messages,
          stream: false,
          temperature: 0.8,
        }),
      });

      if (!response.ok) {
        throw new Error(`GLM API error: ${response.status}`);
      }

      const data = await response.json();
      const content = data.choices?.[0]?.message?.content || '';

      return {
        content,
        model: data.model,
        usage: data.usage,
      };
    } catch (error) {
      console.error('GLM API error:', error);
      throw error;
    }
  }

  async *chatStream(message: string, history?: ChatMessage[]): AsyncGenerator<StreamChunk> {
    yield* this.getLocalResponseStream(message);
  }

  isAvailable(): boolean {
    return !!this.apiKey;
  }

  getName(): string {
    return 'GLM åŸç”Ÿç‰ˆ';
  }

  private getLocalResponse(message: string): ChatResponse {
    const msg = message.toLowerCase();

    if (msg.includes('è¨‚') && msg.includes('ç“¦æ–¯')) {
      return { content: 'å¥½çš„ï¼è«‹å•æ‚¨éœ€è¦è¨‚è³¼ä»€éº¼è¦æ ¼çš„ç“¦æ–¯å‘¢ï¼ŸğŸ›µ' };
    }
    if (msg.includes('æŸ¥') && msg.includes('åº«å­˜')) {
      return { content: 'è®“æˆ‘å¹«æ‚¨æŸ¥è©¢ç›®å‰åº«å­˜...ğŸ“¦ ç›®å‰åº«å­˜å……è¶³å–”ï¼' };
    }
    if (msg.includes('æŸ¥') && msg.includes('è¨‚å–®')) {
      return { content: 'è®“æˆ‘æŸ¥è©¢æ‚¨çš„è¨‚å–®...ğŸ“‹ æŸ¥è©¢å®Œæˆï¼' };
    }

    return {
      content: 'æ”¶åˆ°æ‚¨çš„è¨Šæ¯äº†ï¼æ‚¨å¯ä»¥è©¦è©¦èªªã€Œè¨‚ç“¦æ–¯ã€ã€ã€ŒæŸ¥åº«å­˜ã€æˆ–ã€ŒæŸ¥ç‡Ÿæ”¶ã€å–”ï¼ğŸ’ª',
    };
  }

  private *getLocalResponseStream(message: string): AsyncGenerator<StreamChunk> {
    const response = this.getLocalResponse(message);

    yield { type: 'content', text: response.content };
    yield { type: 'done', done: true };
  }
}

// ========================================
// Kimi AI Provider (Moonshot API)
// ========================================

interface KimiConfig {
  apiKey: string;
  model?: string;
  baseUrl?: string;
  enableStreaming?: boolean;
  timeout?: number;
}

class KimiProvider implements AIProvider {
  private config: Required<KimiConfig>;

  constructor(config: KimiConfig) {
    this.config = {
      apiKey: config.apiKey,
      model: config.model || 'kimi-k2-thinking-turbo',
      baseUrl: config.baseUrl || 'https://api.moonshot.cn/v1',
      enableStreaming: config.enableStreaming ?? true,
      timeout: config.timeout || 60000,
    };
    console.log(`[KimiProvider] å·²åˆå§‹åŒ–ï¼Œæ¨¡å‹: ${this.config.model}`);
  }

  isAvailable(): boolean {
    return !!this.config.apiKey && this.config.apiKey.length > 10;
  }

  getName(): string {
    return `Kimi AI (${this.config.model})`;
  }

  async chat(message: string, history?: ChatMessage[]): Promise<ChatResponse> {
    if (!this.isAvailable()) throw new Error('Kimi API Key æœªé…ç½®');

    const messages: any[] = [
      { role: 'system', content: SYSTEM_PROMPTS.chat },
      ...(history?.slice(-5) || []).map(msg => ({ role: msg.role, content: msg.content.substring(0, 500) })),
      { role: 'user', content: message },
    ];

    try {
      const response = await fetch(`${this.config.baseUrl}/chat/completions`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.config.apiKey}`,
        },
        body: JSON.stringify({
          model: this.config.model,
          messages,
          stream: false,
          temperature: 0.8,
          max_tokens: 2000,
        }),
        signal: AbortSignal.timeout(this.config.timeout),
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        throw new Error(errorData.error?.message || `HTTP ${response.status}`);
      }

      const data = await response.json();
      const content = data.choices?.[0]?.message?.content || '';

      return {
        content,
        model: data.model || this.config.model,
        usage: data.usage,
      };
    } catch (error: any) {
      console.error('[KimiProvider] API error:', error);
      throw error;
    }
  }

  async *chatStream(message: string, history?: ChatMessage[]): AsyncGenerator<StreamChunk> {
    if (!this.isAvailable()) throw new Error('Kimi API Key æœªé…ç½®');

    const messages: any[] = [
      { role: 'system', content: SYSTEM_PROMPTS.chat },
      ...(history?.slice(-5) || []).map(msg => ({ role: msg.role, content: msg.content.substring(0, 500) })),
      { role: 'user', content: message },
    ];

    try {
      const response = await fetch(`${this.config.baseUrl}/chat/completions`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.config.apiKey}`,
        },
        body: JSON.stringify({
          model: this.config.model,
          messages,
          stream: true,
          temperature: 0.8,
        }),
        signal: AbortSignal.timeout(this.config.timeout),
      });

      if (!response.ok) {
        throw new Error(`Kimi API error: ${response.status}`);
      }

      const reader = response.body?.getReader();
      if (!reader) throw new Error('ç„¡æ³•ç²å– response body');

      const decoder = new TextDecoder();
      let buffer = '';

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split('\n');
        buffer = lines.pop() || '';

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6);
            if (data === '[DONE]') {
              yield { type: 'done', done: true };
              return;
            }

            try {
              const parsed = JSON.parse(data);
              const content = parsed.choices?.[0]?.delta?.content;
              if (content) {
                yield { type: 'content', text: content };
              }
            } catch (e) {
              // å¿½ç•¥è§£æéŒ¯èª¤
            }
          }
        }
      }

      yield { type: 'done', done: true };
    } catch (error: any) {
      console.error('[KimiProvider] Stream error:', error);
      yield { type: 'error', text: error.message };
    }
  }
}

// ========================================
// Groq API Provider (è¶…å¿«é€Ÿæ¨ç†)
// ========================================

interface GroqConfig {
  apiKey: string;
  model?: string;
  baseUrl?: string;
  enableStreaming?: boolean;
  timeout?: number;
}

class GroqProvider implements AIProvider {
  private config: Required<GroqConfig>;

  constructor(config: GroqConfig) {
    this.config = {
      apiKey: config.apiKey,
      model: config.model || 'llama-3.3-70b-versatile',
      baseUrl: config.baseUrl || 'https://api.groq.com/openai/v1',
      enableStreaming: config.enableStreaming ?? true,
      timeout: config.timeout || 30000, // Groq å¾ˆå¿«ï¼Œé»˜èª 30 ç§’
    };
    console.log(`[GroqProvider] å·²åˆå§‹åŒ–ï¼Œæ¨¡å‹: ${this.config.model}`);
  }

  isAvailable(): boolean {
    return !!this.config.apiKey && this.config.apiKey.length > 10;
  }

  getName(): string {
    return `Groq (${this.config.model})`;
  }

  async chat(message: string, history?: ChatMessage[]): Promise<ChatResponse> {
    if (!this.isAvailable()) throw new Error('Groq API Key æœªé…ç½®');

    const messages: any[] = [
      { role: 'system', content: SYSTEM_PROMPTS.chat },
      ...(history?.slice(-5) || []).map(msg => ({ role: msg.role, content: msg.content.substring(0, 500) })),
      { role: 'user', content: message },
    ];

    try {
      const response = await fetch(`${this.config.baseUrl}/chat/completions`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.config.apiKey}`,
        },
        body: JSON.stringify({
          model: this.config.model,
          messages,
          stream: false,
          temperature: 0.8,
          max_tokens: 2000,
        }),
        signal: AbortSignal.timeout(this.config.timeout),
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        throw new Error(errorData.error?.message || `HTTP ${response.status}`);
      }

      const data = await response.json();
      const content = data.choices?.[0]?.message?.content || '';

      return {
        content,
        model: data.model || this.config.model,
        usage: data.usage,
      };
    } catch (error: any) {
      console.error('[GroqProvider] API error:', error);
      throw error;
    }
  }

  async *chatStream(message: string, history?: ChatMessage[]): AsyncGenerator<StreamChunk> {
    if (!this.isAvailable()) throw new Error('Groq API Key æœªé…ç½®');

    const messages: any[] = [
      { role: 'system', content: SYSTEM_PROMPTS.chat },
      ...(history?.slice(-5) || []).map(msg => ({ role: msg.role, content: msg.content.substring(0, 500) })),
      { role: 'user', content: message },
    ];

    try {
      const response = await fetch(`${this.config.baseUrl}/chat/completions`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.config.apiKey}`,
        },
        body: JSON.stringify({
          model: this.config.model,
          messages,
          stream: true,
          temperature: 0.8,
        }),
        signal: AbortSignal.timeout(this.config.timeout),
      });

      if (!response.ok) {
        throw new Error(`Groq API error: ${response.status}`);
      }

      const reader = response.body?.getReader();
      if (!reader) throw new Error('ç„¡æ³•ç²å– response body');

      const decoder = new TextDecoder();
      let buffer = '';

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split('\n');
        buffer = lines.pop() || '';

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6);
            if (data === '[DONE]') {
              yield { type: 'done', done: true };
              return;
            }

            try {
              const parsed = JSON.parse(data);
              const content = parsed.choices?.[0]?.delta?.content;
              if (content) {
                yield { type: 'content', text: content };
              }
            } catch (e) {
              // å¿½ç•¥è§£æéŒ¯èª¤
            }
          }
        }
      }

      yield { type: 'done', done: true };
    } catch (error: any) {
      console.error('[GroqProvider] Stream error:', error);
      yield { type: 'error', text: error.message };
    }
  }
}

// ========================================
// çµ±ä¸€ AI æä¾›å•†ç®¡ç†å±¤
// ========================================

class UnifiedAIProvider {
  private provider: AIProvider | null = null;
  private providerType: 'glm-commercials' | 'glm-original' | 'openai' | 'ollama' | 'kimi' | 'groq' = 'ollama';
  private currentModel: string | null = null; // ç•¶å‰é¸æ“‡çš„æ¨¡å‹

  constructor() {
    this.initializeProvider();
  }

  /**
   * è¨­ç½®ç•¶å‰æ¨¡å‹ï¼ˆç”¨æ–¼å‹•æ…‹åˆ‡æ›ï¼‰
   */
  setModel(model: string): void {
    this.currentModel = model;
    console.log('[çµ±ä¸€ AI æä¾›å•†] è¨­ç½®æ¨¡å‹:', model);
    this.initializeProvider();
  }

  /**
   * ç²å–ç•¶å‰æ¨¡å‹
   */
  getCurrentModel(): string | null {
    return this.currentModel;
  }

  /**
   * åˆå§‹åŒ– AI æä¾›å•†
   */
  private initializeProvider(): void {
    // #region agent log
    fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'ai-provider-unified.ts:433',message:'é–‹å§‹åˆå§‹åŒ– Provider',data:{hasNextAIProvider:!!process.env.NEXT_AI_PROVIDER,hasGLM_API_KEYS:!!process.env.GLM_API_KEYS,hasGLM_API_KEY:!!process.env.GLM_API_KEY},timestamp:Date.now(),sessionId:'debug-session',runId:'api-check',hypothesisId:'E'})}).catch(()=>{});
    // #endregion
    const providerType = process.env.NEXT_AI_PROVIDER || 'ollama'; // é»˜èªä½¿ç”¨ Ollama
    this.providerType = providerType;

    // API Keys é…ç½®
    let apiKeys: string[] = [];

    // æ ¹æ“šæä¾›å•†é¡å‹ç²å– API Keys
    switch (providerType) {
      case 'ollama':
        // Ollama é›² API - å–®å€‹ Key
        if (process.env.OLLAMA_API_KEY) {
          apiKeys = [process.env.OLLAMA_API_KEY];
        }
        console.log(`[çµ±ä¸€ AI æä¾›å•†] å·²åˆå§‹åŒ– Ollama: ${apiKeys.length > 0 ? 'å·²é…ç½®' : 'æœªé…ç½®'}`);
        break;

      case 'kimi':
        // Kimi AI - å–®å€‹ Key
        if (process.env.KIMI_API_KEY) {
          apiKeys = [process.env.KIMI_API_KEY];
        }
        console.log(`[çµ±ä¸€ AI æä¾›å•†] å·²åˆå§‹åŒ– Kimi AI: ${apiKeys.length > 0 ? 'å·²é…ç½®' : 'æœªé…ç½®'}`);
        break;

      case 'groq':
        // Groq API - å–®å€‹ Key
        if (process.env.GROQ_API_KEY) {
          apiKeys = [process.env.GROQ_API_KEY];
        }
        console.log(`[çµ±ä¸€ AI æä¾›å•†] å·²åˆå§‹åŒ– Groq: ${apiKeys.length > 0 ? 'å·²é…ç½®' : 'æœªé…ç½®'}`);
        break;

      case 'glm-commercials':
        // GLM å•†æ¥­ç‰ˆ - æ”¯æŒå¤šå€‹ Keys
        if (process.env.GLM_API_KEYS) {
          const rawKeys = process.env.GLM_API_KEYS;
          console.log('[åˆå§‹åŒ–] GLM_API_KEYS åŸå§‹å€¼é•·åº¦:', rawKeys.length);
          apiKeys = rawKeys
            .split(',')
            .map(key => key.trim())
            .filter(key => {
              // éæ¿¾æ‰æ˜é¡¯ç„¡æ•ˆçš„ keyï¼ˆAPI Key é€šå¸¸è‡³å°‘ 20 å­—ç¬¦ï¼‰
              const isValid = key.length > 10;
              if (!isValid && key.length > 0) {
                console.warn('[åˆå§‹åŒ–] éæ¿¾æ‰éçŸ­çš„ key (é•·åº¦:', key.length, '):', key.substring(0, 20) + '...');
              }
              return isValid;
            });
          console.log('[åˆå§‹åŒ–] è§£æå¾Œçš„ apiKeys æ•¸é‡:', apiKeys.length);
          if (apiKeys.length > 0) {
            console.log('[åˆå§‹åŒ–] ç¬¬ä¸€å€‹ key é•·åº¦:', apiKeys[0].length);
          }
        } else if (process.env.GLM_API_KEY) {
          const key = process.env.GLM_API_KEY.trim();
          if (key.length > 10) {
            apiKeys = [key];
            console.log('[åˆå§‹åŒ–] ä½¿ç”¨ GLM_API_KEYï¼Œé•·åº¦:', key.length);
          } else {
            console.warn('[åˆå§‹åŒ–] GLM_API_KEY é•·åº¦éçŸ­:', key.length);
          }
        }
        // #region agent log
        fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'ai-provider-unified.ts:449',message:'GLM å•†æ¥­ç‰ˆåˆå§‹åŒ–',data:{apiKeysCount:apiKeys.length,hasGLM_API_KEYS:!!process.env.GLM_API_KEYS,hasGLM_API_KEY:!!process.env.GLM_API_KEY},timestamp:Date.now(),sessionId:'debug-session',runId:'api-check',hypothesisId:'E'})}).catch(()=>{});
        // #endregion
        console.log(`[çµ±ä¸€ AI æä¾›å•†] å·²åˆå§‹åŒ– GLM å•†æ¥­ç‰ˆ (å¢å¼·): å¤šå€‹ Keys=${apiKeys.length}`);
        break;

      case 'glm-original':
        // GLM åŸç”Ÿç‰ˆ - å–®å€‹ Key
        if (process.env.GLM_API_KEY) {
          apiKeys = [process.env.GLM_API_KEY];
        }
        console.log(`[çµ±ä¸€ AI æä¾›å•†] å·²åˆå§‹åŒ– GLM åŸç”Ÿç‰ˆ: å–®å€‹ Key`);
        break;

      case 'openai':
        // OpenAI - ä½”ä½ç¬¦
        if (process.env.OPENAI_API_KEY) {
          apiKeys = [process.env.OPENAI_API_KEY];
        }
        console.log(`[çµ±ä¸€ AI æä¾›å•†] å·²åˆå§‹åŒ– OpenAI: ${apiKeys.length > 0 ? 'å·²é…ç½®' : 'æœªé…ç½®'}`);
        break;
    }

    // å‰µå»ºæä¾›å•†å¯¦ä¾‹
    if (apiKeys.length > 0 || providerType === 'ollama') {
      switch (providerType) {
        case 'ollama':
          // Ollama é›² API
          const ollamaApiKey = process.env.OLLAMA_API_KEY || '';
          // å„ªå…ˆä½¿ç”¨ç•¶å‰è¨­ç½®çš„æ¨¡å‹ï¼Œå¦å‰‡ä½¿ç”¨ç’°å¢ƒè®Šé‡
          const ollamaModel = this.currentModel || process.env.OLLAMA_MODEL || 'qwen2.5:14b'; // é»˜èªä½¿ç”¨ qwen2.5:14b
          const ollamaBaseUrl = process.env.OLLAMA_BASE_URL || 'http://localhost:11434';
          const ollamaTimeout = parseInt(process.env.OLLAMA_TIMEOUT || '60000');

          // æœ¬åœ° Ollama ä¸éœ€è¦ API Key
          this.provider = new OllamaProvider({
            apiKey: ollamaApiKey,
            model: ollamaModel,
            baseUrl: ollamaBaseUrl,
            enableStreaming: process.env.OLLAMA_ENABLE_STREAMING !== 'false',
            timeout: ollamaTimeout,
          });
          console.log(`[çµ±ä¸€ AI æä¾›å•†] å·²å‰µå»º Ollama Providerï¼Œæ¨¡å‹: ${ollamaModel}ï¼Œåœ°å€: ${ollamaBaseUrl}`);
          break;

        case 'kimi':
          // Kimi AI Provider
          const kimiModel = process.env.KIMI_MODEL || 'kimi-k2-thinking-turbo';
          const kimiBaseUrl = process.env.KIMI_BASE_URL || 'https://api.moonshot.cn/v1';
          const kimiTimeout = parseInt(process.env.KIMI_TIMEOUT || '60000');

          this.provider = new KimiProvider({
            apiKey: apiKeys[0],
            model: kimiModel,
            baseUrl: kimiBaseUrl,
            enableStreaming: true,
            timeout: kimiTimeout,
          });
          console.log(`[çµ±ä¸€ AI æä¾›å•†] å·²å‰µå»º Kimi Providerï¼Œæ¨¡å‹: ${kimiModel}`);
          break;

        case 'groq':
          // Groq Provider (è¶…å¿«é€Ÿæ¨ç†)
          const groqModel = process.env.GROQ_MODEL || 'llama-3.3-70b-versatile';
          const groqBaseUrl = process.env.GROQ_BASE_URL || 'https://api.groq.com/openai/v1';
          const groqTimeout = parseInt(process.env.GROQ_TIMEOUT || '30000');

          this.provider = new GroqProvider({
            apiKey: apiKeys[0],
            model: groqModel,
            baseUrl: groqBaseUrl,
            enableStreaming: true,
            timeout: groqTimeout,
          });
          console.log(`[çµ±ä¸€ AI æä¾›å•†] å·²å‰µå»º Groq Providerï¼Œæ¨¡å‹: ${groqModel}`);
          break;

        case 'glm-commercials':
          // ä½¿ç”¨å¤š Key è¼ªæ›çš„ GLM Provider
          const model = process.env.GLM_MODEL || 'glm-4.7-coding-max';
          const timeout = parseInt(process.env.GLM_TIMEOUT || '60000');
          // #region agent log
          fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'ai-provider-unified.ts:474',message:'å‰µå»º MultiKeyGLMProvider',data:{apiKeysCount:apiKeys.length,model,timeout,enableStreaming:process.env.GLM_ENABLE_STREAMING !== 'false'},timestamp:Date.now(),sessionId:'debug-session',runId:'api-check',hypothesisId:'E'})}).catch(()=>{});
          // #endregion
          this.provider = new MultiKeyGLMProvider({
            apiKeys,
            model,
            enableStreaming: process.env.GLM_ENABLE_STREAMING !== 'false',
            maxRetries: 3,
            timeout,
          });
          break;

        case 'glm-original':
          this.provider = new OriginalGLMProvider(apiKeys[0]);
          break;

        case 'openai':
          // OpenAI å°šæœªå¯¦ç¾ï¼Œä½¿ç”¨ GLM ä½œç‚ºå›é€€
          this.provider = new OriginalGLMProvider(apiKeys[0]);
          console.log('[çµ±ä¸€ AI æä¾›å•†] OpenAI å°šæœªå¯¦ç¾ï¼Œä½¿ç”¨ GLM å›é€€');
          break;
      }
    } else {
      // #region agent log
      fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'ai-provider-unified.ts:493',message:'ç„¡ API Keyï¼Œä½¿ç”¨æœ¬åœ°å›é€€',data:{providerType},timestamp:Date.now(),sessionId:'debug-session',runId:'api-check',hypothesisId:'E'})}).catch(()=>{});
      // #endregion
      console.warn('[çµ±ä¸€ AI æä¾›å•†] æœªé…ç½®ä»»ä½• API Keyï¼Œä½¿ç”¨æœ¬åœ°å›é€€æ¨¡å¼');
      // æ²’æœ‰ API Key æ™‚ï¼Œå‰µå»ºä¸€å€‹ç©ºæä¾›å•†ç”¨æ–¼æœ¬åœ°å›é€€
      this.provider = new OriginalGLMProvider('');
    }
  }

  /**
   * é‡æ–°åŠ è¼‰é…ç½®
   */
  reloadConfig(): void {
    this.initializeProvider();
    console.log('[çµ±ä¸€ AI æä¾›å•†] é…ç½®å·²é‡æ–°åŠ è¼‰');
  }

  /**
   * ç™¼é€èŠå¤©è¨Šæ¯ï¼ˆéä¸²æµï¼‰
   */
  async chat(message: string, history?: ChatMessage[]): Promise<ChatResponse> {
    if (!this.provider || !this.provider.isAvailable()) {
      // å¦‚æœæ²’æœ‰å¯ç”¨çš„ API Keyï¼Œä½¿ç”¨æœ¬åœ°å›é€€
      return this.getLocalResponse(message);
    }

    return await this.provider.chat(message, history);
  }

  /**
   * ç™¼é€èŠå¤©è¨Šæ¯ï¼ˆä¸²æµï¼‰
   */
  async *chatStream(message: string, history?: ChatMessage[]): AsyncGenerator<StreamChunk> {
    if (!this.provider || !this.provider.isAvailable()) {
      // å¦‚æœæ²’æœ‰å¯ç”¨çš„ API Keyï¼Œä½¿ç”¨æœ¬åœ°å›é€€
      yield* this.getLocalResponseStream(message);
      return;
    }

    yield* this.provider.chatStream(message, history);
  }

  /**
   * ä½¿ç”¨å‡½æ•¸èª¿ç”¨ï¼ˆåƒ… GLM å•†æ¥­ç‰ˆæ”¯æŒï¼‰
   */
  async chatWithTools(message: string, tools: any[], history?: ChatMessage[]): Promise<{ content: string; tool_calls?: any[]; usage?: any }> {
    if (!this.provider || !this.provider.isAvailable()) {
      // å¦‚æœæ²’æœ‰å¯ç”¨çš„ API Keyï¼Œä½¿ç”¨æœ¬åœ°å›é€€
      return this.getLocalResponse(message);
    }

    // åƒ… GLM å•†æ¥­ç‰ˆæ”¯æŒå·¥å…·èª¿ç”¨
    if (this.providerType === 'glm-commercials' && (this.provider as any).chatWithTools) {
      return await (this.provider as any).chatWithTools(message, tools, history);
    }

    return this.provider?.chat(message, history) || this.getLocalResponse(message);
  }

  /**
   * æª¢æŸ¥æä¾›å•†æ˜¯å¦å¯ç”¨
   */
  isAvailable(): boolean {
    return this.provider?.isAvailable() || false;
  }

  /**
   * ç²å–æä¾›å•†åç¨±
   */
  getName(): string {
    return this.provider?.getName() || 'æœ¬åœ°å›é€€æ¨¡å¼';
  }

  /**
   * ç²å–ç•¶å‰æä¾›å•†é¡å‹
   */
  getProviderType(): string {
    return this.providerType;
  }
}

// ========================================
// å°å‡ºçµ±ä¸€å¯¦ä¾‹
// ========================================

export const aiProvider = new UnifiedAIProvider();
