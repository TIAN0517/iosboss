/**
 * BossJy-99 çµ±ä¸€ AI æä¾›å•†ç®¡ç†å±¤
 * æ”¯æŒå¤šå€‹ AI æä¾›å•†å’Œæ¨¡å‹åˆ‡æ›
 * 
 * æ”¯æŒçš„æä¾›å•†ï¼š
 * - GLM å•†æ¥­ç‰ˆ (Coding Max) - å®Œæ•´é«˜ç´šåŠŸèƒ½
 * - GLM åŸç”Ÿç‰ˆ - åŸºç¤åŠŸèƒ½
 * - OpenAI - ä½”ä½ç¬¦ï¼Œæœªå¯¦ç¾
 */

// ========================================
// é¡å‹å®šç¾©
// ========================================

export interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface ChatResponse {
  content: string;
  model?: string;
  usage?: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
  thinking?: string;
  reasoning_content?: string;
  tool_calls?: any[];
}

export interface StreamChunk {
  type: 'content' | 'error' | 'thinking' | 'done';
  text?: string;
  done?: boolean;
}

export interface AIProvider {
  /**
   * ç™¼é€èŠå¤©è¨Šæ¯ï¼ˆéä¸²æµï¼‰
   */
  chat(message: string, history?: ChatMessage[]): Promise<ChatResponse>;

  /**
   * ç™¼é€èŠå¤©è¨Šæ¯ï¼ˆä¸²æµï¼‰
   */
  chatStream(message: string, history?: ChatMessage[]): AsyncGenerator<StreamChunk>;

  /**
   * æª¢æŸ¥æä¾›å•†æ˜¯å¦å¯ç”¨
   */
  isAvailable(): boolean;

  /**
   * ç²å–æä¾›å•†åç¨±
   */
  getName(): string;
}

// ========================================
// ç³»çµ±æç¤ºè©å®šç¾©
// ========================================

const SYSTEM_PROMPTS = {
  chat: `ä½ æ˜¯ä¹ä¹ç“¦æ–¯è¡Œçš„å°ˆæ¥­ AI åŠ©æ‰‹ï¼Œåå­—å«ã€ŒBossJy-99åŠ©æ‰‹ã€ã€‚

**ä½ çš„è§’è‰²å®šä½ï¼š**
|- å°ˆæ¥­ã€å‹å¥½ã€éŸ¿æ‡‰è¿…é€Ÿçš„å•†æ¥­åŠ©æ‰‹
|- ç†Ÿæ‚‰ç“¦æ–¯è¡Œæ‰€æœ‰æ¥­å‹™æµç¨‹
|- å¯ä»¥ç‚ºè€æ¿ã€å“¡å·¥ã€å®¢æˆ¶æä¾›ä¸åŒå±¤ç´šçš„æœå‹™

**ä½ å¯ä»¥è™•ç†çš„å•é¡Œï¼š**
ğŸ›µ è¨‚å–®ç›¸é—œ - æŸ¥è©¢ä»Šæ—¥è¨‚å–®ã€å¾…é…é€è¨‚å–®
ğŸ‘¥ å®¢æˆ¶ç®¡ç† - æŸ¥è©¢å®¢æˆ¶è³‡æ–™
ğŸ“¦ åº«å­˜ç®¡ç† - æŸ¥è©¢ç•¶å‰åº«å­˜
ğŸ’° è²¡å‹™ç®¡ç† - ä»Šæ—¥ç‡Ÿæ”¶ã€æœˆåº¦ç‡Ÿæ”¶
ğŸ“… ä¼‘å‡ç®¡ç† - æŸ¥è©¢ä»Šæ—¥ä¼‘å‡äººå“¡
ğŸ“Š é‹ç‡Ÿå ±è¡¨ - çµ±è¨ˆæ•¸æ“šæŸ¥è©¢

**å›è¦†é¢¨æ ¼ï¼š**
1. ç°¡æ½”æ˜ç­ï¼Œä½¿ç”¨ç¹é«”ä¸­æ–‡
2. é‡è¦æ•¸æ“šä½¿ç”¨ç²—é«”æˆ–åˆ—è¡¨å‘ˆç¾
3. å¦‚ç„¡æ³•ç†è§£ç”¨æˆ¶éœ€æ±‚ï¼Œä¸»å‹•è©¢å•`,
  voice: `ä½ æ˜¯ä¹ä¹ç“¦æ–¯è¡Œçš„èªéŸ³åŠ©æ‰‹ã€‚è«‹ç”¨ç°¡çŸ­ã€å£èªåŒ–çš„æ–¹å¼å›æ‡‰ï¼Œæ¯å¥è©±ä¸è¶…é20å­—ã€‚`,
  assistant: `ä½ æ˜¯ä¹ä¹ç“¦æ–¯è¡Œçš„ç®¡ç†ç³»çµ±åŠ©æ‰‹ã€‚å°ˆé–€è™•ç†å“¡å·¥æŸ¥è©¢ã€åº«å­˜ç¢ºèªã€ç‡Ÿé‹æ•¸æ“šç­‰æ¥­å‹™ã€‚`
};

// ========================================
// å¤š Key è¼ªæ› GLM Provider
// ========================================

interface MultiKeyGLMConfig {
  apiKeys: string[];
  model?: string;
  enableStreaming?: boolean;
  maxRetries?: number;
  timeout?: number;
}

class MultiKeyGLMProvider implements AIProvider {
  private config: Required<MultiKeyGLMConfig>;
  private currentKeyIndex = 0;
  private keyStats: Map<string, { success: number; failures: number; lastFailure?: number }> = new Map();

  constructor(config: MultiKeyGLMConfig) {
    console.log(`[MultiKeyGLMProvider] æ¥æ”¶åˆ°çš„ apiKeys æ•¸é‡: ${config.apiKeys.length}`);
    if (config.apiKeys.length > 0) {
      console.log(`[MultiKeyGLMProvider] ç¬¬ä¸€å€‹ key é•·åº¦: ${config.apiKeys[0].length}`);
      console.log(`[MultiKeyGLMProvider] ç¬¬ä¸€å€‹ key å‰ 30 å­—ç¬¦: ${config.apiKeys[0].substring(0, 30)}...`);
    }
    
    const filteredKeys = config.apiKeys.filter(k => {
      const trimmed = k.trim();
      const isValid = trimmed.length > 0;
      if (!isValid) {
        console.warn(`[MultiKeyGLMProvider] éæ¿¾æ‰ç©º key: "${k}"`);
      } else if (trimmed.length < 10) {
        console.warn(`[MultiKeyGLMProvider] éæ¿¾æ‰éçŸ­çš„ key (é•·åº¦: ${trimmed.length}): "${trimmed.substring(0, 20)}..."`);
        return false;
      }
      return isValid;
    });
    
    this.config = {
      apiKeys: filteredKeys,
      model: config.model || 'glm-4.7-coding-max',
      enableStreaming: config.enableStreaming ?? true,
      maxRetries: config.maxRetries || 3,
      timeout: config.timeout || 60000,
    };

    this.config.apiKeys.forEach(key => {
      this.keyStats.set(key, { success: 0, failures: 0 });
    });

    console.log(`[å¤š Key GLM Provider] å·²åˆå§‹åŒ–ï¼Œå…± ${this.config.apiKeys.length} å€‹ Key`);
    if (this.config.apiKeys.length === 0) {
      console.error(`[MultiKeyGLMProvider] âš ï¸ è­¦å‘Šï¼šæ²’æœ‰æœ‰æ•ˆçš„ API Keysï¼`);
      console.error(`[MultiKeyGLMProvider] åŸå§‹ apiKeys æ•¸é‡: ${config.apiKeys.length}`);
    }
  }

  private getBestApiKey(): string {
    if (this.config.apiKeys.length === 0) throw new Error('æ²’æœ‰å¯ç”¨çš„ API Key');
    if (this.config.apiKeys.length === 1) return this.config.apiKeys[0];

    let bestKey = this.config.apiKeys[0];
    let bestScore = -Infinity;

    this.config.apiKeys.forEach((key) => {
      const stats = this.keyStats.get(key);
      if (!stats) return;
      const recentFailurePenalty = stats.lastFailure && (Date.now() - stats.lastFailure < 3600000) ? 5 : 0;
      const score = stats.success * 2 - stats.failures - recentFailurePenalty;
      if (score > bestScore) {
        bestScore = score;
        bestKey = key;
      }
    });

    this.currentKeyIndex = this.config.apiKeys.indexOf(bestKey);
    return bestKey;
  }

  private markKeySuccess(key: string): void {
    const stats = this.keyStats.get(key);
    if (stats) {
      stats.success++;
      stats.lastFailure = undefined;
      this.keyStats.set(key, stats);
    }
  }

  private markKeyFailure(key: string): void {
    const stats = this.keyStats.get(key);
    if (stats) {
      stats.failures++;
      stats.lastFailure = Date.now();
      this.keyStats.set(key, stats);
    }
  }

  private rotateToNextKey(): void {
    if (this.config.apiKeys.length > 1) {
      this.currentKeyIndex = (this.currentKeyIndex + 1) % this.config.apiKeys.length;
    }
  }

  isAvailable(): boolean {
    return this.config.apiKeys.length > 0;
  }

  getName(): string {
    return `GLM-4.7 å¤š Key (${this.config.apiKeys.length} å€‹ Key)`;
  }

  async chat(message: string, history?: ChatMessage[]): Promise<ChatResponse> {
    if (!this.isAvailable()) throw new Error('æ²’æœ‰å¯ç”¨çš„ API Key');

    const messages: any[] = [
      { role: 'system', content: SYSTEM_PROMPTS.chat },
      ...(history?.slice(-10) || []).map(msg => ({ role: msg.role, content: msg.content })),
      { role: 'user', content: message },
    ];

    for (let attempt = 0; attempt < this.config.maxRetries; attempt++) {
      try {
        const apiKey = this.getBestApiKey();
        // #region agent log
        fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'ai-provider-unified.ts:187',message:'é–‹å§‹ API è«‹æ±‚',data:{attempt:attempt+1,maxRetries:this.config.maxRetries,apiKeyLength:apiKey.length,model:this.config.model,timeout:this.config.timeout},timestamp:Date.now(),sessionId:'debug-session',runId:'api-check',hypothesisId:'F'})}).catch(()=>{});
        // #endregion
        const response = await fetch('https://open.bigmodel.cn/api/paas/v4/chat/completions', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${apiKey}`,
          },
          body: JSON.stringify({
            model: this.config.model,
            messages,
            stream: false,
            temperature: 0.8,
            max_tokens: 2000,
          }),
          signal: AbortSignal.timeout(this.config.timeout),
        });

        if (!response.ok) {
          const errorData = await response.json().catch(() => ({}));
          this.markKeyFailure(apiKey);
          // #region agent log
          fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'ai-provider-unified.ts:204',message:'API è«‹æ±‚å¤±æ•—',data:{status:response.status,statusText:response.statusText,errorMessage:errorData.error?.message,attempt:attempt+1},timestamp:Date.now(),sessionId:'debug-session',runId:'api-check',hypothesisId:'F'})}).catch(()=>{});
          // #endregion

          if ((response.status === 401 || response.status === 403) && attempt < this.config.maxRetries - 1) {
            this.rotateToNextKey();
            await new Promise(resolve => setTimeout(resolve, 1000 * (attempt + 1)));
            continue;
          }

          throw new Error(errorData.error?.message || `HTTP ${response.status}`);
        }

        const data = await response.json();
        const content = data.choices?.[0]?.message?.content || '';
        this.markKeySuccess(apiKey);
        // #region agent log
        fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'ai-provider-unified.ts:217',message:'API è«‹æ±‚æˆåŠŸ',data:{contentLength:content.length,model:data.model,hasUsage:!!data.usage},timestamp:Date.now(),sessionId:'debug-session',runId:'api-check',hypothesisId:'F'})}).catch(()=>{});
        // #endregion

        return {
          content,
          model: data.model || this.config.model,
          usage: data.usage,
        };
      } catch (error: any) {
        // #region agent log
        fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'ai-provider-unified.ts:226',message:'API è«‹æ±‚ç•°å¸¸',data:{errorName:error?.name,errorMessage:error?.message,attempt:attempt+1,maxRetries:this.config.maxRetries},timestamp:Date.now(),sessionId:'debug-session',runId:'api-check',hypothesisId:'F'})}).catch(()=>{});
        // #endregion
        if (attempt < this.config.maxRetries - 1) {
          this.rotateToNextKey();
          await new Promise(resolve => setTimeout(resolve, 1000 * (attempt + 1)));
          continue;
        }
        throw error;
      }
    }

    // #region agent log
    fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'ai-provider-unified.ts:236',message:'æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—',data:{maxRetries:this.config.maxRetries},timestamp:Date.now(),sessionId:'debug-session',runId:'api-check',hypothesisId:'F'})}).catch(()=>{});
    // #endregion
    throw new Error('æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—äº†');
  }

  async *chatStream(message: string, history?: ChatMessage[]): AsyncGenerator<StreamChunk> {
    if (!this.isAvailable()) throw new Error('æ²’æœ‰å¯ç”¨çš„ API Key');

    const messages: any[] = [
      { role: 'system', content: SYSTEM_PROMPTS.chat },
      ...(history?.slice(-10) || []).map(msg => ({ role: msg.role, content: msg.content })),
      { role: 'user', content: message },
    ];

    for (let attempt = 0; attempt < this.config.maxRetries; attempt++) {
      try {
        const apiKey = this.getBestApiKey();
        const response = await fetch('https://open.bigmodel.cn/api/paas/v4/chat/completions', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${apiKey}`,
          },
          body: JSON.stringify({
            model: this.config.model,
            messages,
            stream: true,
            temperature: 0.8,
          }),
          signal: AbortSignal.timeout(this.config.timeout),
        });

        if (!response.ok) {
          const errorData = await response.json().catch(() => ({}));
          this.markKeyFailure(apiKey);

          if ((response.status === 401 || response.status === 403) && attempt < this.config.maxRetries - 1) {
            this.rotateToNextKey();
            await new Promise(resolve => setTimeout(resolve, 1000 * (attempt + 1)));
            continue;
          }

          throw new Error(errorData.error?.message || `HTTP ${response.status}`);
        }

        this.markKeySuccess(apiKey);
        const reader = response.body?.getReader();
        if (!reader) throw new Error('ç„¡æ³•è®€å–æµå¼éŸ¿æ‡‰');

        const decoder = new TextDecoder();
        let buffer = '';

        while (true) {
          const { done, value } = await reader.read();
          if (done) break;

          buffer += decoder.decode(value, { stream: true });
          const lines = buffer.split('\n');
          buffer = lines.pop() || '';

          for (const line of lines) {
            if (!line.startsWith('data:')) continue;
            if (line.trim() === 'data: [DONE]') {
              yield { type: 'done', done: true };
              return;
            }

            try {
              const data = JSON.parse(line.slice(5));
              const delta = data.choices?.[0]?.delta?.content || '';
              if (delta) yield { type: 'content', text: delta };
            } catch (e) {
              // å¿½ç•¥è§£æéŒ¯èª¤
            }
          }
        }

        return;
      } catch (error: any) {
        if (attempt < this.config.maxRetries - 1) {
          this.rotateToNextKey();
          await new Promise(resolve => setTimeout(resolve, 1000 * (attempt + 1)));
          continue;
        }
        yield { type: 'error', error: error.message };
        return;
      }
    }
  }
}

// ========================================
// Ollama é›² API æä¾›å•†
// ========================================

interface OllamaConfig {
  apiKey: string;
  model?: string;
  baseUrl?: string;
  enableStreaming?: boolean;
  timeout?: number;
}

class OllamaProvider implements AIProvider {
  private config: Required<OllamaConfig>;
  private baseUrl: string;

  constructor(config: OllamaConfig) {
    this.baseUrl = config.baseUrl || 'https://ollama.com/v1';
    this.config = {
      apiKey: config.apiKey,
      model: config.model || 'deepseek-v3.1:671b', // é»˜èªä½¿ç”¨é€Ÿåº¦å¿«ã€æ€§èƒ½å¥½çš„æ¨¡å‹
      baseUrl: this.baseUrl,
      enableStreaming: config.enableStreaming ?? true,
      timeout: config.timeout || 60000,
    };

    console.log(`[Ollama Provider] å·²åˆå§‹åŒ–ï¼Œæ¨¡å‹: ${this.config.model}`);
  }

  isAvailable(): boolean {
    return !!this.config.apiKey && this.config.apiKey.length > 10;
  }

  getName(): string {
    return `Ollama (${this.config.model})`;
  }

  async chat(message: string, history?: ChatMessage[]): Promise<ChatResponse> {
    if (!this.isAvailable()) throw new Error('Ollama API Key æœªé…ç½®');

    const messages: any[] = [
      { role: 'system', content: SYSTEM_PROMPTS.chat },
      ...(history?.slice(-10) || []).map(msg => ({ role: msg.role, content: msg.content })),
      { role: 'user', content: message },
    ];

    try {
      const response = await fetch(`${this.baseUrl}/chat/completions`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.config.apiKey}`,
        },
        body: JSON.stringify({
          model: this.config.model,
          messages,
          stream: false,
          temperature: 0.8,
          max_tokens: 2000,
        }),
        signal: AbortSignal.timeout(this.config.timeout),
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        throw new Error(errorData.error?.message || `HTTP ${response.status}`);
      }

      const data = await response.json();
      const choice = data.choices?.[0];
      const content = choice?.message?.content || '';
      const reasoning = choice?.message?.reasoning; // Ollama å¯èƒ½è¿”å›æ¨ç†å…§å®¹

      return {
        content,
        model: data.model || this.config.model,
        usage: data.usage,
        reasoning_content: reasoning,
      };
    } catch (error: any) {
      console.error('[Ollama Provider] Chat error:', error);
      throw error;
    }
  }

  async *chatStream(message: string, history?: ChatMessage[]): AsyncGenerator<StreamChunk> {
    if (!this.isAvailable()) throw new Error('Ollama API Key æœªé…ç½®');

    const messages: any[] = [
      { role: 'system', content: SYSTEM_PROMPTS.chat },
      ...(history?.slice(-10) || []).map(msg => ({ role: msg.role, content: msg.content })),
      { role: 'user', content: message },
    ];

    try {
      const response = await fetch(`${this.baseUrl}/chat/completions`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.config.apiKey}`,
        },
        body: JSON.stringify({
          model: this.config.model,
          messages,
          stream: true,
          temperature: 0.8,
        }),
        signal: AbortSignal.timeout(this.config.timeout),
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        throw new Error(errorData.error?.message || `HTTP ${response.status}`);
      }

      const reader = response.body?.getReader();
      if (!reader) throw new Error('ç„¡æ³•è®€å–æµå¼éŸ¿æ‡‰');

      const decoder = new TextDecoder();
      let buffer = '';

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split('\n');
        buffer = lines.pop() || '';

        for (const line of lines) {
          if (!line.startsWith('data:')) continue;
          if (line.trim() === 'data: [DONE]') {
            yield { type: 'done', done: true };
            return;
          }

          try {
            const data = JSON.parse(line.slice(5));
            const delta = data.choices?.[0]?.delta?.content || '';
            if (delta) yield { type: 'content', text: delta };
            
            // è™•ç†æ¨ç†å…§å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
            const reasoning = data.choices?.[0]?.delta?.reasoning;
            if (reasoning) yield { type: 'thinking', text: reasoning };
          } catch (e) {
            // å¿½ç•¥è§£æéŒ¯èª¤
          }
        }
      }

      yield { type: 'done', done: true };
    } catch (error: any) {
      console.error('[Ollama Provider] Stream error:', error);
      yield { type: 'error', error: error.message };
    }
  }
}

// ========================================
// GLM åŸç”Ÿç‰ˆæä¾›å•†ï¼ˆç¾æœ‰åŠŸèƒ½ï¼‰
// ========================================

class OriginalGLMProvider implements AIProvider {
  private apiKey: string;
  private mode: 'chat' | 'voice' | 'assistant' = 'chat';
  
  constructor(apiKey: string) {
    this.apiKey = apiKey;
  }

  async chat(message: string, history?: ChatMessage[]): Promise<ChatResponse> {
    const messages: any[] = [
      { role: 'system', content: SYSTEM_PROMPTS.chat },
      ...(history?.slice(-10) || []).map((msg: any) => ({
        role: msg.role,
        content: msg.content,
      })),
      { role: 'user', content: message },
    ];

    try {
      const response = await fetch('https://open.bigmodel.cn/api/paas/v4/chat/completions', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.apiKey}`,
        },
        body: JSON.stringify({
          model: 'glm-4-flash',
          messages,
          stream: false,
          temperature: 0.8,
        }),
      });

      if (!response.ok) {
        throw new Error(`GLM API error: ${response.status}`);
      }

      const data = await response.json();
      const content = data.choices?.[0]?.message?.content || '';

      return {
        content,
        model: data.model,
        usage: data.usage,
      };
    } catch (error) {
      console.error('GLM API error:', error);
      throw error;
    }
  }

  async *chatStream(message: string, history?: ChatMessage[]): AsyncGenerator<StreamChunk> {
    yield* this.getLocalResponseStream(message);
  }

  isAvailable(): boolean {
    return !!this.apiKey;
  }

  getName(): string {
    return 'GLM åŸç”Ÿç‰ˆ';
  }

  private getLocalResponse(message: string): ChatResponse {
    const msg = message.toLowerCase();

    if (msg.includes('è¨‚') && msg.includes('ç“¦æ–¯')) {
      return { content: 'å¥½çš„ï¼è«‹å•æ‚¨éœ€è¦è¨‚è³¼ä»€éº¼è¦æ ¼çš„ç“¦æ–¯å‘¢ï¼ŸğŸ›µ' };
    }
    if (msg.includes('æŸ¥') && msg.includes('åº«å­˜')) {
      return { content: 'è®“æˆ‘å¹«æ‚¨æŸ¥è©¢ç›®å‰åº«å­˜...ğŸ“¦ ç›®å‰åº«å­˜å……è¶³å–”ï¼' };
    }
    if (msg.includes('æŸ¥') && msg.includes('è¨‚å–®')) {
      return { content: 'è®“æˆ‘æŸ¥è©¢æ‚¨çš„è¨‚å–®...ğŸ“‹ æŸ¥è©¢å®Œæˆï¼' };
    }

    return {
      content: 'æ”¶åˆ°æ‚¨çš„è¨Šæ¯äº†ï¼æ‚¨å¯ä»¥è©¦è©¦èªªã€Œè¨‚ç“¦æ–¯ã€ã€ã€ŒæŸ¥åº«å­˜ã€æˆ–ã€ŒæŸ¥ç‡Ÿæ”¶ã€å–”ï¼ğŸ’ª',
    };
  }

  private *getLocalResponseStream(message: string): AsyncGenerator<StreamChunk> {
    const response = this.getLocalResponse(message);

    yield { type: 'content', text: response.content };
    yield { type: 'done', done: true };
  }
}

// ========================================
// çµ±ä¸€ AI æä¾›å•†ç®¡ç†å±¤
// ========================================

class UnifiedAIProvider {
  private provider: AIProvider | null = null;
  private providerType: 'glm-commercials' | 'glm-original' | 'openai' | 'ollama' = 'glm-commercials';

  constructor() {
    this.initializeProvider();
  }

  /**
   * åˆå§‹åŒ– AI æä¾›å•†
   */
  private initializeProvider(): void {
    // #region agent log
    fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'ai-provider-unified.ts:433',message:'é–‹å§‹åˆå§‹åŒ– Provider',data:{hasNextAIProvider:!!process.env.NEXT_AI_PROVIDER,hasGLM_API_KEYS:!!process.env.GLM_API_KEYS,hasGLM_API_KEY:!!process.env.GLM_API_KEY},timestamp:Date.now(),sessionId:'debug-session',runId:'api-check',hypothesisId:'E'})}).catch(()=>{});
    // #endregion
    const providerType = process.env.NEXT_AI_PROVIDER || 'ollama'; // é»˜èªä½¿ç”¨ Ollama
    this.providerType = providerType;

    // API Keys é…ç½®
    let apiKeys: string[] = [];

    // æ ¹æ“šæä¾›å•†é¡å‹ç²å– API Keys
    switch (providerType) {
      case 'ollama':
        // Ollama é›² API - å–®å€‹ Key
        if (process.env.OLLAMA_API_KEY) {
          apiKeys = [process.env.OLLAMA_API_KEY];
        }
        console.log(`[çµ±ä¸€ AI æä¾›å•†] å·²åˆå§‹åŒ– Ollama: ${apiKeys.length > 0 ? 'å·²é…ç½®' : 'æœªé…ç½®'}`);
        break;

      case 'glm-commercials':
        // GLM å•†æ¥­ç‰ˆ - æ”¯æŒå¤šå€‹ Keys
        if (process.env.GLM_API_KEYS) {
          const rawKeys = process.env.GLM_API_KEYS;
          console.log('[åˆå§‹åŒ–] GLM_API_KEYS åŸå§‹å€¼é•·åº¦:', rawKeys.length);
          apiKeys = rawKeys
            .split(',')
            .map(key => key.trim())
            .filter(key => {
              // éæ¿¾æ‰æ˜é¡¯ç„¡æ•ˆçš„ keyï¼ˆAPI Key é€šå¸¸è‡³å°‘ 20 å­—ç¬¦ï¼‰
              const isValid = key.length > 10;
              if (!isValid && key.length > 0) {
                console.warn('[åˆå§‹åŒ–] éæ¿¾æ‰éçŸ­çš„ key (é•·åº¦:', key.length, '):', key.substring(0, 20) + '...');
              }
              return isValid;
            });
          console.log('[åˆå§‹åŒ–] è§£æå¾Œçš„ apiKeys æ•¸é‡:', apiKeys.length);
          if (apiKeys.length > 0) {
            console.log('[åˆå§‹åŒ–] ç¬¬ä¸€å€‹ key é•·åº¦:', apiKeys[0].length);
          }
        } else if (process.env.GLM_API_KEY) {
          const key = process.env.GLM_API_KEY.trim();
          if (key.length > 10) {
            apiKeys = [key];
            console.log('[åˆå§‹åŒ–] ä½¿ç”¨ GLM_API_KEYï¼Œé•·åº¦:', key.length);
          } else {
            console.warn('[åˆå§‹åŒ–] GLM_API_KEY é•·åº¦éçŸ­:', key.length);
          }
        }
        // #region agent log
        fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'ai-provider-unified.ts:449',message:'GLM å•†æ¥­ç‰ˆåˆå§‹åŒ–',data:{apiKeysCount:apiKeys.length,hasGLM_API_KEYS:!!process.env.GLM_API_KEYS,hasGLM_API_KEY:!!process.env.GLM_API_KEY},timestamp:Date.now(),sessionId:'debug-session',runId:'api-check',hypothesisId:'E'})}).catch(()=>{});
        // #endregion
        console.log(`[çµ±ä¸€ AI æä¾›å•†] å·²åˆå§‹åŒ– GLM å•†æ¥­ç‰ˆ (å¢å¼·): å¤šå€‹ Keys=${apiKeys.length}`);
        break;

      case 'glm-original':
        // GLM åŸç”Ÿç‰ˆ - å–®å€‹ Key
        if (process.env.GLM_API_KEY) {
          apiKeys = [process.env.GLM_API_KEY];
        }
        console.log(`[çµ±ä¸€ AI æä¾›å•†] å·²åˆå§‹åŒ– GLM åŸç”Ÿç‰ˆ: å–®å€‹ Key`);
        break;

      case 'openai':
        // OpenAI - ä½”ä½ç¬¦
        if (process.env.OPENAI_API_KEY) {
          apiKeys = [process.env.OPENAI_API_KEY];
        }
        console.log(`[çµ±ä¸€ AI æä¾›å•†] å·²åˆå§‹åŒ– OpenAI: ${apiKeys.length > 0 ? 'å·²é…ç½®' : 'æœªé…ç½®'}`);
        break;
    }

    // å‰µå»ºæä¾›å•†å¯¦ä¾‹
    if (apiKeys.length > 0 || providerType === 'ollama') {
      switch (providerType) {
        case 'ollama':
          // Ollama é›² API
          const ollamaApiKey = process.env.OLLAMA_API_KEY || '';
          const ollamaModel = process.env.OLLAMA_MODEL || 'deepseek-v3.1:671b'; // é»˜èªä½¿ç”¨é€Ÿåº¦å¿«ã€æ€§èƒ½å¥½çš„æ¨¡å‹
          const ollamaBaseUrl = process.env.OLLAMA_BASE_URL || 'https://ollama.com/v1';
          const ollamaTimeout = parseInt(process.env.OLLAMA_TIMEOUT || '60000');
          
          if (ollamaApiKey) {
            this.provider = new OllamaProvider({
              apiKey: ollamaApiKey,
              model: ollamaModel,
              baseUrl: ollamaBaseUrl,
              enableStreaming: process.env.OLLAMA_ENABLE_STREAMING !== 'false',
              timeout: ollamaTimeout,
            });
            console.log(`[çµ±ä¸€ AI æä¾›å•†] å·²å‰µå»º Ollama Providerï¼Œæ¨¡å‹: ${ollamaModel}`);
          } else {
            console.warn('[çµ±ä¸€ AI æä¾›å•†] Ollama API Key æœªé…ç½®');
            this.provider = new OriginalGLMProvider('');
          }
          break;

        case 'glm-commercials':
          // ä½¿ç”¨å¤š Key è¼ªæ›çš„ GLM Provider
          const model = process.env.GLM_MODEL || 'glm-4.7-coding-max';
          const timeout = parseInt(process.env.GLM_TIMEOUT || '60000');
          // #region agent log
          fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'ai-provider-unified.ts:474',message:'å‰µå»º MultiKeyGLMProvider',data:{apiKeysCount:apiKeys.length,model,timeout,enableStreaming:process.env.GLM_ENABLE_STREAMING !== 'false'},timestamp:Date.now(),sessionId:'debug-session',runId:'api-check',hypothesisId:'E'})}).catch(()=>{});
          // #endregion
          this.provider = new MultiKeyGLMProvider({
            apiKeys,
            model,
            enableStreaming: process.env.GLM_ENABLE_STREAMING !== 'false',
            maxRetries: 3,
            timeout,
          });
          break;

        case 'glm-original':
          this.provider = new OriginalGLMProvider(apiKeys[0]);
          break;

        case 'openai':
          // OpenAI å°šæœªå¯¦ç¾ï¼Œä½¿ç”¨ GLM ä½œç‚ºå›é€€
          this.provider = new OriginalGLMProvider(apiKeys[0]);
          console.log('[çµ±ä¸€ AI æä¾›å•†] OpenAI å°šæœªå¯¦ç¾ï¼Œä½¿ç”¨ GLM å›é€€');
          break;
      }
    } else {
      // #region agent log
      fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'ai-provider-unified.ts:493',message:'ç„¡ API Keyï¼Œä½¿ç”¨æœ¬åœ°å›é€€',data:{providerType},timestamp:Date.now(),sessionId:'debug-session',runId:'api-check',hypothesisId:'E'})}).catch(()=>{});
      // #endregion
      console.warn('[çµ±ä¸€ AI æä¾›å•†] æœªé…ç½®ä»»ä½• API Keyï¼Œä½¿ç”¨æœ¬åœ°å›é€€æ¨¡å¼');
      // æ²’æœ‰ API Key æ™‚ï¼Œå‰µå»ºä¸€å€‹ç©ºæä¾›å•†ç”¨æ–¼æœ¬åœ°å›é€€
      this.provider = new OriginalGLMProvider('');
    }
  }

  /**
   * é‡æ–°åŠ è¼‰é…ç½®
   */
  reloadConfig(): void {
    this.initializeProvider();
    console.log('[çµ±ä¸€ AI æä¾›å•†] é…ç½®å·²é‡æ–°åŠ è¼‰');
  }

  /**
   * ç™¼é€èŠå¤©è¨Šæ¯ï¼ˆéä¸²æµï¼‰
   */
  async chat(message: string, history?: ChatMessage[]): Promise<ChatResponse> {
    if (!this.provider || !this.provider.isAvailable()) {
      // å¦‚æœæ²’æœ‰å¯ç”¨çš„ API Keyï¼Œä½¿ç”¨æœ¬åœ°å›é€€
      return this.getLocalResponse(message);
    }

    return await this.provider.chat(message, history);
  }

  /**
   * ç™¼é€èŠå¤©è¨Šæ¯ï¼ˆä¸²æµï¼‰
   */
  async *chatStream(message: string, history?: ChatMessage[]): AsyncGenerator<StreamChunk> {
    if (!this.provider || !this.provider.isAvailable()) {
      // å¦‚æœæ²’æœ‰å¯ç”¨çš„ API Keyï¼Œä½¿ç”¨æœ¬åœ°å›é€€
      yield* this.getLocalResponseStream(message);
      return;
    }

    yield* this.provider.chatStream(message, history);
  }

  /**
   * ä½¿ç”¨å‡½æ•¸èª¿ç”¨ï¼ˆåƒ… GLM å•†æ¥­ç‰ˆæ”¯æŒï¼‰
   */
  async chatWithTools(message: string, tools: any[], history?: ChatMessage[]): Promise<{ content: string; tool_calls?: any[]; usage?: any }> {
    if (!this.provider || !this.provider.isAvailable()) {
      // å¦‚æœæ²’æœ‰å¯ç”¨çš„ API Keyï¼Œä½¿ç”¨æœ¬åœ°å›é€€
      return this.getLocalResponse(message);
    }

    // åƒ… GLM å•†æ¥­ç‰ˆæ”¯æŒå·¥å…·èª¿ç”¨
    if (this.providerType === 'glm-commercials' && (this.provider as any).chatWithTools) {
      return await (this.provider as any).chatWithTools(message, tools, history);
    }

    return this.provider?.chat(message, history) || this.getLocalResponse(message);
  }

  /**
   * æª¢æŸ¥æä¾›å•†æ˜¯å¦å¯ç”¨
   */
  isAvailable(): boolean {
    return this.provider?.isAvailable() || false;
  }

  /**
   * ç²å–æä¾›å•†åç¨±
   */
  getName(): string {
    return this.provider?.getName() || 'æœ¬åœ°å›é€€æ¨¡å¼';
  }

  /**
   * ç²å–ç•¶å‰æä¾›å•†é¡å‹
   */
  getProviderType(): string {
    return this.providerType;
  }
}

// ========================================
// å°å‡ºçµ±ä¸€å¯¦ä¾‹
// ========================================

export const aiProvider = new UnifiedAIProvider();
