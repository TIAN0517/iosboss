/**
 * æµå¼èªéŸ³èŠå¤© API
 * åƒ ChatGPT Voice ä¸€æ¨£æµå¼è¼¸å‡º
 *
 * æµç¨‹ï¼š
 * 1. æ¥æ”¶éŸ³é »æ–‡ä»¶
 * 2. Deepgram ASR è½‰æ–‡å­—ï¼ˆæµå¼ï¼‰
 * 3. GLM AI ç”Ÿæˆå›æ‡‰ï¼ˆæµå¼ï¼‰
 * 4. TTS ç”ŸæˆéŸ³é »ï¼ˆæµå¼ç‰‡æ®µï¼‰
 * 5. é€šé SSE å¯¦æ™‚æ¨é€
 */

import { NextRequest } from 'next/server'
import {
  transcribeWithDeepgram,
  synthesizeWithElevenLabs,
  synthesizeWithAzure,
  synthesizeWithGLM,
  checkServiceAvailability,
} from '@/lib/voice-service'
import { aiProvider } from '@/lib/ai-provider-unified'

// è‡ªç„¶å°è©±ç³»çµ±æç¤º - è¶…ç°¡æ½”ç‰ˆ
const NATURAL_SYSTEM_PROMPT = `ä½ æ˜¯ä¹ä¹ç“¦æ–¯è¡ŒåŠ©æ‰‹ã€‚è¦å‰‡ï¼š
1. æ¯æ¬¡å›æ‡‰ä¸è¶…é20å­—
2. ç›´æ¥å›ç­”ï¼Œä¸è¦å»¢è©±
3. ä½¿ç”¨emoji
4. è¨‚ç“¦æ–¯â†’å•è¦æ ¼ï¼›æŸ¥åº«å­˜â†’èªªæ•¸é‡ï¼›æŸ¥è¨‚å–®â†’å ±ç‹€æ…‹
5. ç”¨æˆ¶èªªç´¯/å¿™â†’é—œå¿ƒï¼›èªªç¬¨â†’èª¿çš®
ç¯„ä¾‹ï¼š
- "è¨‚ç“¦æ–¯"â†’"å¥½çš„ï¼è¦20æ¡¶é‚„æ˜¯50æ¡¶ï¼ŸğŸ›µ"
- "æŸ¥åº«å­˜"â†’"20æ¡¶165å€‹ï¼Œ50æ¡¶42å€‹ğŸ“¦"
- "ç´¯æ­»äº†"â†’"è¾›è‹¦å•¦ï¼å–å£æ°´ä¼‘æ¯ğŸ’ª"
- "ä½ å¥½ç¬¨"â†’"å‘œå‘œæˆ‘æœƒåŠ æ²¹çš„ğŸ¥º"`

// SSE äº‹ä»¶é¡å‹
type SSEEvent =
  | { type: 'transcript'; data: string }
  | { type: 'text'; data: string }
  | { type: 'audio'; data: string; mimeType: string }
  | { type: 'done'; data: null }
  | { type: 'error'; data: string }

function sendSSE(event: SSEEvent, controller: ReadableStreamDefaultController) {
  const data = JSON.stringify(event)
  controller.enqueue(`data: ${data}\n\n`.encode())
}

/**
 * POST /api/voice/stream
 * æµå¼èªéŸ³èŠå¤©ç«¯é»
 */
export async function POST(request: NextRequest) {
  // å‰µå»º SSE æµ
  const encoder = new TextEncoder()

  const stream = new ReadableStream({
    async start(controller) {
      try {
        // 1. è§£æè¡¨å–®æ•¸æ“š
        const formData = await request.formData()
        const audioFile = formData.get('audio') as File | null
        const conversationHistoryStr = formData.get('conversationHistory') as string | null

        if (!audioFile) {
          sendSSE({ type: 'error', data: 'ç¼ºå°‘éŸ³é »æ–‡ä»¶' }, controller)
          controller.close()
          return
        }

        console.log('[Voice Stream] æ”¶åˆ°éŸ³é »:', {
          size: audioFile.size,
          type: audioFile.type,
        })

        // 2. Deepgram ASR - éŸ³é »è½‰æ–‡å­—
        sendSSE({ type: 'text', data: 'ğŸ¤ æ­£åœ¨è­˜åˆ¥...' }, controller)

        let transcript = ''
        try {
          const audioBuffer = Buffer.from(await audioFile.arrayBuffer())
          const asrResult = await transcribeWithDeepgram(audioBuffer, audioFile.type || 'audio/webm')
          transcript = asrResult.text

          console.log('[Voice Stream] ASR çµæœ:', transcript)

          if (!transcript || transcript.trim().length === 0) {
            sendSSE({ type: 'error', data: 'ç„¡æ³•è­˜åˆ¥èªéŸ³å…§å®¹' }, controller)
            controller.close()
            return
          }

          // ç™¼é€è­˜åˆ¥çµæœ
          sendSSE({ type: 'transcript', data: transcript }, controller)
        } catch (asrError: any) {
          console.error('[Voice Stream] ASR å¤±æ•—:', asrError)
          sendSSE({ type: 'error', data: `èªéŸ³è­˜åˆ¥å¤±æ•—: ${asrError.message}` }, controller)
          controller.close()
          return
        }

        // 3. AI ç”Ÿæˆå›æ‡‰ï¼ˆæµå¼ï¼‰
        sendSSE({ type: 'text', data: 'ğŸ¤” æ­£åœ¨æ€è€ƒ...' }, controller)

        let aiResponse = ''
        try {
          // è§£æå°è©±æ­·å²
          let conversationHistory: any[] = []
          if (conversationHistoryStr) {
            try {
              conversationHistory = JSON.parse(conversationHistoryStr)
            } catch (e) {
              console.warn('[Voice Stream] å°è©±æ­·å²è§£æå¤±æ•—:', e)
            }
          }

          // æ§‹å»ºæ¶ˆæ¯
          const messages = [
            { role: 'system' as const, content: NATURAL_SYSTEM_PROMPT },
            ...conversationHistory.slice(-10).map((msg: any) => ({
              role: msg.role as 'user' | 'assistant' | 'system',
              content: msg.content || msg.text || '',
            })),
          ]

          // ä½¿ç”¨æµå¼ AIï¼ˆasync generatorï¼‰
          for await (const chunk of aiProvider.chatStream(transcript, messages)) {
            if (chunk.type === 'content' && chunk.text) {
              aiResponse += chunk.text
              sendSSE({ type: 'text', data: chunk.text }, controller)
            } else if (chunk.type === 'error') {
              console.error('[Voice Stream] AI stream error:', chunk.text)
              sendSSE({ type: 'error', data: chunk.text || 'AI ç”ŸæˆéŒ¯èª¤' }, controller)
              break
            } else if (chunk.type === 'done') {
              break
            }
          }

          console.log('[Voice Stream] AI å®Œæ•´å›æ‡‰:', aiResponse?.substring(0, 100))
        } catch (aiError: any) {
          console.error('[Voice Stream] AI å¤±æ•—:', aiError)
          // ä½¿ç”¨æœ¬åœ°å›é€€
          const fallbackResponse = getLocalResponse(transcript)
          sendSSE({ type: 'text', data: fallbackResponse }, controller)
          aiResponse = fallbackResponse
        }

        // 4. TTS ç”ŸæˆéŸ³é »ï¼ˆæµå¼ç‰‡æ®µï¼‰
        sendSSE({ type: 'text', data: 'ğŸ”Š æ­£åœ¨ç”ŸæˆèªéŸ³...' }, controller)

        try {
          const services = checkServiceAvailability()

          // å„ªå…ˆç´šï¼šElevenLabs > Azure > GLM
          if (services.elevenlabs) {
            console.log('[Voice Stream] ä½¿ç”¨ ElevenLabs TTS')
            try {
              const result = await synthesizeWithElevenLabs(aiResponse)
              // å°‡éŸ³é »è½‰æ›ç‚º base64 ä¸¦åˆ†å¡Šç™¼é€
              const base64 = result.audioBuffer.toString('base64')
              const chunkSize = 8192 // 8KB chunks

              for (let i = 0; i < base64.length; i += chunkSize) {
                const chunk = base64.slice(i, i + chunkSize)
                sendSSE({
                  type: 'audio',
                  data: chunk,
                  mimeType: result.mimeType,
                }, controller)
              }

              console.log('[Voice Stream] TTS æˆåŠŸ: elevenlabs')
            } catch (e: any) {
              console.warn('[Voice Stream] ElevenLabs å¤±æ•—:', e.message)
              throw e
            }
          } else if (services.azure) {
            console.log('[Voice Stream] ä½¿ç”¨ Azure TTS')
            try {
              const result = await synthesizeWithAzure(aiResponse)
              const base64 = result.audioBuffer.toString('base64')
              const chunkSize = 8192

              for (let i = 0; i < base64.length; i += chunkSize) {
                const chunk = base64.slice(i, i + chunkSize)
                sendSSE({
                  type: 'audio',
                  data: chunk,
                  mimeType: result.mimeType,
                }, controller)
              }

              console.log('[Voice Stream] TTS æˆåŠŸ: azure')
            } catch (e: any) {
              console.warn('[Voice Stream] Azure å¤±æ•—:', e.message)
              throw e
            }
          } else if (process.env.GLM_API_KEY) {
            console.log('[Voice Stream] ä½¿ç”¨ GLM TTS')
            try {
              const result = await synthesizeWithGLM(aiResponse)
              const base64 = result.audioBuffer.toString('base64')
              const chunkSize = 8192

              for (let i = 0; i < base64.length; i += chunkSize) {
                const chunk = base64.slice(i, i + chunkSize)
                sendSSE({
                  type: 'audio',
                  data: chunk,
                  mimeType: result.mimeType,
                }, controller)
              }

              console.log('[Voice Stream] TTS æˆåŠŸ: glm')
            } catch (e: any) {
              console.warn('[Voice Stream] GLM å¤±æ•—:', e.message)
              throw e
            }
          } else {
            console.warn('[Voice Stream] æ‰€æœ‰ TTS æœå‹™éƒ½å¤±æ•—äº†')
          }
        } catch (ttsError: any) {
          console.error('[Voice Stream] TTS éŒ¯èª¤:', ttsError)
          // TTS å¤±æ•—ä¸æ˜¯è‡´å‘½éŒ¯èª¤ï¼Œç¹¼çºŒ
        }

        // 5. å®Œæˆæ¨™è¨˜
        sendSSE({ type: 'done', data: null }, controller)
        controller.close()

      } catch (error: any) {
        console.error('[Voice Stream] è™•ç†éŒ¯èª¤:', error)
        sendSSE({ type: 'error', data: error.message || 'æœªçŸ¥éŒ¯èª¤' }, controller)
        controller.close()
      }
    },
  })

  return new Response(stream, {
    headers: {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive',
      'X-Accel-Buffering': 'no', // ç¦ç”¨ nginx ç·©è¡
    },
  })
}

/**
 * GET /api/voice/stream
 * æª¢æŸ¥æœå‹™ç‹€æ…‹
 */
export async function GET() {
  const services = checkServiceAvailability()

  return Response.json({
    services,
    status: 'ready',
    message: services.deepgram
      ? 'æµå¼èªéŸ³èŠå¤©æœå‹™æ­£å¸¸'
      : 'è­¦å‘Š: Deepgram æœªé…ç½®',
  })
}

// æœ¬åœ°å›é€€å›æ‡‰ - è¶…å¿«é€Ÿç‰ˆ
function getLocalResponse(message: string): string {
  const msg = message.toLowerCase()

  // å¿«é€ŸåŒ¹é…ï¼ˆå…ˆçŸ­å¾Œé•·ï¼‰
  if (msg.includes('ä½ å¥½') || msg.includes('å—¨') || msg.includes('æ‚¨å¥½')) return 'å—¨ï¼æœ‰ä»€éº¼èƒ½å¹«æ‚¨ï¼ŸğŸ˜Š'
  if (msg.includes('è¨‚ç“¦æ–¯')) return 'å¥½çš„ï¼20æ¡¶é‚„æ˜¯50æ¡¶ï¼ŸğŸ›µ'
  if (msg.includes('åº«å­˜')) return '20æ¡¶165å€‹ï¼Œ50æ¡¶42å€‹ğŸ“¦'
  if (msg.includes('è¨‚å–®')) return 'ä»Šå¤©3ç­†è¨‚å–®å·²å®ŒæˆğŸ“‹'
  if (msg.includes('ç‡Ÿæ”¶') || msg.includes('è³º')) return 'ä»Šå¤©ç‡Ÿæ”¶$12,500ğŸ’°'
  if (msg.includes('ç´¯') || msg.includes('å¿™')) return 'è¾›è‹¦å•¦ï¼å–å£æ°´ä¼‘æ¯ğŸ’ª'
  if (msg.includes('ç¬¨') || msg.includes('çˆ›')) return 'å‘œå‘œæˆ‘æœƒåŠ æ²¹çš„ğŸ¥º'
  if (msg.includes('è¬è¬')) return 'ä¸å®¢æ°£ï¼ğŸ‘'
  if (msg.includes('å†è¦‹')) return 'å†è¦‹ï¼æœ‰éœ€è¦å†å«æˆ‘ğŸ‘‹'

  return 'æ”¶åˆ°ï¼è¦æˆ‘å¹«æ‚¨æŸ¥è¨‚å–®é‚„æ˜¯åº«å­˜ï¼ŸğŸ’ª'
}
