"""
å¤š AI æä¾›è€…è™•ç†å™¨
æ”¯æ´ï¼šKimi (Moonshot) + GLM + Ollama
è‡ªå‹•æ•…éšœè½‰ç§»
"""
import os
import requests

# ==================== Kimi (Moonshot) é…ç½® ====================
KIMI_API_URL = os.getenv(
    "KIMI_API_URL",
    "https://api.moonshot.cn/v1/chat/completions"
)
KIMI_API_KEY = os.getenv("KIMI_API_KEY", "sk-kimi-xiWAXckoC7h2MqbHFKdWEKjSNcOpEEzgytYTCUa9DgJLCJugYNbBeKMr72hss1eM")
KIMI_MODEL = os.getenv("KIMI_MODEL", "kimi-k2-thinking-turbo")

# ==================== GLM é…ç½® ====================
GLM_API_URL = os.getenv(
    "GLM_API_URL",
    "https://open.bigmodel.cn/api/paas/v4/chat/completions"
)
GLM_MODEL = os.getenv("GLM_MODEL", "glm-4.7")
GLM_TIMEOUT = int(os.getenv("GLM_TIMEOUT", "10"))

# GLM API Keys
def get_glm_keys() -> list[str]:
    """ç²å–æ‰€æœ‰ GLM API Keys"""
    keys = os.getenv("GLM_API_KEYS") or os.getenv("GLM_KEYS", "")
    key_list = [k.strip() for k in keys.split(",") if k.strip()]
    return key_list

# ==================== Ollama é…ç½® ====================
OLLAMA_API_URL = os.getenv(
    "OLLAMA_API_URL",
    "http://localhost:11434/api/chat"  # æœ¬åœ° Ollama
)
# ä½¿ç”¨ qwen2.5:14b - å·²æ¸¬è©¦å¯å›æ‡‰ä¸­æ–‡
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "qwen2.5:14b")


# ==================== AI æä¾›è€… ====================

def ask_kimi(user_text: str, system_prompt: str, history: list[dict] | None = None) -> str:
    """ä½¿ç”¨ Kimi (Moonshot) API"""
    history = history or []

    messages = [{"role": "system", "content": system_prompt}]
    messages.extend(history)
    messages.append({"role": "user", "content": user_text})

    headers = {
        "Authorization": f"Bearer {KIMI_API_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": KIMI_MODEL,
        "messages": messages,
        "temperature": 0.7,
    }

    response = requests.post(
        KIMI_API_URL,
        headers=headers,
        json=payload,
        timeout=15
    )
    response.raise_for_status()

    data = response.json()
    return data["choices"][0]["message"]["content"]


def ask_ollama(user_text: str, system_prompt: str, history: list[dict] | None = None) -> str:
    """ä½¿ç”¨ Ollama æœ¬åœ°æ¨¡å‹"""
    history = history or []

    messages = [{"role": "system", "content": system_prompt}]
    messages.extend(history)
    messages.append({"role": "user", "content": user_text})

    payload = {
        "model": OLLAMA_MODEL,
        "messages": messages,
        "stream": False,
        "options": {
            "num_predict": 80,  # é™åˆ¶ç”Ÿæˆå­—æ•¸ï¼ŒåŠ å¿«å›æ‡‰ï¼ˆç´„ 60-80 ä¸­æ–‡å­—ï¼‰
        }
    }

    response = requests.post(
        OLLAMA_API_URL,
        json=payload,
        timeout=30
    )

    if response.status_code == 404:
        raise RuntimeError("Ollama æœªé‹è¡Œï¼Œè«‹å…ˆå•Ÿå‹• Ollama æœå‹™")

    response.raise_for_status()
    data = response.json()
    return data["message"]["content"]


# ==================== ä¸»è¦æ¥å£ ====================

def ask_glm_internal(user_text: str, system_prompt: str, history: list[dict] | None = None) -> str:
    """ä½¿ç”¨ GLM APIï¼ˆå¿«é€Ÿå¤±æ•—ï¼Œåªè©¦å‰ 2 å€‹ keyï¼‰- å…§éƒ¨å‡½æ•¸"""
    history = history or []

    messages = [{"role": "system", "content": system_prompt}]
    messages.extend(history)
    messages.append({"role": "user", "content": user_text})

    keys = get_glm_keys()

    # åªè©¦å‰ 2 å€‹ keyï¼ŒåŠ å¿«å›æ‡‰é€Ÿåº¦
    max_attempts = min(2, len(keys))

    for idx in range(max_attempts):
        key = keys[idx]
        try:
            headers = {
                "Authorization": f"Bearer {key}",
                "Content-Type": "application/json"
            }

            payload = {
                "model": GLM_MODEL,
                "messages": messages,
                "temperature": 0.7,
            }

            # é™ä½è¶…æ™‚æ™‚é–“åˆ° 2 ç§’ï¼Œå¿«é€Ÿå¤±æ•—
            response = requests.post(
                GLM_API_URL,
                headers=headers,
                json=payload,
                timeout=2
            )
            response.raise_for_status()

            data = response.json()
            return data["choices"][0]["message"]["content"]

        except (requests.exceptions.Timeout, requests.exceptions.HTTPError) as e:
            # 401/429/Timeout â†’ å¿«é€Ÿè©¦ä¸‹ä¸€å€‹ key
            if isinstance(e, requests.exceptions.HTTPError):
                if e.response.status_code in [401, 429]:
                    if idx < max_attempts - 1:
                        continue
            # Timeout ç¹¼çºŒè©¦ä¸‹ä¸€å€‹
            if idx < max_attempts - 1:
                continue
            raise RuntimeError(f"GLM API éŒ¯èª¤: {e}")

        except Exception as e:
            if idx < max_attempts - 1:
                continue
            raise RuntimeError(f"GLM è«‹æ±‚å¤±æ•—: {e}")

    raise RuntimeError("GLM API ä¸å¯ç”¨")


def ask_glm(
    user_text: str,
    system_prompt: str,
    history: list[dict] | None = None,
) -> str:
    """
    AI è«‹æ±‚ï¼ˆåªä½¿ç”¨ Ollamaï¼‰

    å¿«é€Ÿã€ç©©å®šã€åªå›æ‡‰ä¸­æ–‡
    """
    history = history or []

    # å¼·èª¿åªèƒ½å›æ‡‰ä¸­æ–‡
    chinese_only_prompt = system_prompt + "\n\né‡è¦ï¼šè«‹å‹™å¿…ä½¿ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰ï¼Œä¸è¦ä½¿ç”¨ä»»ä½•è‹±æ–‡æˆ–å…¶ä»–èªè¨€ã€‚"

    # åªä½¿ç”¨ Ollamaï¼ˆæœ¬åœ° GPUï¼Œæœ€å¿«æœ€ç©©å®šï¼‰
    try:
        print("[AI] ä½¿ç”¨ Ollama æœ¬åœ°æ¨¡å‹...")
        return ask_ollama(user_text, chinese_only_prompt, history)
    except Exception as e:
        print(f"[éŒ¯èª¤] Ollama é€£ç·šå¤±æ•—: {e}")
        raise RuntimeError(f"AI æœå‹™ä¸å¯ç”¨: {e}")


# å…¨å±€å®¢æˆ¶ç«¯å¯¦ä¾‹ï¼ˆä¿æŒå‘ä¸‹å…¼å®¹ï¼‰
_client = None


def get_client():
    """ç²å–å…¨å±€å®¢æˆ¶ï¼ˆå‘ä¸‹å…¼å®¹ï¼‰"""
    global _client
    if _client is None:
        _client = True
    return _client


if __name__ == "__main__":
    print("ğŸ§ª æ¸¬è©¦å¤š AI æœå‹™...")
    try:
        from app.prompt_loader import PROMPTS

        result = ask_glm("ä½ å¥½", PROMPTS["core"] + "\n\n" + PROMPTS["dihuang"])
        print(f"âœ… é€£æ¥æˆåŠŸï¼\nå›è¦†ï¼š{result}")
    except Exception as e:
        print(f"âŒ é€£æ¥å¤±æ•—ï¼š{e}")
