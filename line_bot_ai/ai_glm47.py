"""
GLM-4.7 MAX AI å®¢æˆ·ç«¯
ä½¿ç”¨ ZhipuAI SDK æˆ–ç›´æ¥ API è°ƒç”¨
"""
import os
import time
import requests
from typing import Iterator, Optional
from config import config

# å¯¼å…¥ ZhipuAI SDKï¼ˆå¦‚æœå¯ç”¨ï¼‰
try:
    from zhipuai import ZhipuAI
    ZHIPU_SDK_AVAILABLE = True
except ImportError:
    ZHIPU_SDK_AVAILABLE = False
    print("[WARNING] ZhipuAI SDK not installed, using direct API calls")


class GLMClient:
    """GLM-4.7 å®¢æˆ·ç«¯ç±»"""

    def __init__(self):
        self.api_keys = config.GLM_API_KEYS
        self.current_key_index = 0
        self.model = config.GLM_MODEL
        self.fallback_model = config.GLM_FALLBACK_MODEL
        self.timeout = config.GLM_TIMEOUT
        self.max_retries = config.GLM_MAX_RETRIES
        self.api_base = config.GLM_API_BASE

        # ä½¿ç”¨ SDK å®¢æˆ·ç«¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.sdk_client: Optional["ZhipuAI"] = None
        if ZHIPU_SDK_AVAILABLE and any(self.api_keys):
            self.sdk_client = ZhipuAI(api_key=self._get_current_key())

    def _get_current_key(self) -> str:
        """è·å–å½“å‰ä½¿ç”¨çš„ API Key"""
        if not self.api_keys:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„ GLM API Key")
        return self.api_keys[self.current_key_index]

    def _rotate_key(self):
        """è½®æ¢åˆ°ä¸‹ä¸€ä¸ª API Key"""
        if len(self.api_keys) > 1:
            self.current_key_index = (self.current_key_index + 1) % len(self.api_keys)
            if self.sdk_client:
                self.sdk_client = ZhipuAI(api_key=self._get_current_key())

    def _extract_api_key(self, api_key: str) -> str:
        """ä» API Key ä¸­æå– ID å’Œ Secretï¼ˆæ ¼å¼ï¼šid.secretï¼‰"""
        if "." in api_key:
            return api_key
        # å…¼å®¹æ—§æ ¼å¼ï¼Œå‡è®¾æ•´ä¸ªå­—ç¬¦ä¸²æ˜¯å¯†é’¥
        return api_key

    def chat(
        self,
        message: str,
        history: list[dict] | None = None,
        system_prompt: str | None = None,
        stream: bool = False,
    ) -> str | Iterator[str]:
        """
        å‘é€èŠå¤©è¯·æ±‚

        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            history: å†å²å¯¹è¯è®°å½•
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            stream: æ˜¯å¦ä½¿ç”¨æµå¼å“åº”

        Returns:
            AI å›å¤æ–‡æœ¬ï¼ˆæˆ–æµå¼è¿­ä»£å™¨ï¼‰
        """
        history = history or []

        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.extend(history)
        messages.append({"role": "user", "content": message})

        # å°è¯•è¯·æ±‚ï¼ˆæ”¯æŒé‡è¯•å’Œå¯†é’¥è½®æ¢ï¼‰
        for attempt in range(self.max_retries):
            try:
                if self.sdk_client and ZHIPU_SDK_AVAILABLE:
                    return self._chat_sdk(messages, stream)
                else:
                    return self._chat_direct(messages, stream)
            except Exception as e:
                error_msg = str(e).lower()
                # è®¤è¯é”™è¯¯ï¼Œå°è¯•åˆ‡æ¢å¯†é’¥
                if "auth" in error_msg or "401" in error_msg:
                    self._rotate_key()
                    continue
                # å…¶ä»–é”™è¯¯ï¼Œå°è¯•é‡è¯•
                if attempt < self.max_retries - 1:
                    wait_time = min(2 ** attempt, 10)
                    time.sleep(wait_time)
                    continue
                raise

        raise Exception("AI æœåŠ¡è¯·æ±‚å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")

    def _chat_sdk(
        self, messages: list[dict], stream: bool
    ) -> str | Iterator[str]:
        """ä½¿ç”¨ ZhipuAI SDK å‘é€è¯·æ±‚"""
        response = self.sdk_client.chat.completions.create(
            model=self.model,
            messages=messages,
            stream=stream,
        )

        if stream:
            return self._process_sdk_stream(response)
        return response.choices[0].message.content

    def _process_sdk_stream(self, response) -> Iterator[str]:
        """å¤„ç† SDK æµå¼å“åº”"""
        for chunk in response:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

    def _chat_direct(
        self, messages: list[dict], stream: bool
    ) -> str | Iterator[str]:
        """ä½¿ç”¨ç›´æ¥ HTTP è¯·æ±‚"""
        api_key = self._get_current_key()

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
        }

        payload = {
            "model": self.model,
            "messages": messages,
            "stream": stream,
        }

        response = requests.post(
            f"{self.api_base}chat/completions",
            headers=headers,
            json=payload,
            timeout=self.timeout,
            stream=stream,
        )
        response.raise_for_status()

        if stream:
            return self._process_direct_stream(response)
        return response.json()["choices"][0]["message"]["content"]

    def _process_direct_stream(self, response) -> Iterator[str]:
        """å¤„ç†ç›´æ¥ HTTP æµå¼å“åº”"""
        for line in response.iter_lines():
            if line:
                line = line.decode("utf-8")
                if line.startswith("data: "):
                    data = line[6:]
                    if data == "[DONE]":
                        break
                    try:
                        import json
                        chunk = json.loads(data)
                        if "choices" in chunk and chunk["choices"]:
                            delta = chunk["choices"][0].get("delta", {})
                            content = delta.get("content", "")
                            if content:
                                yield content
                    except json.JSONDecodeError:
                        continue


# ä¾¿æ·å‡½æ•°
def ask_glm(
    prompt: str,
    history: list[dict] | None = None,
    system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€å†·é™ã€åŠ¡å®å¯¼å‘çš„ä¼ä¸šçº§åŠ©ç†ã€‚",
) -> str:
    """
    å‘ GLM-4.7 å‘é€é—®é¢˜

    Args:
        prompt: ç”¨æˆ·é—®é¢˜
        history: å¯¹è¯å†å²
        system_prompt: ç³»ç»Ÿæç¤ºè¯

    Returns:
        AI å›å¤æ–‡æœ¬
    """
    client = GLMClient()
    result = client.chat(prompt, history=history, system_prompt=system_prompt)

    # å¤„ç†æµå¼å“åº”ï¼ˆå¦‚æœæ˜¯ï¼‰
    if isinstance(result, str):
        return result

    # å¦‚æœæ˜¯è¿­ä»£å™¨ï¼Œæ”¶é›†æ‰€æœ‰å†…å®¹
    return "".join(list(result))


# å…¨å±€å®¢æˆ·ç«¯å®ä¾‹
_glm_client: Optional[GLMClient] = None


def get_glm_client() -> GLMClient:
    """è·å–å…¨å±€ GLM å®¢æˆ·ç«¯å®ä¾‹"""
    global _glm_client
    if _glm_client is None:
        _glm_client = GLMClient()
    return _glm_client


if __name__ == "__main__":
    # æµ‹è¯•
    print("ğŸ§ª æµ‹è¯• GLM-4.7 è¿æ¥...")
    try:
        response = ask_glm("ä½ å¥½ï¼Œè¯·è‡ªæˆ‘ä»‹ç»ã€‚")
        print(f"âœ… è¿æ¥æˆåŠŸï¼\nå›å¤ï¼š{response}")
    except Exception as e:
        print(f"âŒ è¿æ¥å¤±è´¥ï¼š{e}")
