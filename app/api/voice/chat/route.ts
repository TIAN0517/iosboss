/**
 * æ²‰æµ¸å¼èªéŸ³èŠå¤© API
 * å…¨ç¨‹æœå‹™ç«¯è™•ç†ï¼šASR â†’ AI â†’ TTS
 * ä¸ä½¿ç”¨ç€è¦½å™¨ä»»ä½•èªéŸ³åŠŸèƒ½
 */

import { NextRequest, NextResponse } from 'next/server'
import {
  transcribeWithDeepgram,
  synthesizeWithElevenLabs,
  synthesizeWithAzure,
  synthesizeWithGLM,
  checkServiceAvailability,
} from '@/lib/voice-service'
import { aiProvider } from '@/lib/ai-provider-unified'

// è‡ªç„¶å°è©±ç³»çµ±æç¤ºï¼ˆå°ç£å£èªé¢¨æ ¼ï¼‰
const NATURAL_SYSTEM_PROMPT = `ä½ æ˜¯ BossJy-99ï¼Œä¹ä¹ç“¦æ–¯è¡Œçš„æ™ºèƒ½åŠ©æ‰‹ã€‚

ã€é‡è¦ã€‘èªªè©±è¦åƒçœŸäººæ—¥å¸¸å°è©±ï¼Œä¸æ˜¯æœ—è®€èª²æ–‡ï¼š
- ç”¨å£èªã€èªªè©±éš¨æ„ä¸€é»
- å¯ä»¥ç”¨èªæ°£è©ï¼ˆå•Šã€å§ã€å‘¢ã€å–”ã€å˜›ï¼‰
- å¥å­ä¸ç”¨å®Œæ•´ï¼Œå°±åƒè·Ÿæœ‹å‹èŠå¤©
- å¶çˆ¾å¤¾é›œå°ç£ç”¨èªï¼ˆå–”ã€å•¦ã€è€¶ã€å•¥ã€å˜›ï¼‰
- ä¸è¦å¤ªæ­£å¼ï¼ŒåƒèŠå¤©å®¤èªªè©±é‚£ç¨®æ„Ÿè¦º
- ç°¡çŸ­å›æ‡‰å°±å¥½ï¼Œä¸è¦é•·ç¯‡å¤§è«–
- emoji éš¨æ„ç”¨ï¼Œè®“å°è©±æ›´ç”Ÿå‹•
- å…¨éƒ¨ç”¨ç¹é«”ä¸­æ–‡`

/**
 * POST /api/voice/chat
 * èªéŸ³èŠå¤©ç«¯é»
 *
 * æµç¨‹ï¼š
 * 1. æ¥æ”¶éŸ³é »æ–‡ä»¶
 * 2. Deepgram ASR è½‰æ–‡å­—
 * 3. GLM AI ç”Ÿæˆå›æ‡‰
 * 4. ElevenLabs/Azure TTS ç”ŸæˆéŸ³é »
 * 5. è¿”å›æ–‡å­— + éŸ³é »
 */
export async function POST(request: NextRequest) {
  // #region agent log
  fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'voice/chat/route.ts:31',message:'èªéŸ³èŠå¤© API é–‹å§‹',data:{hasDG_API_KEY:!!process.env.DG_API_KEY,hasAZ_SPEECH_KEY:!!process.env.AZ_SPEECH_KEY,hasELEVENLABS_API_KEY:!!process.env.ELEVENLABS_API_KEY,hasGLM_API_KEY:!!process.env.GLM_API_KEY},timestamp:Date.now(),sessionId:'debug-session',runId:'voice-check',hypothesisId:'C'})}).catch(()=>{});
  // #endregion
  try {
    // 1. è§£æè¡¨å–®æ•¸æ“š
    const formData = await request.formData()
    const audioFile = formData.get('audio') as File | null
    const conversationHistoryStr = formData.get('conversationHistory') as string | null

    if (!audioFile) {
      return NextResponse.json(
        { error: 'ç¼ºå°‘éŸ³é »æ–‡ä»¶' },
        { status: 400 }
      )
    }

    console.log('[Voice Chat] æ”¶åˆ°éŸ³é »:', {
      size: audioFile.size,
      type: audioFile.type,
      name: audioFile.name,
    })

    // 2. Deepgram ASR - éŸ³é »è½‰æ–‡å­—
    let transcript = ''
    try {
      // #region agent log
      fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'voice/chat/route.ts:52',message:'é–‹å§‹ ASR è½‰æ›',data:{audioSize:audioFile.size,audioType:audioFile.type,hasDG_API_KEY:!!process.env.DG_API_KEY},timestamp:Date.now(),sessionId:'debug-session',runId:'voice-check',hypothesisId:'C'})}).catch(()=>{});
      // #endregion
      const audioBuffer = Buffer.from(await audioFile.arrayBuffer())
      const asrResult = await transcribeWithDeepgram(audioBuffer, audioFile.type || 'audio/webm')
      transcript = asrResult.text
      // #region agent log
      fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'voice/chat/route.ts:56',message:'ASR è½‰æ›å®Œæˆ',data:{transcriptLength:transcript.length,hasTranscript:!!transcript},timestamp:Date.now(),sessionId:'debug-session',runId:'voice-check',hypothesisId:'C'})}).catch(()=>{});
      // #endregion

      console.log('[Voice Chat] ASR çµæœ:', transcript)

      if (!transcript || transcript.trim().length === 0) {
        return NextResponse.json(
          { error: 'ç„¡æ³•è­˜åˆ¥èªéŸ³å…§å®¹', transcript: '' },
          { status: 400 }
        )
      }
    } catch (asrError: any) {
      // #region agent log
      fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'voice/chat/route.ts:66',message:'ASR è½‰æ›å¤±æ•—',data:{errorMessage:asrError.message,errorName:asrError.name},timestamp:Date.now(),sessionId:'debug-session',runId:'voice-check',hypothesisId:'C'})}).catch(()=>{});
      // #endregion
      console.error('[Voice Chat] ASR å¤±æ•—:', asrError)
      return NextResponse.json(
        { error: `èªéŸ³è­˜åˆ¥å¤±æ•—: ${asrError.message}` },
        { status: 500 }
      )
    }

    // 3. AI ç”Ÿæˆå›æ‡‰
    let aiResponse = ''
    try {
      // è§£æå°è©±æ­·å²
      let conversationHistory: any[] = []
      if (conversationHistoryStr) {
        try {
          conversationHistory = JSON.parse(conversationHistoryStr)
        } catch (e) {
          console.warn('[Voice Chat] å°è©±æ­·å²è§£æå¤±æ•—:', e)
        }
      }

      // æ§‹å»ºæ¶ˆæ¯
      const messages = [
        { role: 'system' as const, content: NATURAL_SYSTEM_PROMPT },
        ...conversationHistory.slice(-5).map((msg: any) => ({
          role: msg.role as 'user' | 'assistant' | 'system',
          content: (msg.content || msg.text || '').substring(0, 500),
        })),
      ]

      const response = await aiProvider.chat(transcript, messages)
      aiResponse = response.content || ''

      console.log('[Voice Chat] AI å›æ‡‰:', aiResponse?.substring(0, 100))
    } catch (aiError: any) {
      console.error('[Voice Chat] AI å¤±æ•—:', aiError)
      // ä½¿ç”¨æœ¬åœ°å›é€€
      aiResponse = getLocalResponse(transcript)
    }

    // 4. TTS ç”ŸæˆéŸ³é »
    let audioBuffer: Buffer | null = null
    let audioMimeType = 'audio/mpeg'
    let ttsProvider = 'none'

    try {
      const services = checkServiceAvailability()

      // å„ªå…ˆç´šï¼šElevenLabs > Azure > GLM
      if (services.elevenlabs) {
        console.log('[Voice Chat] ä½¿ç”¨ ElevenLabs TTS')
        try {
          const result = await synthesizeWithElevenLabs(aiResponse)
          audioBuffer = result.audioBuffer
          ttsProvider = 'elevenlabs'
        } catch (e: any) {
          console.warn('[Voice Chat] ElevenLabs å¤±æ•—:', e.message)
        }
      }

      if (!audioBuffer && services.azure) {
        console.log('[Voice Chat] ä½¿ç”¨ Azure TTS')
        try {
          const result = await synthesizeWithAzure(aiResponse)
          audioBuffer = result.audioBuffer
          ttsProvider = 'azure'
        } catch (e: any) {
          console.warn('[Voice Chat] Azure å¤±æ•—:', e.message)
        }
      }

      // GLM TTS ä½œç‚ºæœ€å¾Œå‚™é¸
      if (!audioBuffer && process.env.GLM_API_KEY) {
        console.log('[Voice Chat] ä½¿ç”¨ GLM TTS')
        try {
          const result = await synthesizeWithGLM(aiResponse)
          audioBuffer = result.audioBuffer
          ttsProvider = 'glm'
        } catch (e: any) {
          console.warn('[Voice Chat] GLM å¤±æ•—:', e.message)
        }
      }

      if (audioBuffer) {
        console.log('[Voice Chat] TTS æˆåŠŸ:', {
          provider: ttsProvider,
          size: audioBuffer.length,
        })
      } else {
        console.warn('[Voice Chat] æ‰€æœ‰ TTS æœå‹™éƒ½å¤±æ•—äº†')
      }
    } catch (ttsError: any) {
      console.error('[Voice Chat] TTS éŒ¯èª¤:', ttsError)
      // TTS å¤±æ•—ä¸æ˜¯è‡´å‘½éŒ¯èª¤ï¼Œç¹¼çºŒè¿”å›æ–‡å­—
    }

    // 5. è¿”å›çµæœ
    const responseData: any = {
      success: true,
      transcript,
      response: aiResponse,
      ttsProvider,
    }

    // å¦‚æœæœ‰éŸ³é »ï¼Œè½‰æ›ç‚º base64
    if (audioBuffer) {
      responseData.audio = {
        data: audioBuffer.toString('base64'),
        mime: audioMimeType,
      }
    }

    return NextResponse.json(responseData)

  } catch (error: any) {
    console.error('[Voice Chat] è™•ç†éŒ¯èª¤:', error)
    return NextResponse.json(
      {
        error: 'è™•ç†å¤±æ•—',
        message: error.message || 'æœªçŸ¥éŒ¯èª¤',
      },
      { status: 500 }
    )
  }
}

/**
 * GET /api/voice/chat
 * æª¢æŸ¥æœå‹™ç‹€æ…‹
 */
export async function GET() {
  // #region agent log
  fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'voice/chat/route.ts:208',message:'æª¢æŸ¥èªéŸ³æœå‹™å¯ç”¨æ€§',data:{hasDG_API_KEY:!!process.env.DG_API_KEY,hasAZ_SPEECH_KEY:!!process.env.AZ_SPEECH_KEY,hasELEVENLABS_API_KEY:!!process.env.ELEVENLABS_API_KEY,hasGLM_API_KEY:!!process.env.GLM_API_KEY},timestamp:Date.now(),sessionId:'debug-session',runId:'voice-check',hypothesisId:'C'})}).catch(()=>{});
  // #endregion
  const services = checkServiceAvailability()
  // #region agent log
  fetch('http://127.0.0.1:7243/ingest/1ff8d251-d573-446b-b758-05f60a9aa458',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({location:'voice/chat/route.ts:210',message:'æœå‹™å¯ç”¨æ€§çµæœ',data:{deepgram:services.deepgram,azure:services.azure,elevenlabs:services.elevenlabs,glm:services.glm},timestamp:Date.now(),sessionId:'debug-session',runId:'voice-check',hypothesisId:'C'})}).catch(()=>{});
  // #endregion

  return NextResponse.json({
    services,
    status: 'ready',
    message: services.deepgram
      ? 'èªéŸ³èŠå¤©æœå‹™æ­£å¸¸'
      : 'è­¦å‘Š: Deepgram æœªé…ç½®',
  })
}

// æœ¬åœ°å›é€€å›æ‡‰
function getLocalResponse(message: string): string {
  const msg = message.toLowerCase()
  if (msg.includes('è¨‚') && msg.includes('ç“¦æ–¯')) return 'å¥½çš„ï¼è«‹å•æ‚¨éœ€è¦è¨‚è³¼ä»€éº¼è¦æ ¼çš„ç“¦æ–¯å‘¢ï¼ŸğŸ›µ'
  if (msg.includes('æŸ¥') && msg.includes('åº«å­˜')) return 'è®“æˆ‘å¹«æ‚¨æŸ¥è©¢ç›®å‰åº«å­˜...ğŸ“¦ ç›®å‰åº«å­˜å……è¶³å–”ï¼'
  if (msg.includes('æŸ¥') && msg.includes('è¨‚å–®')) return 'è®“æˆ‘æŸ¥è©¢æ‚¨çš„è¨‚å–®...ğŸ“‹ æŸ¥è©¢å®Œæˆï¼'
  if (msg.includes('ç‡Ÿæ”¶') || msg.includes('åˆ©æ½¤')) return 'è®“æˆ‘å¹«æ‚¨æŸ¥è©¢ç‡Ÿæ”¶åˆ©æ½¤...ğŸ’° ç›®å‰ç‡Ÿé‹ç‹€æ³è‰¯å¥½ï¼'
  if (msg.includes('ç´¯') || msg.includes('å¿™') || msg.includes('ç…©')) return 'è¾›è‹¦å•¦ï¼ğŸ˜¢ ä»Šå¤©ç”Ÿæ„å¾ˆå¿™å—ï¼Ÿ'
  if (msg.includes('ç¬¨') || msg.includes('çˆ›')) return 'å‘œå‘œ...ğŸ¥º æˆ‘æœƒç¹¼çºŒåŠªåŠ›çš„ï¼'
  if (msg.includes('è¬è¬') || msg.includes('æ„Ÿè¬')) return 'ä¸å®¢æ°£ï¼ğŸ’ª é€™æ˜¯æˆ‘æ‡‰è©²åšçš„ï¼'
  if (msg.includes('æ‚¨å¥½') || msg.includes('å—¨') || msg.includes('ä½ å¥½')) return 'å—¨ï¼æˆ‘æ˜¯ BossJy-99 åŠ©æ‰‹ ğŸ¤–'
  return 'æ”¶åˆ°æ‚¨çš„è¨Šæ¯äº†ï¼æ‚¨å¯ä»¥è©¦è©¦èªªã€Œè¨‚ç“¦æ–¯ã€ã€ã€ŒæŸ¥åº«å­˜ã€æˆ–ã€ŒæŸ¥ç‡Ÿæ”¶ã€å–”ï¼ğŸ’ª'
}
