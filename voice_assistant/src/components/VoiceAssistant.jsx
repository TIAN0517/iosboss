// React ä¸»æ‡‰ç”¨çµ„ä»¶
import React, { useState, useEffect, useRef } from 'react';
import { 
    SpeechRecognitionService, 
    AudioRecorderService, 
    VoiceProcessingUtils 
} from '../services/speechRecognition.js';
import { AIService, ConversationManager } from '../services/aiChat.js';
import { TextToSpeechService } from '../services/textToSpeech.js';
import '../styles/VoiceAssistant.css';

const VoiceAssistant = () => {
    // ç‹€æ…‹ç®¡ç†
    const [isListening, setIsListening] = useState(false);
    const [isSpeaking, setIsSpeaking] = useState(false);
    const [currentMessage, setCurrentMessage] = useState('');
    const [conversationHistory, setConversationHistory] = useState([]);
    const [isProcessing, setIsProcessing] = useState(false);
    const [error, setError] = useState(null);
    const [recordingLevel, setRecordingLevel] = useState(0);
    const [selectedVoice, setSelectedVoice] = useState(null);
    const [conversationStats, setConversationStats] = useState({
        totalMessages: 0,
        sessionDuration: 0
    });

    // æœå‹™å¯¦ä¾‹
    const speechRecognitionRef = useRef(null);
    const audioRecorderRef = useRef(null);
    const ttsServiceRef = useRef(null);
    const conversationManagerRef = useRef(null);
    const sessionStartTimeRef = useRef(Date.now());

    // åˆå§‹åŒ–æœå‹™
    useEffect(() => {
        initializeServices();
        return () => {
            cleanup();
        };
    }, []);

    const initializeServices = () => {
        try {
            // åˆå§‹åŒ–èªéŸ³è­˜åˆ¥
            speechRecognitionRef.current = new SpeechRecognitionService();
            speechRecognitionRef.current.onResult = handleSpeechResult;
            speechRecognitionRef.current.onError = handleSpeechError;

            // åˆå§‹åŒ–éŸ³é »éŒ„è£½
            audioRecorderRef.current = new AudioRecorderService();
            audioRecorderRef.current.onError = handleAudioError;

            // åˆå§‹åŒ–èªéŸ³åˆæˆ
            ttsServiceRef.current = new TextToSpeechService();
            ttsServiceRef.current.onStart = () => setIsSpeaking(true);
            ttsServiceRef.current.onEnd = () => setIsSpeaking(false);

            // åˆå§‹åŒ–å°è©±ç®¡ç†
            conversationManagerRef.current = new ConversationManager();

            // é–‹å§‹æœƒè©±æ™‚é–“
            sessionStartTimeRef.current = Date.now();

            console.log('èªéŸ³åŠ©æ‰‹æœå‹™åˆå§‹åŒ–å®Œæˆ');
        } catch (error) {
            console.error('æœå‹™åˆå§‹åŒ–å¤±æ•—:', error);
            setError('æœå‹™åˆå§‹åŒ–å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç€è¦½å™¨æ¬Šé™');
        }
    };

    const cleanup = () => {
        if (speechRecognitionRef.current) {
            speechRecognitionRef.current.stop();
        }
        if (audioRecorderRef.current) {
            audioRecorderRef.current.cleanup();
        }
        if (ttsServiceRef.current) {
            ttsServiceRef.current.stop();
        }
    };

    // è™•ç†èªéŸ³è­˜åˆ¥çµæœ
    const handleSpeechResult = (result) => {
        const { final, interim, confidence } = result;
        
        // æ™ºèƒ½æ‰“æ–·é‚è¼¯ï¼šå¦‚æœç”¨æˆ¶é–‹å§‹èªªè©±ä¸”æ­£åœ¨æ’­æ”¾èªéŸ³ï¼Œå‰‡åœæ­¢æ’­æ”¾
        if ((final || interim) && isSpeaking) {
            console.log('æª¢æ¸¬åˆ°ç”¨æˆ¶èªªè©±ï¼Œæ‰“æ–· AI èªéŸ³');
            stopSpeaking();
        }
        
        if (final) {
            setCurrentMessage(final);
            processUserMessage(final);
        } else if (interim) {
            setCurrentMessage(interim);
        }
    };

    // è™•ç†èªéŸ³éŒ¯èª¤
    const handleSpeechError = (error) => {
        console.error('èªéŸ³è­˜åˆ¥éŒ¯èª¤:', error);
        setError(`èªéŸ³è­˜åˆ¥éŒ¯èª¤: ${error}`);
        setIsListening(false);
    };

    // è™•ç†éŸ³é »éŒ¯èª¤
    const handleAudioError = (error) => {
        console.error('éŸ³é »éŒ„è£½éŒ¯èª¤:', error);
        setError(`éŸ³é »éŒ„è£½éŒ¯èª¤: ${error}`);
    };

    // è™•ç†ç”¨æˆ¶æ¶ˆæ¯
    const processUserMessage = async (message) => {
        if (!message.trim()) return;

        setIsProcessing(true);
        
        try {
            // æ·»åŠ ç”¨æˆ¶æ¶ˆæ¯åˆ°å°è©±æ­·å²
            const userMessage = {
                type: 'user',
                content: message,
                timestamp: new Date().toISOString()
            };
            
            setConversationHistory(prev => [...prev, userMessage]);
            
            // è™•ç†AIå›æ‡‰
            const response = await conversationManagerRef.current.processUserMessage(
                'user-001', // è‡¨æ™‚ç”¨æˆ¶ID
                message
            );
            
            // æ·»åŠ AIå›æ‡‰åˆ°å°è©±æ­·å²
            const aiMessage = {
                type: 'ai',
                content: response.aiResponse,
                emotion: response.emotion,
                timestamp: new Date().toISOString(),
                confidence: response.confidence
            };
            
            setConversationHistory(prev => [...prev, aiMessage]);
            
            // èªéŸ³åˆæˆAIå›æ‡‰
            if (ttsServiceRef.current && response.aiResponse) {
                console.log('æº–å‚™èªéŸ³åˆæˆå›æ‡‰:', response.aiResponse);
                
                // è¨­ç½®ç‹€æ…‹ç‚ºè™•ç†ä¸­ï¼ˆç”ŸæˆèªéŸ³ï¼‰
                setIsProcessing(true);
                
                try {
                    await ttsServiceRef.current.speak(response.aiResponse, {
                        language: 'zh-TW',
                        rate: 1.0, // API TTS é»˜èªèªé€Ÿ
                        pitch: 1.0
                    });
                    
                    setIsProcessing(false);
                    
                    // èªéŸ³æ’­æ”¾å®Œç•¢å¾Œï¼Œè‡ªå‹•é‡æ–°é–‹å•Ÿç›£è½ï¼ˆé€£çºŒå°è©±ï¼‰
                    // é€™è£¡éœ€è¦ä¸€å€‹çŸ­æš«å»¶é²ï¼Œé¿å…æ¡é›†åˆ°ç³»çµ±å‰›æ’­å®Œçš„å°¾éŸ³
                    setTimeout(() => {
                        if (!isListening && !isProcessing) {
                            console.log('é€£çºŒå°è©±ï¼šé‡æ–°é–‹å•Ÿç›£è½');
                            toggleListening(); 
                        }
                    }, 500);
                    
                } catch (ttsError) {
                    console.error('èªéŸ³åˆæˆå¤±æ•—:', ttsError);
                    setIsProcessing(false);
                    setError('èªéŸ³æ’­æ”¾å¤±æ•—: ' + ttsError.message);
                }
            } else {
                console.warn('TTSæœå‹™æœªå°±ç·’æˆ–ç„¡å›æ‡‰å…§å®¹');
                setIsProcessing(false);
            }
            
            // æ›´æ–°çµ±è¨ˆä¿¡æ¯
            updateConversationStats(response.conversationStats);
            
        } catch (error) {
            console.error('è™•ç†æ¶ˆæ¯å¤±æ•—:', error);
            setError('è™•ç†æ¶ˆæ¯å¤±æ•—ï¼Œè«‹é‡è©¦');
        } finally {
            setIsProcessing(false);
            setCurrentMessage('');
        }
    };

    // æ›´æ–°å°è©±çµ±è¨ˆ
    const updateConversationStats = (stats) => {
        setConversationStats({
            totalMessages: stats.totalMessages,
            sessionDuration: Date.now() - sessionStartTimeRef.current
        });
    };

    // é–‹å§‹/åœæ­¢èªéŸ³è­˜åˆ¥
    const toggleListening = async () => {
        if (isListening) {
            // åœæ­¢èªéŸ³è­˜åˆ¥
            speechRecognitionRef.current?.stop();
            audioRecorderRef.current?.stopRecording();
            setIsListening(false);
            setCurrentMessage('');
        } else {
            // é–‹å§‹èªéŸ³è­˜åˆ¥
            try {
                await audioRecorderRef.current?.init();
                speechRecognitionRef.current?.start();
                audioRecorderRef.current?.startRecording();
                setIsListening(true);
                setError(null);
            } catch (error) {
                console.error('é–‹å§‹èªéŸ³è­˜åˆ¥å¤±æ•—:', error);
                setError('ç„¡æ³•é–‹å§‹èªéŸ³è­˜åˆ¥ï¼Œè«‹æª¢æŸ¥éº¥å…‹é¢¨æ¬Šé™');
            }
        }
    };

    // åœæ­¢èªéŸ³æ’­æ”¾
    const stopSpeaking = () => {
        ttsServiceRef.current?.stop();
        setIsSpeaking(false);
    };

    // æ¸…ç©ºå°è©±æ­·å²
    const clearHistory = () => {
        setConversationHistory([]);
        conversationManagerRef.current?.clearHistory();
        sessionStartTimeRef.current = Date.now();
        setConversationStats({ totalMessages: 0, sessionDuration: 0 });
    };

    // æ ¼å¼åŒ–æ™‚é–“
    const formatDuration = (milliseconds) => {
        const seconds = Math.floor(milliseconds / 1000);
        const minutes = Math.floor(seconds / 60);
        const hours = Math.floor(minutes / 60);
        
        if (hours > 0) {
            return `${hours}:${String(minutes % 60).padStart(2, '0')}:${String(seconds % 60).padStart(2, '0')}`;
        } else {
            return `${minutes}:${String(seconds % 60).padStart(2, '0')}`;
        }
    };

    // æ ¼å¼åŒ–çµ±è¨ˆæ™‚é–“
    const formatStatsTime = (milliseconds) => {
        const seconds = Math.floor(milliseconds / 1000);
        return `${seconds}ç§’`;
    };

    // æ¨¡æ“¬éŸ³é‡è®ŠåŒ–ï¼ˆç”¨æ–¼å‹•ç•«ï¼‰
    useEffect(() => {
        if (viewState === 'listening' || viewState === 'speaking') {
            const simulateVolume = () => {
                // ç”¢ç”Ÿ 0.2 ~ 0.8 ä¹‹é–“çš„éš¨æ©Ÿæ³¢å‹•
                const vol = 0.2 + Math.random() * 0.6;
                setAudioVolume(vol);
                animationFrameRef.current = requestAnimationFrame(simulateVolume);
            };
            simulateVolume();
        } else {
            setAudioVolume(0);
            if (animationFrameRef.current) {
                cancelAnimationFrame(animationFrameRef.current);
            }
        }
        
        return () => {
            if (animationFrameRef.current) {
                cancelAnimationFrame(animationFrameRef.current);
            }
        };
    }, [viewState]);

    return (
        <div className="voice-assistant">
            {/* é ­éƒ¨ */}
            <header className="voice-header">
                <div className="header-content">
                    <div className="assistant-info">
                        <span className="assistant-name">è±†åŒ…èªéŸ³åŠ©æ‰‹</span>
                    </div>
                    <div className="session-stats">
                        <span className="stats-messages">
                            ğŸ’¬ {conversationStats.totalMessages}
                        </span>
                        <span className="stats-duration">
                            â±ï¸ {formatStatsTime(conversationStats.sessionDuration)}
                        </span>
                    </div>
                </div>
            </header>

            {/* ä¸»è¦å€åŸŸ - æ²‰æµ¸å¼ä½ˆå±€ */}
            <main className="voice-main immersive-mode">
                
                {/* æ ¸å¿ƒè¦–è¦ºå€åŸŸ */}
                <div className="visual-core">
                    <VoiceSphere state={viewState} volume={audioVolume} />
                </div>

                {/* å¯¦æ™‚å­—å¹•/å°è©±æ°£æ³¡ */}
                <div className="live-captions">
                    {/* é¡¯ç¤ºæœ€è¿‘ä¸€æ¢ AI æ¶ˆæ¯ */}
                    {conversationHistory.length > 0 && conversationHistory[conversationHistory.length - 1].type === 'ai' && (
                        <div className="caption ai-caption">
                            {conversationHistory[conversationHistory.length - 1].content}
                        </div>
                    )}
                    
                    {/* é¡¯ç¤ºç•¶å‰ç”¨æˆ¶è¼¸å…¥ */}
                    {currentMessage && (
                        <div className="caption user-caption">
                            {currentMessage}
                        </div>
                    )}
                </div>

                {/* éŒ¯èª¤é¡¯ç¤º */}
                {error && (
                    <div className="error-message">
                        <span className="error-icon">âš ï¸</span>
                        <span className="error-text">{error}</span>
                        <button 
                            className="error-close"
                            onClick={() => setError(null)}
                        >
                            âœ•
                        </button>
                    </div>
                )}
            </main>

            {/* æ§åˆ¶é¢æ¿ */}
            <footer className="voice-controls">
                <div className="controls-container">
                    {/* ä¸»è¦æ§åˆ¶æŒ‰éˆ• */}
                    <div className="main-controls">
                        <button 
                            className={`control-btn primary ${isListening ? 'active' : ''}`}
                            onClick={toggleListening}
                            disabled={isProcessing}
                        >
                            <div className={`btn-icon ${isListening ? 'listening' : ''}`}>
                                ğŸ¤
                            </div>
                            <span className="btn-text">
                                {isListening ? 'é»æ“Šåœæ­¢' : 'é»æ“Šå°è©±'}
                            </span>
                        </button>
                    </div>

                    {/* æ¬¡è¦æ§åˆ¶ */}
                    <div className="secondary-controls">
                        <button 
                            className="control-btn small"
                            onClick={clearHistory}
                            disabled={conversationHistory.length === 0}
                        >
                            ğŸ—‘ï¸ é‡ç½®
                        </button>
                    </div>
                </div>
            </footer>
        </div>
    );
};

export default VoiceAssistant;