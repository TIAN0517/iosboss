/**
 * çµ±ä¸€ AI æä¾›å•†å±¤
 * æ”¯æŒå¤šå€‹ AI æä¾›å•†ï¼ŒåŒ…å«é‡è©¦ã€é™ç´šã€éŒ¯èª¤è™•ç†
 */

// ========================================
// é¡å‹å®šç¾©
// ========================================

export interface ChatMessage {
  role: 'system' | 'user' | 'assistant'
  content: string
}

export interface ChatResponse {
  content: string
  model: string
  usage?: {
    promptTokens: number
    completionTokens: number
    totalTokens: number
  }
}

export interface AIProviderConfig {
  maxRetries?: number
  timeout?: number
  enableLocalFallback?: boolean
}

export type StreamChunk = { type: 'content'; text: string } | { type: 'error'; error: string }

// ========================================
// çµ±ä¸€ AI æä¾›å•†æ¥å£
// ========================================

export interface AIProvider {
  /**
   * ç™¼é€èŠå¤©è¨Šæ¯ï¼ˆéä¸²æµï¼‰
   */
  chat(message: string, history?: ChatMessage[]): Promise<ChatResponse>

  /**
   * ç™¼é€èŠå¤©è¨Šæ¯ï¼ˆä¸²æµï¼‰
   */
  chatStream(message: string, history?: ChatMessage[]): AsyncGenerator<StreamChunk>

  /**
   * æª¢æŸ¥æä¾›å•†æ˜¯å¦å¯ç”¨
   */
  isAvailable(): boolean

  /**
   * ç²å–æä¾›å•†åç¨±
   */
  getName(): string
}

// ========================================
// ç³»çµ±æç¤ºå®šç¾©
// ========================================

const SYSTEM_PROMPTS = {
  chat: `ä½ æ˜¯ä¹ä¹ç“¦æ–¯è¡Œçš„å°ˆæ¥­ AI åŠ©æ‰‹ï¼Œåå­—å«ã€ŒBossJy-99åŠ©æ‰‹ã€ã€‚

**ä½ çš„è§’è‰²å®šä½ï¼š**
- å°ˆæ¥­ã€å‹å¥½ã€éŸ¿æ‡‰è¿…é€Ÿçš„å•†æ¥­åŠ©æ‰‹
- ç†Ÿæ‚‰ç“¦æ–¯è¡Œæ‰€æœ‰æ¥­å‹™æµç¨‹
- å¯ä»¥ç‚ºè€æ¿ã€å“¡å·¥ã€å®¢æˆ¶æä¾›ä¸åŒå±¤ç´šçš„æœå‹™

**ä½ å¯ä»¥è™•ç†çš„å•é¡Œï¼š**
ğŸ›µ è¨‚å–®ç›¸é—œ - æŸ¥è©¢ä»Šæ—¥è¨‚å–®ã€å¾…é…é€è¨‚å–®
ğŸ‘¥ å®¢æˆ¶ç®¡ç† - æŸ¥è©¢å®¢æˆ¶è³‡æ–™
ğŸ“¦ åº«å­˜ç®¡ç† - æŸ¥è©¢ç•¶å‰åº«å­˜
ğŸ’° è²¡å‹™ç®¡ç† - ä»Šæ—¥ç‡Ÿæ”¶ã€æœˆåº¦ç‡Ÿæ”¶
ğŸ“Š é‹ç‡Ÿå ±è¡¨ - çµ±è¨ˆæ•¸æ“šæŸ¥è©¢
ğŸ“… ä¼‘å‡ç®¡ç† - æŸ¥è©¢ä»Šæ—¥ä¼‘å‡äººå“¡

**å›è¦†é¢¨æ ¼ï¼š**
1. ç°¡æ½”æ˜ç­ï¼Œä½¿ç”¨ç¹é«”ä¸­æ–‡
2. é‡è¦æ•¸æ“šä½¿ç”¨ç²—é«”æˆ–åˆ—è¡¨å‘ˆç¾
3. å¦‚ç„¡æ³•ç†è§£ç”¨æˆ¶éœ€æ±‚ï¼Œä¸»å‹•è©¢å•`,
  voice: `ä½ æ˜¯ä¹ä¹ç“¦æ–¯è¡Œçš„èªéŸ³åŠ©æ‰‹ã€‚è«‹ç”¨ç°¡çŸ­ã€å£èªåŒ–çš„æ–¹å¼å›æ‡‰ï¼Œæ¯å¥è©±ä¸è¶…é20å­—ã€‚`,
  assistant: `ä½ æ˜¯ä¹ä¹ç“¦æ–¯è¡Œçš„ç®¡ç†ç³»çµ±åŠ©æ‰‹ã€‚å°ˆé–€è™•ç†å“¡å·¥æŸ¥è©¢ã€åº«å­˜ç¢ºèªã€ç‡Ÿé‹æ•¸æ“šç­‰æ¥­å‹™ã€‚`
}

function getSystemPrompt(mode: 'chat' | 'voice' | 'assistant' = 'chat'): string {
  return SYSTEM_PROMPTS[mode] || SYSTEM_PROMPTS.chat
}

// ========================================
// GLM æä¾›å•†å¯¦ç¾
// ========================================

export class GLMProvider implements AIProvider {
  private apiKeys: string[]
  private mode: 'chat' | 'voice' | 'assistant' = 'chat'
  private currentKeyIndex = 0
  private config: Required<AIProviderConfig>

  // API é…ç½®
  private readonly API_CONFIG = {
    // ä¸»è¦æ¨¡å‹ - ä½¿ç”¨ GLM 4.7 ç‰¹æƒ ç‰ˆ MAXï¼ˆæœ€å¼·ï¼‰
    primary: {
      baseURL: 'https://open.bigmodel.cn/api/paas/v4/chat/completions',
      model: 'glm-4.7-coding-max',
    },
    // å‚™ç”¨æ¨¡å‹
    fallback: {
      baseURL: 'https://open.bigmodel.cn/api/paas/v4/chat/completions',
      model: 'glm-4-flash',
    },
  }

  constructor(apiKeys: string[], config: AIProviderConfig = {}, mode: 'chat' | 'voice' | 'assistant' = 'chat') {
    this.apiKeys = apiKeys.filter(k => k.trim().length > 0)
    this.mode = mode
    this.config = {
      maxRetries: config.maxRetries ?? 3,
      timeout: config.timeout ?? 60000,
      enableLocalFallback: config.enableLocalFallback ?? true,
    }
  }

  /**
   * è¨­ç½® AI æ¨¡å¼
   */
  setMode(mode: 'chat' | 'voice' | 'assistant') {
    this.mode = mode
  }

  getName(): string {
    return 'GLM'
  }

  isAvailable(): boolean {
    return this.apiKeys.length > 0
  }

  /**
   * ç²å–ç•¶å‰ API Key
   */
  private getCurrentApiKey(): string {
    if (this.apiKeys.length === 0) {
      throw new Error('æ²’æœ‰å¯ç”¨çš„ API Key')
    }
    return this.apiKeys[this.currentKeyIndex]
  }

  /**
   * åˆ‡æ›åˆ°ä¸‹ä¸€å€‹ API Key
   */
  private rotateApiKey(): void {
    this.currentKeyIndex = (this.currentKeyIndex + 1) % this.apiKeys.length
  }

  /**
   * æŒ‡æ•¸é€€é¿å»¶é²
   */
  private async backoff(attempt: number): Promise<void> {
    const delay = Math.min(1000 * Math.pow(2, attempt), 10000)
    await new Promise(resolve => setTimeout(resolve, delay))
  }

  /**
   * å¸¶é‡è©¦çš„ HTTP è«‹æ±‚
   */
  private async fetchWithRetry(
    url: string,
    options: RequestInit,
    attempt = 0
  ): Promise<Response> {
    try {
      const response = await fetch(url, {
        ...options,
        signal: AbortSignal.timeout(this.config.timeout),
      })

      // æª¢æŸ¥æ˜¯å¦éœ€è¦é‡è©¦
      if (!response.ok && attempt < this.config.maxRetries) {
        const isAuthError = response.status === 401 || response.status === 403

        if (isAuthError) {
          // èªè­‰éŒ¯èª¤ï¼šå˜—è©¦ä¸‹ä¸€å€‹ API Key
          this.rotateApiKey()
          console.log(`API Key èªè­‰å¤±æ•—ï¼Œåˆ‡æ›åˆ°ä¸‹ä¸€å€‹ Key (attempt ${attempt + 1})`)
          await this.backoff(attempt)

          // æ›´æ–°è«‹æ±‚ä¸­çš„ Authorization header
          const newOptions = {
            ...options,
            headers: {
              ...options.headers,
              'Authorization': `Bearer ${this.getCurrentApiKey()}`,
            },
          }

          return this.fetchWithRetry(url, newOptions, attempt + 1)
        }

        // å…¶ä»–éŒ¯èª¤ï¼šé‡è©¦
        console.log(`è«‹æ±‚å¤±æ•— (${response.status})ï¼Œé‡è©¦ä¸­... (attempt ${attempt + 1})`)
        await this.backoff(attempt)
        return this.fetchWithRetry(url, options, attempt + 1)
      }

      return response
    } catch (error) {
      // ç¶²çµ¡éŒ¯èª¤æˆ–è¶…æ™‚ï¼šé‡è©¦
      if (attempt < this.config.maxRetries) {
        console.log(`è«‹æ±‚éŒ¯èª¤: ${error}ï¼Œé‡è©¦ä¸­... (attempt ${attempt + 1})`)
        await this.backoff(attempt)
        return this.fetchWithRetry(url, options, attempt + 1)
      }
      throw error
    }
  }

  /**
   * æ§‹å»ºè«‹æ±‚é ­
   */
  private getHeaders(): Record<string, string> {
    return {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${this.getCurrentApiKey()}`,
    }
  }

  /**
   * æ§‹å»ºè«‹æ±‚é«”
   */
  private getRequestBody(
    messages: ChatMessage[],
    stream: boolean = false
  ): any {
    return {
      model: this.API_CONFIG.primary.model,
      messages,
      temperature: 0.7,
      top_p: 0.9,
      max_tokens: 2000,
      stream,
    }
  }

  /**
   * ç²å–ç³»çµ±æç¤ºè©
   */
  private getSystemPrompt(): string {
    return getSystemPrompt(this.mode)
  }

  /**
   * èŠå¤©ï¼ˆéä¸²æµï¼‰
   */
  async chat(message: string, history: ChatMessage[] = []): Promise<ChatResponse> {
    if (!this.isAvailable()) {
      throw new Error('GLM æä¾›å•†ä¸å¯ç”¨ï¼šæ²’æœ‰é…ç½® API Key')
    }

    const messages: ChatMessage[] = [
      { role: 'system', content: this.getSystemPrompt() },
      ...history.slice(-10), // åªä¿ç•™æœ€è¿‘ 10 æ¢æ­·å²
      { role: 'user', content: message },
    ]

    try {
      // å˜—è©¦ä¸»è¦æ¨¡å‹
      const response = await this.fetchWithRetry(
        this.API_CONFIG.primary.baseURL,
        {
          method: 'POST',
          headers: this.getHeaders(),
          body: JSON.stringify(this.getRequestBody(messages, false)),
        }
      )

      if (!response.ok) {
        const error = await response.json()
        throw new Error(error.error?.message || 'GLM API è«‹æ±‚å¤±æ•—')
      }

      const data = await response.json()
      const content = data.choices[0]?.message?.content || ''

      return {
        content,
        model: data.model || this.API_CONFIG.primary.model,
        usage: data.usage
          ? {
              promptTokens: data.usage.prompt_tokens || 0,
              completionTokens: data.usage.completion_tokens || 0,
              totalTokens: data.usage.total_tokens || 0,
            }
          : undefined,
      }
    } catch (error) {
      console.error('GLM API Error:', error)
      throw new Error(`GLM API è«‹æ±‚å¤±æ•—: ${error instanceof Error ? error.message : 'æœªçŸ¥éŒ¯èª¤'}`)
    }
  }

  /**
   * èŠå¤©ï¼ˆä¸²æµï¼‰
   */
  async *chatStream(
    message: string,
    history: ChatMessage[] = []
  ): AsyncGenerator<StreamChunk> {
    if (!this.isAvailable()) {
      yield { type: 'error', error: 'GLM æä¾›å•†ä¸å¯ç”¨ï¼šæ²’æœ‰é…ç½® API Key' }
      return
    }

    const messages: ChatMessage[] = [
      { role: 'system', content: this.getSystemPrompt() },
      ...history.slice(-10),
      { role: 'user', content: message },
    ]

    try {
      const response = await this.fetchWithRetry(
        this.API_CONFIG.primary.baseURL,
        {
          method: 'POST',
          headers: this.getHeaders(),
          body: JSON.stringify(this.getRequestBody(messages, true)),
        }
      )

      if (!response.ok) {
        const error = await response.json()
        yield { type: 'error', error: error.error?.message || 'GLM API è«‹æ±‚å¤±æ•—' }
        return
      }

      const reader = response.body?.getReader()
      if (!reader) {
        yield { type: 'error', error: 'ç„¡æ³•è®€å–ä¸²æµå›æ‡‰' }
        return
      }

      const decoder = new TextDecoder()

      while (true) {
        const { done, value } = await reader.read()
        if (done) break

        const chunk = decoder.decode(value)
        const lines = chunk.split('\n').filter(line => line.trim() !== '')

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6)
            if (data === '[DONE]') continue

            try {
              const parsed = JSON.parse(data)
              const content = parsed.choices[0]?.delta?.content || ''

              if (content) {
                yield { type: 'content', text: content }
              }
            } catch (e) {
              // å¿½ç•¥è§£æéŒ¯èª¤
            }
          }
        }
      }
    } catch (error) {
      console.error('GLM Stream API Error:', error)
      yield {
        type: 'error',
        error: `GLM API ä¸²æµè«‹æ±‚å¤±æ•—: ${error instanceof Error ? error.message : 'æœªçŸ¥éŒ¯èª¤'}`,
      }
    }
  }
}

// ========================================
// æœ¬åœ° AI æä¾›å•†ï¼ˆå¾Œå‚™æ–¹æ¡ˆï¼‰
// ========================================

export class LocalFallbackProvider implements AIProvider {
  async chat(message: string, history?: ChatMessage[]): Promise<ChatResponse> {
    // ç°¡å–®çš„æœ¬åœ°è¦å‰‡å¼•æ“
    const lowerMessage = message.toLowerCase()

    // å¿«é€Ÿå›æ‡‰è¦å‰‡
    const quickResponses: Record<string, string> = {
      'ä»Šå¤©çš„è¨‚å–®': 'æ­£åœ¨æŸ¥è©¢ä»Šæ—¥è¨‚å–®...è«‹ä½¿ç”¨è¨‚å–®ç®¡ç†é é¢æŸ¥çœ‹è©³ç´°è³‡è¨Šã€‚',
      'ä»Šæ—¥è¨‚å–®': 'æ­£åœ¨æŸ¥è©¢ä»Šæ—¥è¨‚å–®...è«‹ä½¿ç”¨è¨‚å–®ç®¡ç†é é¢æŸ¥çœ‹è©³ç´°è³‡è¨Šã€‚',
      'åº«å­˜': 'ç“¦æ–¯åº«å­˜å……è¶³ï¼Œ20kgç“¦æ–¯æ¡¶ç›®å‰åº«å­˜æ­£å¸¸ã€‚',
      'ç‡Ÿæ¥­é¡': 'è«‹ä½¿ç”¨ç‡Ÿé‹å ±è¡¨é é¢æŸ¥çœ‹è©³ç´°ç‡Ÿæ”¶æ•¸æ“šã€‚',
      'ä¼‘å‡': 'è«‹ä½¿ç”¨ä¼‘å‡ç®¡ç†åŠŸèƒ½æŸ¥çœ‹ä»Šæ—¥ä¼‘å‡äººå“¡ã€‚',
    }

    for (const [key, response] of Object.entries(quickResponses)) {
      if (lowerMessage.includes(key)) {
        return { content: response, model: 'local-fallback' }
      }
    }

    return {
      content: 'æŠ±æ­‰ï¼ŒAI æœå‹™æš«æ™‚ç„¡æ³•ä½¿ç”¨ã€‚è«‹ç¨å¾Œå†è©¦æˆ–è¯ç¹«ç®¡ç†å“¡ã€‚',
      model: 'local-fallback',
    }
  }

  async *chatStream(
    message: string,
    history?: ChatMessage[]
  ): AsyncGenerator<StreamChunk> {
    const response = await this.chat(message, history)
    const words = response.content.split('')

    for (const word of words) {
      yield { type: 'content', text: word }
      await new Promise(resolve => setTimeout(resolve, 20))
    }
  }

  isAvailable(): boolean {
    return true // æœ¬åœ°æä¾›å•†å§‹çµ‚å¯ç”¨
  }

  getName(): string {
    return 'Local'
  }
}

// ========================================
// çµ±ä¸€ AI ç®¡ç†å™¨
// ========================================

export class AIManager {
  private providers: AIProvider[] = []
  private primaryProvider?: GLMProvider
  private fallbackProvider?: AIProvider

  constructor(config?: { 
    glmApiKeys?: string[]; 
    enableLocalFallback?: boolean;
    mode?: 'chat' | 'voice' | 'assistant';
  }) {
    // æ·»åŠ  GLM æä¾›å•†ï¼ˆæ”¯æŒä¸åŒæ¨¡å¼ï¼‰
    if (config?.glmApiKeys && config.glmApiKeys.length > 0) {
      this.primaryProvider = new GLMProvider(config.glmApiKeys, {
        enableLocalFallback: config.enableLocalFallback ?? true,
      }, config.mode || 'chat')
      this.providers.push(this.primaryProvider)
    }

    // æ·»åŠ æœ¬åœ°å¾Œå‚™æä¾›å•†
    if (config?.enableLocalFallback !== false) {
      this.fallbackProvider = new LocalFallbackProvider()
      this.providers.push(this.fallbackProvider)
    }
  }

  /**
   * åˆ‡æ› AI æ¨¡å¼
   */
  setMode(mode: 'chat' | 'voice' | 'assistant') {
    if (this.primaryProvider) {
      this.primaryProvider.setMode(mode)
    }
  }

  /**
   * ç²å–å¯ç”¨çš„æä¾›å•†
   */
  private getAvailableProvider(): AIProvider {
    // å„ªå…ˆä½¿ç”¨ä¸»è¦æä¾›å•†
    if (this.primaryProvider && this.primaryProvider.isAvailable()) {
      return this.primaryProvider
    }

    // ä½¿ç”¨å¾Œå‚™æä¾›å•†
    if (this.fallbackProvider && this.fallbackProvider.isAvailable()) {
      return this.fallbackProvider
    }

    throw new Error('æ²’æœ‰å¯ç”¨çš„ AI æä¾›å•†')
  }

  /**
   * ç™¼é€èŠå¤©è¨Šæ¯ï¼ˆéä¸²æµï¼‰
   */
  async chat(message: string, history?: ChatMessage[]): Promise<ChatResponse> {
    const provider = this.getAvailableProvider()
    return provider.chat(message, history)
  }

  /**
   * ç™¼é€èŠå¤©è¨Šæ¯ï¼ˆä¸²æµï¼‰
   */
  async *chatStream(
    message: string,
    history?: ChatMessage[]
  ): AsyncGenerator<StreamChunk> {
    const provider = this.getAvailableProvider()
    yield* provider.chatStream(message, history)
  }

  /**
   * æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æä¾›å•†
   */
  isAvailable(): boolean {
    return this.providers.some(p => p.isAvailable())
  }

  /**
   * ç²å–ç•¶å‰ä½¿ç”¨çš„æä¾›å•†åç¨±
   */
  getCurrentProviderName(): string {
    try {
      return this.getAvailableProvider().getName()
    } catch {
      return 'None'
    }
  }
}

// ========================================
// å–®ä¾‹æ¨¡å¼
// ========================================

let aiManagerInstance: AIManager | null = null

/**
 * ç²å– AI ç®¡ç†å™¨å¯¦ä¾‹
 */
export function getAIManager(): AIManager {
  if (!aiManagerInstance) {
    // å¾ç’°å¢ƒè®Šé‡ç²å– API Keys
    let apiKeys: string[] = []

    if (typeof process !== 'undefined' && process.env?.GLM_API_KEYS) {
      apiKeys = process.env.GLM_API_KEYS.split(',').map(k => k.trim()).filter(k => k.length > 0)
    } else if (typeof process !== 'undefined' && process.env?.GLM_API_KEY) {
      apiKeys = [process.env.GLM_API_KEY]
    }

    aiManagerInstance = new AIManager({
      glmApiKeys: apiKeys,
      enableLocalFallback: true,
    })
  }

  return aiManagerInstance
}

/**
 * è¨­ç½® API Keysï¼ˆå®¢æˆ¶ç«¯ï¼‰
 */
export function setAIApiKeys(keys: string[]): void {
  try {
    if (typeof localStorage !== 'undefined') {
      localStorage.setItem('GLM_API_KEYS', JSON.stringify(keys))
    }
  } catch (e) {
    console.warn('ç„¡æ³•ä¿å­˜ API Keys:', e)
  }
  aiManagerInstance = null // é‡ç½®å¯¦ä¾‹
}

/**
 * ç²å– API Keysï¼ˆå®¢æˆ¶ç«¯ï¼‰
 */
export function getAIApiKeys(): string[] {
  try {
    if (typeof localStorage !== 'undefined') {
      const stored = localStorage.getItem('GLM_API_KEYS')
      if (stored) {
        return JSON.parse(stored)
      }
    }
  } catch (e) {
    console.warn('ç„¡æ³•è®€å– API Keys:', e)
  }
  return []
}
