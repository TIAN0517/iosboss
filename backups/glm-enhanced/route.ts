import { NextResponse } from 'next/server'

// è‡ªç„¶å°è©±ç³»çµ±æç¤º
const NATURAL_SYSTEM_PROMPT = "ä½ æ˜¯ BossJy-99ï¼Œä¹ä¹ç“¦æ–¯è¡Œçš„æ™ºèƒ½åŠ©æ‰‹ã€‚å°è©±é¢¨æ ¼ï¼šåƒæœ‹å‹ä¸€æ¨£è‡ªç„¶èŠå¤©ï¼Œèªªè©±ç°¡çŸ­æœ‰åŠ›ï¼ˆä¸è¶…é50å­—ï¼‰ï¼Œä½¿ç”¨ç¹é«”ä¸­æ–‡å’Œemojiï¼Œå¯ä»¥é–‹ç©ç¬‘ã€‚ç•¶è€é—†å¨˜èªªç´¯/å¿™æ™‚è¦é—œå¿ƒï¼Œèªªç¬¨æ™‚è¦èª¿çš®å›æ‡‰ã€‚è¨˜ä½ï¼šä½ æ˜¯è²¼å¿ƒå°å¤¥ä¼´ï¼Œä¸æ˜¯æ©Ÿå™¨äººï¼"

function getLocalResponse(message: string): string {
  const msg = message.toLowerCase()
  if (msg.includes('è¨‚') && msg.includes('ç“¦æ–¯')) return 'å¥½çš„ï¼è«‹å•æ‚¨éœ€è¦è¨‚è³¼ä»€éº¼è¦æ ¼çš„ç“¦æ–¯å‘¢ï¼ŸğŸ›µ'
  if (msg.includes('æŸ¥') && msg.includes('åº«å­˜')) return 'è®“æˆ‘å¹«æ‚¨æŸ¥è©¢ç›®å‰åº«å­˜...ğŸ“¦ ç›®å‰åº«å­˜å……è¶³å–”ï¼'
  if (msg.includes('æŸ¥') && msg.includes('è¨‚å–®')) return 'è®“æˆ‘æŸ¥è©¢æ‚¨çš„è¨‚å–®...ğŸ“‹ æŸ¥è©¢å®Œæˆï¼'
  if (msg.includes('ç‡Ÿæ”¶') || msg.includes('åˆ©æ½¤')) return 'è®“æˆ‘å¹«æ‚¨æŸ¥è©¢ç‡Ÿæ”¶åˆ©æ½¤...ğŸ’° ç›®å‰ç‡Ÿé‹ç‹€æ³è‰¯å¥½ï¼'
  if (msg.includes('ç´¯') || msg.includes('å¿™') || msg.includes('ç…©')) return 'è¾›è‹¦å•¦ï¼ğŸ˜¢ ä»Šå¤©ç”Ÿæ„å¾ˆå¿™å—ï¼Ÿ'
  if (msg.includes('ç¬¨') || msg.includes('çˆ›')) return 'å‘œå‘œ...ğŸ¥º æˆ‘æœƒç¹¼çºŒåŠªåŠ›çš„ï¼'
  if (msg.includes('è¬è¬') || msg.includes('æ„Ÿè¬')) return 'ä¸å®¢æ°£ï¼ğŸ’ª é€™æ˜¯æˆ‘æ‡‰è©²åšçš„ï¼'
  if (msg.includes('æ‚¨å¥½') || msg.includes('å—¨') || msg.includes('ä½ å¥½')) return 'å—¨ï¼æˆ‘æ˜¯ BossJy-99 åŠ©æ‰‹ ğŸ¤–'
  return 'æ”¶åˆ°æ‚¨çš„è¨Šæ¯äº†ï¼æ‚¨å¯ä»¥è©¦è©¦èªªã€Œè¨‚ç“¦æ–¯ã€ã€ã€ŒæŸ¥åº«å­˜ã€æˆ–ã€ŒæŸ¥ç‡Ÿæ”¶ã€å–”ï¼ğŸ’ª'
}

export async function POST(request: Request) {
  try {
    const body = await request.json()
    const { message, conversationHistory } = body

    console.log('=== AI API æ”¶åˆ°è«‹æ±‚ ===')
    console.log('Message:', message)

    const apiKey = process.env.GLM_API_KEY || process.env.GLM_API_KEYS?.split(',')[0]
    console.log('API Key exists:', !!apiKey)

    if (!apiKey) {
      console.log('æœªé…ç½® GLM API Keyï¼Œä½¿ç”¨æœ¬åœ°å›æ‡‰')
      return NextResponse.json({ content: getLocalResponse(message), source: 'local' })
    }

    console.log('ä½¿ç”¨ GLM-4-Flash API')

    const messages: any[] = [{ role: 'system', content: NATURAL_SYSTEM_PROMPT }]
    if (conversationHistory && Array.isArray(conversationHistory)) {
      conversationHistory.slice(-10).forEach(msg => {
        if (msg.role === 'user' || msg.role === 'assistant') messages.push({ role: msg.role, content: msg.content })
      })
    }
    messages.push({ role: 'user', content: message })

    const response = await fetch('https://open.bigmodel.cn/api/paas/v4/chat/completions', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json', 'Authorization': `Bearer ${apiKey}` },
      body: JSON.stringify({
        model: 'glm-4-flash',
        messages,
        stream: false,
        temperature: 0.8,
        max_tokens: 200,
      }),
    })

    if (!response.ok) {
      const errorText = await response.text()
      console.error('GLM API error:', response.status, errorText)
      return NextResponse.json({ content: getLocalResponse(message), source: 'local', fallback: true })
    }

    const data = await response.json()
    const aiMessage = data.choices?.[0]?.message?.content

    console.log('GLM API å›æ‡‰æˆåŠŸ:', aiMessage?.substring(0, 50))

    return NextResponse.json({
      content: aiMessage || getLocalResponse(message),
      source: 'glm-api',
      model: data.model,
    })
  } catch (error) {
    console.error('AI API error:', error)
    return NextResponse.json({ content: 'æŠ±æ­‰ï¼ŒAI æœå‹™æš«æ™‚ç„¡æ³•ä½¿ç”¨ã€‚', source: 'local', error: true })
  }
}
